{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a26078",
   "metadata": {},
   "source": [
    "# Initial Comparison\n",
    "\n",
    "We run integrated gradients and activation patching on the same model and dataset, to compare attribution scores.\n",
    "\n",
    "- Model: GPT2-Small (12 layers, 12 attention heads per layer, embedding size 768, 3,072 neurons per MLP layer)\n",
    "- Dataset: Indirect Object Identification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b754ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8a70a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "\n",
    "from attribution_methods import integrated_gradients, activation_patching, highlight_components\n",
    "from testing import Task, TaskDataset, logit_diff_metric, average_correlation, measure_overlap\n",
    "from plotting import plot_attn, plot_attn_comparison, plot_correlation, plot_correlation_comparison, plot_mean_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23191a5",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70601f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-0.0307, -0.9269, -0.4937,  2.2320,  0.6754,  4.0447, -0.1785,  1.1947,\n",
      "         1.1514,  1.7507,  0.1791,  4.2971,  2.9955, -0.7016, -2.1907, -3.5684,\n",
      "        -4.4879, -1.2934, -3.8906, -0.6969, -0.8222,  0.0708,  0.2167,  4.4769,\n",
      "         1.0375, -1.2644,  0.9309,  2.8114,  0.9975,  2.4103,  2.6244,  0.0125,\n",
      "        -0.8472, -0.6130, -1.1623, -0.5109,  3.0073,  0.6154, -1.1229,  0.2680,\n",
      "        -2.7379,  5.2855,  2.5019,  0.3219, -1.3112,  1.2942, -2.1428,  3.1053,\n",
      "         1.6090,  3.1023,  1.8912,  0.4089,  4.0511,  2.5005,  3.5176, -1.5472,\n",
      "         2.2213, -0.8523,  0.6682,  0.4244,  0.8053,  3.2905,  0.7295,  0.9946,\n",
      "        -3.6073, -2.2671,  1.7894, -0.6390,  0.6320, -1.5326,  1.3206, -0.1224,\n",
      "         0.1692,  1.9326,  3.1771,  1.1320, -0.0876,  3.1172,  2.3856,  3.2836,\n",
      "        -2.0859,  3.6953,  2.8494, -2.4261,  1.1299,  0.1732, -1.4748, -2.1046,\n",
      "        -0.6516, -0.6167,  0.0277, -1.7128,  0.6374,  2.6352, -1.4080,  3.2583,\n",
      "         0.6919, -2.4436,  0.6666,  2.5258], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([-3.8746e-02, -9.4513e-01, -5.1027e-01,  2.2153e+00,  6.2990e-01,\n",
      "        -3.2074e+00, -1.8231e-01,  1.1766e+00, -3.0072e+00,  1.7392e+00,\n",
      "         1.4938e-01,  2.5878e-01, -4.2863e+00, -7.1594e-01, -2.2568e+00,\n",
      "        -3.5882e+00, -4.4825e+00, -1.3015e+00, -3.9175e+00, -7.0902e-01,\n",
      "        -8.1898e-01,  4.9940e-02,  2.0002e-01,  4.4623e+00,  1.0164e+00,\n",
      "        -1.2665e+00,  9.0841e-01, -4.5392e+00,  9.8613e-01,  2.4149e+00,\n",
      "         2.6026e+00, -9.6668e-03, -8.9293e-01, -6.2356e-01, -1.1595e+00,\n",
      "        -5.1318e-01, -1.1410e+00,  6.0697e-01, -1.1723e+00,  2.4156e-01,\n",
      "        -2.7512e+00, -1.6696e+00,  2.4944e+00,  3.2915e-01, -1.3232e+00,\n",
      "         1.2809e+00, -2.1493e+00,  3.1009e+00,  1.6058e+00, -2.0700e+00,\n",
      "        -1.9341e+00,  3.9242e-01, -1.7774e+00,  2.4945e+00, -4.5008e+00,\n",
      "        -1.6102e+00,  2.2123e+00, -8.6050e-01,  6.3914e-01,  4.0987e-01,\n",
      "         7.8476e-01, -3.1501e+00,  7.3348e-01,  9.7457e-01, -3.6393e+00,\n",
      "        -2.2872e+00,  1.7992e+00, -6.3969e-01, -2.1137e+00, -1.5704e+00,\n",
      "         1.2943e+00, -1.2390e-01,  1.5703e-01,  1.9095e+00,  3.1132e+00,\n",
      "         1.1251e+00, -1.0500e-01,  3.1100e+00,  2.3432e+00, -2.7600e+00,\n",
      "        -2.1219e+00, -3.6873e+00, -4.4972e+00, -2.4419e+00,  1.1330e+00,\n",
      "         1.4468e-01, -1.4990e+00, -2.1135e+00, -6.9057e-01, -6.2552e-01,\n",
      "         1.6040e-03, -1.7046e+00,  6.0108e-01, -4.6585e+00, -1.4097e+00,\n",
      "         3.2455e+00,  6.7000e-01, -2.4674e+00,  6.4137e-01, -4.3624e+00],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=100)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n",
      "            1.3576e-01,  1.0932e-01],\n",
      "          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n",
      "           -3.8492e-01,  2.4711e-01],\n",
      "          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n",
      "            1.2671e-01, -1.8432e-01],\n",
      "          ...,\n",
      "          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n",
      "           -1.7141e-01, -1.1625e-01],\n",
      "          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n",
      "            2.5598e-01, -7.1256e-02],\n",
      "          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n",
      "           -3.0645e-01,  3.7273e-01]],\n",
      "\n",
      "         [[ 6.7831e-02,  5.0939e-02, -1.1132e-02,  ..., -3.2761e-02,\n",
      "            1.1142e-01,  1.0431e-01],\n",
      "          [ 9.2455e-02,  4.3506e-02,  1.1513e-02,  ...,  3.6808e-01,\n",
      "            2.6727e-01,  3.8849e-02],\n",
      "          [-2.5921e-02, -1.4968e-01,  3.8238e-01,  ..., -4.0184e-02,\n",
      "            1.2549e-01, -1.6114e-01],\n",
      "          ...,\n",
      "          [-3.2323e-01, -4.1469e-01,  1.7529e-01,  ..., -4.1531e-02,\n",
      "           -1.6781e-01, -1.4834e-01],\n",
      "          [-1.0166e-01, -3.5033e-01, -1.6252e-01,  ...,  2.0222e-01,\n",
      "            2.2830e-01, -5.0067e-02],\n",
      "          [ 1.5458e-01, -4.2453e-01, -1.2530e-01,  ...,  1.8897e-01,\n",
      "           -2.1968e-01,  3.2371e-01]],\n",
      "\n",
      "         [[ 4.7630e-02,  4.1370e-02,  4.9112e-02,  ...,  2.8627e-02,\n",
      "           -2.5047e-02,  3.9866e-02],\n",
      "          [-3.4625e-02, -1.8339e-01, -3.7899e-02,  ...,  1.1424e-01,\n",
      "            2.2033e-02,  1.5376e-01],\n",
      "          [-1.8984e-02, -1.5602e-01,  3.6694e-01,  ..., -2.1955e-02,\n",
      "            1.3544e-01, -1.5779e-01],\n",
      "          ...,\n",
      "          [-3.3776e-01, -3.9975e-01,  1.7314e-01,  ..., -3.9948e-02,\n",
      "           -1.6569e-01, -1.4448e-01],\n",
      "          [-1.5470e-01, -3.6870e-01, -1.3930e-01,  ...,  9.4379e-02,\n",
      "            1.7840e-01,  5.6227e-03],\n",
      "          [ 1.3006e-01, -2.8991e-01, -1.1905e-01,  ...,  1.9156e-01,\n",
      "           -1.9444e-01,  3.5320e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7964e-02,  1.6472e-02,  6.1408e-02,  ..., -3.1008e-02,\n",
      "            2.1461e-02,  6.4884e-02],\n",
      "          [-1.5999e-01,  1.2143e-01, -5.6784e-02,  ...,  2.2765e-02,\n",
      "            1.0889e-01,  2.3986e-01],\n",
      "          [ 2.4424e-02, -1.0785e-01,  2.8684e-01,  ...,  3.0898e-03,\n",
      "            1.2460e-01, -7.4821e-02],\n",
      "          ...,\n",
      "          [-3.1681e-01, -3.7926e-01,  1.2947e-01,  ...,  4.1872e-02,\n",
      "           -1.5842e-01, -1.3259e-01],\n",
      "          [-1.3369e-01, -2.4390e-01, -1.6870e-01,  ...,  1.5684e-01,\n",
      "            7.0074e-02, -9.0703e-02],\n",
      "          [ 1.3219e-01, -1.3280e-02, -1.0322e-01,  ...,  1.1605e-01,\n",
      "            8.3867e-03,  3.1787e-01]],\n",
      "\n",
      "         [[ 2.2995e-02,  3.1015e-02,  2.9417e-02,  ..., -3.1645e-02,\n",
      "            6.6402e-02,  7.6391e-02],\n",
      "          [-1.8374e-01,  1.1877e-01, -5.2637e-02,  ...,  3.3817e-02,\n",
      "            1.1707e-01,  2.4052e-01],\n",
      "          [-7.3880e-03, -1.3060e-01,  3.5035e-01,  ..., -6.1699e-03,\n",
      "            1.3752e-01, -1.0543e-01],\n",
      "          ...,\n",
      "          [-3.1204e-01, -4.2457e-01,  1.4641e-01,  ...,  4.8586e-02,\n",
      "           -1.5239e-01, -1.4980e-01],\n",
      "          [-1.4710e-01, -2.7297e-01, -1.8023e-01,  ...,  1.4163e-01,\n",
      "            1.2103e-01, -7.2755e-02],\n",
      "          [ 1.5631e-01, -5.3971e-02, -1.4057e-01,  ...,  1.4260e-01,\n",
      "           -2.9078e-02,  3.3889e-01]],\n",
      "\n",
      "         [[ 2.5622e-02,  4.0842e-02,  5.9342e-03,  ..., -3.2277e-02,\n",
      "            9.7705e-02,  8.4893e-02],\n",
      "          [-2.0260e-01,  1.1639e-01, -4.9727e-02,  ...,  3.8608e-02,\n",
      "            1.1786e-01,  2.3868e-01],\n",
      "          [-2.9670e-02, -1.4739e-01,  3.9738e-01,  ..., -1.3245e-02,\n",
      "            1.4863e-01, -1.2849e-01],\n",
      "          ...,\n",
      "          [-3.0994e-01, -4.5522e-01,  1.5647e-01,  ...,  5.2514e-02,\n",
      "           -1.4775e-01, -1.6256e-01],\n",
      "          [-1.5696e-01, -2.9720e-01, -1.8853e-01,  ...,  1.2999e-01,\n",
      "            1.6088e-01, -5.7852e-02],\n",
      "          [ 1.7492e-01, -8.4511e-02, -1.6990e-01,  ...,  1.6289e-01,\n",
      "           -5.7177e-02,  3.5560e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n",
      "            1.3576e-01,  1.0932e-01],\n",
      "          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n",
      "           -3.8492e-01,  2.4711e-01],\n",
      "          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n",
      "            1.2671e-01, -1.8432e-01],\n",
      "          ...,\n",
      "          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n",
      "           -1.7141e-01, -1.1625e-01],\n",
      "          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n",
      "            2.5598e-01, -7.1256e-02],\n",
      "          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n",
      "           -3.0645e-01,  3.7273e-01]],\n",
      "\n",
      "         [[ 2.1523e-02,  5.7400e-02,  2.8887e-02,  ..., -5.8523e-02,\n",
      "            1.4723e-01,  1.1393e-01],\n",
      "          [-4.3695e-02, -7.5245e-02, -1.4187e-01,  ...,  5.0209e-01,\n",
      "            1.1648e-01, -5.2276e-02],\n",
      "          [-4.4642e-02, -1.4096e-01,  3.9064e-01,  ..., -6.6014e-02,\n",
      "            1.2606e-01, -1.7095e-01],\n",
      "          ...,\n",
      "          [-3.0188e-01, -4.5466e-01,  1.8813e-01,  ..., -5.4116e-02,\n",
      "           -1.6290e-01, -1.4502e-01],\n",
      "          [-7.0380e-02, -3.2941e-01, -1.3598e-01,  ...,  1.7175e-01,\n",
      "            1.9076e-01, -2.4613e-02],\n",
      "          [ 8.8913e-02, -3.9729e-01, -1.1350e-01,  ...,  1.7845e-01,\n",
      "           -1.3368e-01,  3.2550e-01]],\n",
      "\n",
      "         [[-1.2164e-02,  5.4927e-02,  6.5491e-02,  ..., -8.0310e-02,\n",
      "            1.5219e-01,  1.1623e-01],\n",
      "          [ 3.1268e-01,  1.1758e-01,  1.1420e-02,  ...,  1.4337e-01,\n",
      "            3.5595e-01, -1.4494e-01],\n",
      "          [-1.2890e-01, -1.0738e-01,  2.6023e-01,  ..., -4.0946e-02,\n",
      "            6.9806e-02, -9.3136e-02],\n",
      "          ...,\n",
      "          [-3.1635e-01, -4.5066e-01,  2.1648e-01,  ..., -4.8265e-02,\n",
      "           -1.8836e-01, -1.3195e-01],\n",
      "          [-7.2406e-02, -3.3335e-01, -1.7236e-01,  ...,  1.9480e-01,\n",
      "            1.8478e-01, -4.5556e-02],\n",
      "          [ 6.6565e-02, -3.5727e-01, -9.8946e-02,  ...,  1.8946e-01,\n",
      "           -5.4857e-02,  3.0191e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.6816e-03,  1.2588e-02,  9.3057e-02,  ..., -3.5217e-02,\n",
      "            2.7490e-03,  4.3493e-02],\n",
      "          [-3.5205e-01,  1.2843e-01,  1.7570e-01,  ...,  4.0965e-01,\n",
      "           -2.3407e-02,  1.3685e-01],\n",
      "          [ 2.5526e-02, -7.8613e-02,  1.7550e-01,  ..., -5.4352e-02,\n",
      "            9.6773e-02, -4.7376e-02],\n",
      "          ...,\n",
      "          [-3.1075e-01, -3.1455e-01,  1.0531e-01,  ...,  5.7625e-02,\n",
      "           -2.0267e-01, -7.4197e-02],\n",
      "          [-2.1245e-02, -2.2209e-01, -1.8996e-01,  ...,  8.1946e-02,\n",
      "           -8.3366e-02, -1.1034e-01],\n",
      "          [ 1.0643e-01, -2.7306e-02, -9.2942e-02,  ...,  1.2196e-01,\n",
      "            1.2620e-01,  3.0618e-01]],\n",
      "\n",
      "         [[ 6.5722e-03,  5.4432e-03,  1.3853e-01,  ..., -5.5087e-02,\n",
      "            5.3991e-02,  6.0835e-02],\n",
      "          [ 3.3694e-03,  1.9547e-01, -2.5315e-01,  ...,  2.3109e-01,\n",
      "            3.2080e-01,  1.0274e-01],\n",
      "          [ 7.3476e-02, -5.2200e-02,  2.3359e-01,  ..., -3.7195e-02,\n",
      "            5.0588e-02, -1.0580e-01],\n",
      "          ...,\n",
      "          [-3.0177e-01, -3.2666e-01,  9.0518e-02,  ...,  4.6607e-02,\n",
      "           -2.1560e-01, -9.1173e-02],\n",
      "          [-7.4994e-02, -1.4298e-01, -1.6510e-01,  ...,  1.7534e-01,\n",
      "           -3.8307e-02, -1.3786e-01],\n",
      "          [ 1.1587e-01, -2.6075e-02, -1.0854e-01,  ...,  1.0982e-01,\n",
      "            9.9208e-02,  3.1005e-01]],\n",
      "\n",
      "         [[-1.2901e-03,  2.0926e-02,  5.0628e-02,  ..., -5.5895e-02,\n",
      "            8.5195e-02,  7.8551e-02],\n",
      "          [-2.2481e-01,  1.1556e-01, -5.0402e-02,  ...,  3.0379e-02,\n",
      "            1.0313e-01,  2.3451e-01],\n",
      "          [ 1.2420e-02, -8.9845e-02,  3.0118e-01,  ..., -7.3929e-02,\n",
      "            1.2166e-01, -7.5274e-02],\n",
      "          ...,\n",
      "          [-3.0356e-01, -3.8901e-01,  1.2217e-01,  ...,  5.0004e-02,\n",
      "           -1.9620e-01, -1.2368e-01],\n",
      "          [-8.9914e-02, -2.1615e-01, -1.7096e-01,  ...,  1.4241e-01,\n",
      "            4.6534e-02, -9.2885e-02],\n",
      "          [ 1.4338e-01, -4.7637e-03, -1.2777e-01,  ...,  1.2251e-01,\n",
      "            8.8459e-02,  3.2753e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n",
      "            1.3576e-01,  1.0932e-01],\n",
      "          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n",
      "           -3.8492e-01,  2.4711e-01],\n",
      "          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n",
      "            1.2671e-01, -1.8432e-01],\n",
      "          ...,\n",
      "          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n",
      "           -1.7141e-01, -1.1625e-01],\n",
      "          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n",
      "            2.5598e-01, -7.1256e-02],\n",
      "          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n",
      "           -3.0645e-01,  3.7273e-01]],\n",
      "\n",
      "         [[ 6.7831e-02,  5.0939e-02, -1.1132e-02,  ..., -3.2761e-02,\n",
      "            1.1142e-01,  1.0431e-01],\n",
      "          [ 9.2455e-02,  4.3506e-02,  1.1513e-02,  ...,  3.6808e-01,\n",
      "            2.6727e-01,  3.8849e-02],\n",
      "          [-2.5921e-02, -1.4968e-01,  3.8238e-01,  ..., -4.0184e-02,\n",
      "            1.2549e-01, -1.6114e-01],\n",
      "          ...,\n",
      "          [-3.2323e-01, -4.1469e-01,  1.7529e-01,  ..., -4.1531e-02,\n",
      "           -1.6781e-01, -1.4834e-01],\n",
      "          [-1.0166e-01, -3.5033e-01, -1.6252e-01,  ...,  2.0222e-01,\n",
      "            2.2830e-01, -5.0067e-02],\n",
      "          [ 1.5458e-01, -4.2453e-01, -1.2530e-01,  ...,  1.8897e-01,\n",
      "           -2.1968e-01,  3.2371e-01]],\n",
      "\n",
      "         [[ 6.2231e-02,  3.8492e-02,  4.2852e-02,  ..., -3.4892e-02,\n",
      "            6.9039e-02,  1.1085e-01],\n",
      "          [-1.2554e-01,  3.0019e-01, -1.7803e-01,  ...,  3.1857e-01,\n",
      "           -1.0196e-01,  9.4652e-02],\n",
      "          [-1.7336e-02, -1.4727e-01,  3.7776e-01,  ..., -3.6973e-02,\n",
      "            1.1803e-01, -1.5076e-01],\n",
      "          ...,\n",
      "          [-3.2621e-01, -3.9623e-01,  1.7416e-01,  ..., -3.7113e-02,\n",
      "           -1.6524e-01, -1.4770e-01],\n",
      "          [-1.1398e-01, -3.8862e-01, -1.2154e-01,  ...,  1.7874e-01,\n",
      "            1.1648e-01, -4.2923e-02],\n",
      "          [ 1.5772e-01, -2.8063e-01, -7.3890e-02,  ...,  2.3557e-01,\n",
      "           -1.0678e-01,  3.0178e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7288e-02,  1.6706e-02,  6.0358e-02,  ..., -7.6639e-02,\n",
      "            3.7089e-03,  7.7111e-02],\n",
      "          [-1.5997e-01,  1.2162e-01, -5.6870e-02,  ...,  2.2827e-02,\n",
      "            1.0889e-01,  2.3988e-01],\n",
      "          [ 3.7095e-02, -1.0668e-01,  2.8207e-01,  ..., -1.6799e-02,\n",
      "            1.2862e-01, -6.0727e-02],\n",
      "          ...,\n",
      "          [-2.9725e-01, -3.2975e-01,  1.4199e-01,  ...,  3.5431e-02,\n",
      "           -1.6311e-01, -1.2313e-01],\n",
      "          [-1.3485e-01, -2.4810e-01, -1.8378e-01,  ...,  1.6925e-01,\n",
      "            4.6526e-02, -8.0878e-02],\n",
      "          [ 1.2137e-01, -1.7317e-02, -9.1313e-02,  ...,  1.2222e-01,\n",
      "            1.9244e-02,  3.1854e-01]],\n",
      "\n",
      "         [[ 3.0787e-02,  3.1926e-02,  2.5110e-02,  ..., -6.9372e-02,\n",
      "            5.3611e-02,  8.7770e-02],\n",
      "          [-1.8373e-01,  1.1885e-01, -5.2677e-02,  ...,  3.3846e-02,\n",
      "            1.1707e-01,  2.4053e-01],\n",
      "          [ 2.5327e-03, -1.2937e-01,  3.4687e-01,  ..., -2.2632e-02,\n",
      "            1.4051e-01, -9.3996e-02],\n",
      "          ...,\n",
      "          [-2.9648e-01, -3.8323e-01,  1.5613e-01,  ...,  4.3255e-02,\n",
      "           -1.5623e-01, -1.4170e-01],\n",
      "          [-1.4818e-01, -2.7624e-01, -1.9337e-01,  ...,  1.5243e-01,\n",
      "            1.0071e-01, -6.4406e-02],\n",
      "          [ 1.4600e-01, -5.6761e-02, -1.2921e-01,  ...,  1.4788e-01,\n",
      "           -1.8575e-02,  3.3913e-01]],\n",
      "\n",
      "         [[ 3.2444e-02,  4.1614e-02,  3.4685e-03,  ..., -6.5672e-02,\n",
      "            8.6151e-02,  9.4199e-02],\n",
      "          [-2.0260e-01,  1.1644e-01, -4.9751e-02,  ...,  3.8627e-02,\n",
      "            1.1786e-01,  2.3868e-01],\n",
      "          [-2.0797e-02, -1.4652e-01,  3.9447e-01,  ..., -2.7402e-02,\n",
      "            1.5174e-01, -1.1850e-01],\n",
      "          ...,\n",
      "          [-2.9700e-01, -4.2040e-01,  1.6427e-01,  ...,  4.7704e-02,\n",
      "           -1.5137e-01, -1.5579e-01],\n",
      "          [-1.5794e-01, -2.9993e-01, -2.0009e-01,  ...,  1.3937e-01,\n",
      "            1.4296e-01, -5.0533e-02],\n",
      "          [ 1.6515e-01, -8.5985e-02, -1.5858e-01,  ...,  1.6738e-01,\n",
      "           -4.6747e-02,  3.5549e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n",
      "            1.3576e-01,  1.0932e-01],\n",
      "          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n",
      "           -3.8492e-01,  2.4711e-01],\n",
      "          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n",
      "            1.2671e-01, -1.8432e-01],\n",
      "          ...,\n",
      "          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n",
      "           -1.7141e-01, -1.1625e-01],\n",
      "          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n",
      "            2.5598e-01, -7.1256e-02],\n",
      "          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n",
      "           -3.0645e-01,  3.7273e-01]],\n",
      "\n",
      "         [[ 2.1523e-02,  5.7400e-02,  2.8887e-02,  ..., -5.8523e-02,\n",
      "            1.4723e-01,  1.1393e-01],\n",
      "          [-4.3695e-02, -7.5245e-02, -1.4187e-01,  ...,  5.0209e-01,\n",
      "            1.1648e-01, -5.2276e-02],\n",
      "          [-4.4642e-02, -1.4096e-01,  3.9064e-01,  ..., -6.6014e-02,\n",
      "            1.2606e-01, -1.7095e-01],\n",
      "          ...,\n",
      "          [-3.0188e-01, -4.5466e-01,  1.8813e-01,  ..., -5.4116e-02,\n",
      "           -1.6290e-01, -1.4502e-01],\n",
      "          [-7.0380e-02, -3.2941e-01, -1.3598e-01,  ...,  1.7175e-01,\n",
      "            1.9076e-01, -2.4613e-02],\n",
      "          [ 8.8913e-02, -3.9729e-01, -1.1350e-01,  ...,  1.7845e-01,\n",
      "           -1.3368e-01,  3.2550e-01]],\n",
      "\n",
      "         [[-1.2164e-02,  5.4927e-02,  6.5491e-02,  ..., -8.0310e-02,\n",
      "            1.5219e-01,  1.1623e-01],\n",
      "          [ 3.1268e-01,  1.1758e-01,  1.1420e-02,  ...,  1.4337e-01,\n",
      "            3.5595e-01, -1.4494e-01],\n",
      "          [-1.2890e-01, -1.0738e-01,  2.6023e-01,  ..., -4.0946e-02,\n",
      "            6.9806e-02, -9.3136e-02],\n",
      "          ...,\n",
      "          [-3.1635e-01, -4.5066e-01,  2.1648e-01,  ..., -4.8265e-02,\n",
      "           -1.8836e-01, -1.3195e-01],\n",
      "          [-7.2406e-02, -3.3335e-01, -1.7236e-01,  ...,  1.9480e-01,\n",
      "            1.8478e-01, -4.5556e-02],\n",
      "          [ 6.6565e-02, -3.5727e-01, -9.8946e-02,  ...,  1.8946e-01,\n",
      "           -5.4857e-02,  3.0191e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0756e-02, -2.8227e-02,  6.0958e-02,  ..., -5.9904e-02,\n",
      "            1.9186e-02,  7.8854e-02],\n",
      "          [ 8.7646e-02,  2.0115e-01, -2.5177e-01,  ...,  2.3242e-01,\n",
      "            3.2985e-01,  1.0001e-01],\n",
      "          [ 9.2931e-02, -5.6490e-02,  2.0541e-01,  ..., -1.1183e-02,\n",
      "            2.5234e-02, -1.1514e-01],\n",
      "          ...,\n",
      "          [-3.0885e-01, -3.3446e-01,  7.8582e-02,  ...,  6.8496e-02,\n",
      "           -1.8361e-01, -8.8821e-02],\n",
      "          [-6.2606e-02, -1.5169e-01, -1.7072e-01,  ...,  2.2807e-01,\n",
      "           -3.9386e-03, -1.4421e-01],\n",
      "          [ 8.5318e-02, -3.0050e-02, -6.4854e-02,  ...,  1.2855e-01,\n",
      "            8.2223e-02,  2.9356e-01]],\n",
      "\n",
      "         [[-2.5266e-02, -1.3075e-03,  1.5223e-02,  ..., -5.5757e-02,\n",
      "            6.0826e-02,  9.2956e-02],\n",
      "          [-1.9273e-01,  1.1981e-01, -5.5339e-02,  ...,  2.6279e-02,\n",
      "            1.0813e-01,  2.4025e-01],\n",
      "          [-1.6209e-02, -1.1311e-01,  3.0033e-01,  ..., -4.5686e-02,\n",
      "            1.0248e-01, -7.3649e-02],\n",
      "          ...,\n",
      "          [-3.1234e-01, -3.9055e-01,  1.1094e-01,  ...,  6.7266e-02,\n",
      "           -1.6299e-01, -1.1635e-01],\n",
      "          [-8.5502e-02, -2.2060e-01, -1.8158e-01,  ...,  1.8352e-01,\n",
      "            7.1219e-02, -9.9214e-02],\n",
      "          [ 1.1353e-01, -8.0909e-03, -8.9529e-02,  ...,  1.4826e-01,\n",
      "            6.5619e-02,  3.1606e-01]],\n",
      "\n",
      "         [[-1.3884e-02,  1.6328e-02, -1.1405e-02,  ..., -5.2417e-02,\n",
      "            1.0107e-01,  1.0085e-01],\n",
      "          [-2.1596e-01,  1.1580e-01, -4.9730e-02,  ...,  3.7028e-02,\n",
      "            1.1370e-01,  2.3769e-01],\n",
      "          [-3.8826e-02, -1.3414e-01,  3.5957e-01,  ..., -4.7087e-02,\n",
      "            1.2009e-01, -1.0362e-01],\n",
      "          ...,\n",
      "          [-3.0978e-01, -4.3266e-01,  1.2953e-01,  ...,  6.7882e-02,\n",
      "           -1.5556e-01, -1.3729e-01],\n",
      "          [-1.0518e-01, -2.5392e-01, -1.9041e-01,  ...,  1.6549e-01,\n",
      "            1.2148e-01, -7.9822e-02],\n",
      "          [ 1.3682e-01, -4.3306e-02, -1.2380e-01,  ...,  1.6879e-01,\n",
      "            2.8479e-02,  3.3507e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n",
      "            1.3576e-01,  1.0932e-01],\n",
      "          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n",
      "           -3.8492e-01,  2.4711e-01],\n",
      "          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n",
      "            1.2671e-01, -1.8432e-01],\n",
      "          ...,\n",
      "          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n",
      "           -1.7141e-01, -1.1625e-01],\n",
      "          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n",
      "            2.5598e-01, -7.1256e-02],\n",
      "          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n",
      "           -3.0645e-01,  3.7273e-01]],\n",
      "\n",
      "         [[ 2.1523e-02,  5.7400e-02,  2.8887e-02,  ..., -5.8523e-02,\n",
      "            1.4723e-01,  1.1393e-01],\n",
      "          [-4.3695e-02, -7.5245e-02, -1.4187e-01,  ...,  5.0209e-01,\n",
      "            1.1648e-01, -5.2276e-02],\n",
      "          [-4.4642e-02, -1.4096e-01,  3.9064e-01,  ..., -6.6014e-02,\n",
      "            1.2606e-01, -1.7095e-01],\n",
      "          ...,\n",
      "          [-3.0188e-01, -4.5466e-01,  1.8813e-01,  ..., -5.4116e-02,\n",
      "           -1.6290e-01, -1.4502e-01],\n",
      "          [-7.0380e-02, -3.2941e-01, -1.3598e-01,  ...,  1.7175e-01,\n",
      "            1.9076e-01, -2.4613e-02],\n",
      "          [ 8.8913e-02, -3.9729e-01, -1.1350e-01,  ...,  1.7845e-01,\n",
      "           -1.3368e-01,  3.2550e-01]],\n",
      "\n",
      "         [[-1.2164e-02,  5.4927e-02,  6.5491e-02,  ..., -8.0310e-02,\n",
      "            1.5219e-01,  1.1623e-01],\n",
      "          [ 3.1268e-01,  1.1758e-01,  1.1420e-02,  ...,  1.4337e-01,\n",
      "            3.5595e-01, -1.4494e-01],\n",
      "          [-1.2890e-01, -1.0738e-01,  2.6023e-01,  ..., -4.0946e-02,\n",
      "            6.9806e-02, -9.3136e-02],\n",
      "          ...,\n",
      "          [-3.1635e-01, -4.5066e-01,  2.1648e-01,  ..., -4.8265e-02,\n",
      "           -1.8836e-01, -1.3195e-01],\n",
      "          [-7.2406e-02, -3.3335e-01, -1.7236e-01,  ...,  1.9480e-01,\n",
      "            1.8478e-01, -4.5556e-02],\n",
      "          [ 6.6565e-02, -3.5727e-01, -9.8946e-02,  ...,  1.8946e-01,\n",
      "           -5.4857e-02,  3.0191e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8608e-02,  7.1365e-03,  7.4730e-02,  ..., -8.3937e-02,\n",
      "           -4.2975e-02, -3.4284e-03],\n",
      "          [-6.3457e-01, -7.3459e-02, -5.1148e-01,  ...,  1.3504e-02,\n",
      "           -4.0334e-01,  2.1766e-01],\n",
      "          [ 9.7154e-03, -6.5866e-02,  2.0335e-01,  ..., -7.7259e-02,\n",
      "            9.2393e-02, -5.0889e-02],\n",
      "          ...,\n",
      "          [-2.9054e-01, -2.3318e-01,  1.1733e-01,  ...,  3.5472e-02,\n",
      "           -1.8261e-01, -5.0470e-02],\n",
      "          [-1.0572e-01, -2.4202e-01, -1.2862e-01,  ...,  1.2705e-01,\n",
      "           -8.7099e-02, -7.9065e-02],\n",
      "          [ 7.0676e-02,  1.0436e-01, -8.3094e-02,  ...,  1.0904e-01,\n",
      "            1.1369e-01,  3.0259e-01]],\n",
      "\n",
      "         [[ 8.2012e-02, -3.7515e-02,  1.0490e-01,  ..., -9.6700e-02,\n",
      "           -5.5429e-02, -3.6551e-02],\n",
      "          [ 3.5346e-03,  1.9548e-01, -2.5325e-01,  ...,  2.3110e-01,\n",
      "            3.2093e-01,  1.0272e-01],\n",
      "          [ 7.6052e-02, -4.7396e-02,  2.3380e-01,  ..., -4.0906e-02,\n",
      "            5.5735e-02, -9.6840e-02],\n",
      "          ...,\n",
      "          [-2.9091e-01, -3.1693e-01,  7.7023e-02,  ...,  3.5106e-02,\n",
      "           -2.1032e-01, -7.6424e-02],\n",
      "          [-9.8474e-02, -1.5732e-01, -1.6645e-01,  ...,  1.8334e-01,\n",
      "           -5.1533e-02, -1.2735e-01],\n",
      "          [ 9.1867e-02, -1.2525e-02, -8.3893e-02,  ...,  1.1003e-01,\n",
      "            6.2350e-02,  3.1521e-01]],\n",
      "\n",
      "         [[ 1.8475e-02,  6.0696e-03,  5.8676e-02,  ..., -9.2604e-02,\n",
      "            3.8612e-02,  3.5449e-02],\n",
      "          [-2.2473e-01,  1.1538e-01, -5.0706e-02,  ...,  3.0090e-02,\n",
      "            1.0333e-01,  2.3457e-01],\n",
      "          [ 5.8994e-04, -9.0620e-02,  3.2284e-01,  ..., -7.6351e-02,\n",
      "            1.1999e-01, -7.4880e-02],\n",
      "          ...,\n",
      "          [-2.9604e-01, -3.7701e-01,  1.1794e-01,  ...,  4.5369e-02,\n",
      "           -1.8404e-01, -1.0846e-01],\n",
      "          [-1.1454e-01, -2.2538e-01, -1.7271e-01,  ...,  1.4723e-01,\n",
      "            3.8717e-02, -8.7144e-02],\n",
      "          [ 1.2094e-01,  2.5641e-02, -1.0165e-01,  ...,  1.2382e-01,\n",
      "            4.6379e-02,  3.3639e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n",
      "            1.3576e-01,  1.0932e-01],\n",
      "          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n",
      "           -3.8492e-01,  2.4711e-01],\n",
      "          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n",
      "            1.2671e-01, -1.8432e-01],\n",
      "          ...,\n",
      "          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n",
      "           -1.7141e-01, -1.1625e-01],\n",
      "          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n",
      "            2.5598e-01, -7.1256e-02],\n",
      "          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n",
      "           -3.0645e-01,  3.7273e-01]],\n",
      "\n",
      "         [[ 2.1523e-02,  5.7400e-02,  2.8887e-02,  ..., -5.8523e-02,\n",
      "            1.4723e-01,  1.1393e-01],\n",
      "          [-4.3695e-02, -7.5245e-02, -1.4187e-01,  ...,  5.0209e-01,\n",
      "            1.1648e-01, -5.2276e-02],\n",
      "          [-4.4642e-02, -1.4096e-01,  3.9064e-01,  ..., -6.6014e-02,\n",
      "            1.2606e-01, -1.7095e-01],\n",
      "          ...,\n",
      "          [-3.0188e-01, -4.5466e-01,  1.8813e-01,  ..., -5.4116e-02,\n",
      "           -1.6290e-01, -1.4502e-01],\n",
      "          [-7.0380e-02, -3.2941e-01, -1.3598e-01,  ...,  1.7175e-01,\n",
      "            1.9076e-01, -2.4613e-02],\n",
      "          [ 8.8913e-02, -3.9729e-01, -1.1350e-01,  ...,  1.7845e-01,\n",
      "           -1.3368e-01,  3.2550e-01]],\n",
      "\n",
      "         [[-1.2164e-02,  5.4927e-02,  6.5491e-02,  ..., -8.0310e-02,\n",
      "            1.5219e-01,  1.1623e-01],\n",
      "          [ 3.1268e-01,  1.1758e-01,  1.1420e-02,  ...,  1.4337e-01,\n",
      "            3.5595e-01, -1.4494e-01],\n",
      "          [-1.2890e-01, -1.0738e-01,  2.6023e-01,  ..., -4.0946e-02,\n",
      "            6.9806e-02, -9.3136e-02],\n",
      "          ...,\n",
      "          [-3.1635e-01, -4.5066e-01,  2.1648e-01,  ..., -4.8265e-02,\n",
      "           -1.8836e-01, -1.3195e-01],\n",
      "          [-7.2406e-02, -3.3335e-01, -1.7236e-01,  ...,  1.9480e-01,\n",
      "            1.8478e-01, -4.5556e-02],\n",
      "          [ 6.6565e-02, -3.5727e-01, -9.8946e-02,  ...,  1.8946e-01,\n",
      "           -5.4857e-02,  3.0191e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3440e-02,  8.9803e-02,  1.0669e-01,  ...,  2.4471e-03,\n",
      "            1.9382e-02,  8.7499e-02],\n",
      "          [ 1.0377e-01,  5.2688e-03,  2.8455e-01,  ..., -7.6336e-03,\n",
      "            3.5762e-02,  2.7668e-01],\n",
      "          [ 1.4139e-01, -4.0515e-02,  1.9683e-01,  ..., -1.2266e-02,\n",
      "            8.2007e-03, -1.0373e-01],\n",
      "          ...,\n",
      "          [-3.0394e-01, -3.1673e-01,  1.4650e-01,  ...,  8.9574e-02,\n",
      "           -2.2106e-01, -7.7773e-02],\n",
      "          [-1.2969e-01, -1.4439e-01, -1.7820e-01,  ...,  2.3928e-01,\n",
      "            1.4061e-02, -1.2397e-01],\n",
      "          [ 1.0518e-01, -8.1988e-02, -1.1292e-01,  ...,  1.2277e-01,\n",
      "            6.2576e-02,  2.8142e-01]],\n",
      "\n",
      "         [[-1.9452e-02,  3.6092e-02,  9.5420e-02,  ..., -8.2224e-02,\n",
      "           -1.4545e-01,  5.7255e-02],\n",
      "          [-1.4081e-01,  7.7155e-02, -1.5373e-01,  ...,  2.0688e-01,\n",
      "           -6.1764e-01,  1.2072e-01],\n",
      "          [ 3.7085e-02, -1.0354e-01,  2.0362e-01,  ..., -4.1039e-02,\n",
      "            6.4273e-02, -6.3025e-02],\n",
      "          ...,\n",
      "          [-3.2408e-01, -2.9456e-01,  1.6215e-01,  ...,  7.1292e-02,\n",
      "           -2.0183e-01, -8.7325e-02],\n",
      "          [-1.0905e-01, -2.5046e-01, -1.1783e-01,  ...,  1.1119e-01,\n",
      "           -5.4611e-02, -1.8091e-02],\n",
      "          [ 9.8108e-02, -1.6410e-02, -9.3155e-02,  ...,  9.9488e-02,\n",
      "            1.0158e-01,  2.6306e-01]],\n",
      "\n",
      "         [[-1.3334e-02,  6.0280e-02,  1.2299e-01,  ..., -4.8853e-02,\n",
      "           -6.1720e-02,  1.1728e-01],\n",
      "          [ 8.7200e-02,  1.9434e-01, -2.5047e-01,  ...,  2.3342e-01,\n",
      "            3.2680e-01,  9.7086e-02],\n",
      "          [ 1.4441e-01, -4.7938e-02,  1.9726e-01,  ..., -1.0066e-02,\n",
      "            6.9686e-04, -1.1955e-01],\n",
      "          ...,\n",
      "          [-2.8720e-01, -3.3738e-01,  1.2320e-01,  ...,  7.5176e-02,\n",
      "           -2.2788e-01, -1.0572e-01],\n",
      "          [-1.1276e-01, -1.2906e-01, -1.5781e-01,  ...,  2.1370e-01,\n",
      "           -1.0275e-02, -1.2757e-01],\n",
      "          [ 1.0510e-01, -6.5813e-02, -9.2994e-02,  ...,  1.0912e-01,\n",
      "            6.9112e-02,  2.8223e-01]]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Forward hook did not obtain any outputs for given layer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m layer_baseline = torch.zeros_like(layer_input)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Shape [batch, seq_len, d_head, d_model]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m attributions = \u001b[43mcompute_layer_to_output_attributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_diff_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Calculate score based on mean over each embedding, for each token\u001b[39;00m\n\u001b[32m     40\u001b[39m per_token_score = attributions.mean(dim=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradient_causal_knowledge/attribution_methods.py:70\u001b[39m, in \u001b[36mcompute_layer_to_output_attributions\u001b[39m\u001b[34m(model, original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, metric_labels)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Attribute to the target_layer's output\u001b[39;00m\n\u001b[32m     67\u001b[39m ig_embed = LayerIntegratedGradients(\n\u001b[32m     68\u001b[39m     forward_fn, target_layer, multiply_by_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     69\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m attributions, error = \u001b[43mig_embed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mError (delta) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/log/dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:521\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    519\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_ids = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.forward_func, \u001b[33m\"\u001b[39m\u001b[33mdevice_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m inputs_layer = \u001b[43m_forward_layer_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m input_layer_list: List[Tuple[Tensor, ...]]\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# if we have one output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/_utils/gradient.py:210\u001b[39m, in \u001b[36m_forward_layer_eval\u001b[39m\u001b[34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_layer_eval\u001b[39m(\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\u001b[39;00m\n\u001b[32m    202\u001b[39m     forward_fn: Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     grad_enabled: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    209\u001b[39m ) -> Union[Tuple[Tensor, ...], List[Tuple[Tensor, ...]]]:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_forward_layer_eval_with_neuron_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[6]: For 3rd argument expected `Module` but got\u001b[39;49;00m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  `ModuleOrModuleList`.\u001b[39;49;00m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_neuron_selector\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/_utils/gradient.py:506\u001b[39m, in \u001b[36m_forward_layer_eval_with_neuron_grads\u001b[39m\u001b[34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001b[39m\n\u001b[32m    503\u001b[39m grad_enabled = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gradient_neuron_selector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_enabled\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_grad_enabled(grad_enabled):\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     saved_layer = \u001b[43m_forward_layer_distributed_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m device_ids = _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001b[32m    514\u001b[39m \u001b[38;5;66;03m# Identifies correct device ordering based on device ids.\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# key_list is a list of devices in appropriate ordering for concatenation.\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# If only one key exists (standard model), key list simply has one element.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/_utils/gradient.py:354\u001b[39m, in \u001b[36m_forward_layer_distributed_eval\u001b[39m\u001b[34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[39m\n\u001b[32m    351\u001b[39m         hook.remove()\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(saved_layer) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mForward hook did not obtain any outputs for given layer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m forward_hook_with_return:\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_layer, output\n",
      "\u001b[31mAssertionError\u001b[39m: Forward hook did not obtain any outputs for given layer"
     ]
    }
   ],
   "source": [
    "from transformer_lens.utils import get_act_name\n",
    "from attribution_methods import compute_layer_to_output_attributions\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# Standard integrated gradients with zero baseline\n",
    "\n",
    "n_samples = clean_tokens.size(0)\n",
    "\n",
    "ig_mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "ig_attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "# Calculate integrated gradients for each layer\n",
    "for layer in range(model.cfg.n_layers):\n",
    "\n",
    "    # Gradient attribution on heads\n",
    "    hook_name = get_act_name(\"result\", layer)\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    prev_layer_hook = get_act_name(\"z\", layer)\n",
    "    prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "    layer_input = clean_cache[prev_layer_hook]\n",
    "    # Use zero activations as the baseline\n",
    "    layer_baseline = torch.zeros_like(layer_input)\n",
    "\n",
    "    # Shape [batch, seq_len, d_head, d_model]\n",
    "    attributions = compute_layer_to_output_attributions(\n",
    "        model,\n",
    "        clean_tokens,\n",
    "        layer_input,\n",
    "        layer_baseline,\n",
    "        target_layer,\n",
    "        prev_layer,\n",
    "        logit_diff_metric,\n",
    "        labels,\n",
    "    )\n",
    "\n",
    "    # Calculate score based on mean over each embedding, for each token\n",
    "    per_token_score = attributions.mean(dim=3)\n",
    "    score = per_token_score.mean(dim=1)\n",
    "    ig_attn_results[:, layer] = score\n",
    "\n",
    "    # Gradient attribution on MLP neurons\n",
    "    hook_name = get_act_name(\"post\", layer)\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    prev_layer_hook = get_act_name(\"mlp_in\", layer)\n",
    "    prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "    layer_input = clean_cache[prev_layer_hook]\n",
    "    layer_baseline = torch.zeros_like(layer_input)\n",
    "\n",
    "    # Shape [batch, seq_len, d_model]\n",
    "    attributions = compute_layer_to_output_attributions(\n",
    "        model,\n",
    "        clean_tokens,\n",
    "        layer_input,\n",
    "        layer_baseline,\n",
    "        target_layer,\n",
    "        prev_layer,\n",
    "        logit_diff_metric,\n",
    "        labels,\n",
    "    )\n",
    "    score = attributions.mean(dim=1)\n",
    "    ig_mlp_results[:, layer] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9990a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation patching\n",
    "ap_mlp_results, ap_attn_results = activation_patching(model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, logit_diff_metric, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6ac77",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "To evaluate the similarity between standard integrated gradients and activation patching, we:\n",
    "\n",
    "- Visualise the attention heads highlighted by each method for the sample\n",
    "- Plot the correlation between the attribution scores\n",
    "- Measure the amount of overlap between highlighted components\n",
    "- Visualise the mean-difference plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df008f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_comparison(ig_attn_results[:3], ap_attn_results[:3], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42977a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_comparison(ig_mlp_results[:3], ap_mlp_results[:3], ig_attn_results[:3], ap_attn_results[:3], Task.IOI)\n",
    "\n",
    "mlp_corr = average_correlation(ig_mlp_results, ap_mlp_results)\n",
    "print(f\"Average correlation between MLP neuron scores: {mlp_corr}\")\n",
    "\n",
    "attn_corr = average_correlation(ig_attn_results, ap_attn_results)\n",
    "print(f\"Average correlation between attention head scores: {attn_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_attn_significant, _ = highlight_components(ig_attn_results)\n",
    "ap_attn_significant, _ = highlight_components(ap_attn_results)\n",
    "\n",
    "plot_attn_comparison(ig_attn_significant[:3], ap_attn_significant[:3], model)\n",
    "\n",
    "attn_overlap = measure_overlap(ig_attn_significant, ap_attn_significant)\n",
    "print(f\"Overlap between IG and AP highlighted attention heads: {attn_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_mlp_significant, _ = highlight_components(ig_mlp_results)\n",
    "ap_mlp_significant, _ = highlight_components(ap_mlp_results)\n",
    "\n",
    "mlp_overlap = measure_overlap(ig_mlp_significant, ap_mlp_significant)\n",
    "print(f\"Overlap between IG and AP highlighted MLP neurons: {mlp_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_diff(ig_mlp_results, ap_mlp_results, \"Mean-difference plot for neurons in IOI task\")\n",
    "plot_mean_diff(ig_attn_results, ap_attn_results, \"Mean-difference plot for attention heads in IOI task\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
