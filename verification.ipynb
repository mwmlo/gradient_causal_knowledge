{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8138abe5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Gradient-based Verification of Circuits\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3cd0593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7ed81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Task(Enum):\n",
    "    IOI = 1\n",
    "    GENDER_BIAS = 2\n",
    "    GREATER_THAN = 3\n",
    "    CAPITAL_COUNTRY = 4\n",
    "    SVA = 5\n",
    "    HYPERNYMY = 6\n",
    "\n",
    "# Implementation of dataset loader based on https://github.com/hannamw/eap-ig-faithfulness\n",
    "\n",
    "def collate_EAP(xs, task: Task):\n",
    "    clean, corrupted, labels = zip(*xs)\n",
    "    clean = list(clean)\n",
    "    corrupted = list(corrupted)\n",
    "    if task != Task.HYPERNYMY:\n",
    "        labels = torch.tensor(labels)\n",
    "    return clean, corrupted, labels\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, task: Task):\n",
    "        filename = task.name.lower()\n",
    "        self.task = task\n",
    "        self.df = pd.read_csv(f'datasets/{filename}.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def head(self, n: int):\n",
    "        self.df = self.df.head(n)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        label = None\n",
    "\n",
    "        if self.task == Task.IOI:\n",
    "            label = [row['correct_idx'], row['incorrect_idx']]\n",
    "            return row['clean'], row['corrupted_hard'], label\n",
    "        \n",
    "        if self.task == Task.GREATER_THAN:\n",
    "            label = row['correct_idx']\n",
    "        elif self.task == Task.HYPERNYMY:\n",
    "            answer = torch.tensor(eval(row['answers_idx']))\n",
    "            corrupted_answer = torch.tensor(eval(row['corrupted_answers_idx']))\n",
    "            label = [answer, corrupted_answer]\n",
    "        elif self.task == Task.CAPITAL_COUNTRY:\n",
    "            label = [row['country_idx'], row['corrupted_country_idx']]\n",
    "        elif self.task == Task.GENDER_BIAS:\n",
    "            label = [row['clean_answer_idx'], row['corrupted_answer_idx']]\n",
    "        elif self.task == Task.SVA:\n",
    "            label = row['plural']\n",
    "        else:\n",
    "            raise ValueError(f'Got invalid task: {self.task}')\n",
    "        \n",
    "        return row['clean'], row['corrupted'], label\n",
    "    \n",
    "    def to_dataloader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=partial(collate_EAP, task=self.task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60d7a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_metric(logits, metric_labels):\n",
    "    correct_index = metric_labels[:, 0]\n",
    "    incorrect_index = metric_labels[:, 1]\n",
    "    logits_last = logits[:, -1, :]\n",
    "    batch_size = logits.size(0)\n",
    "    correct_logits = logits_last[torch.arange(batch_size), correct_index]\n",
    "    incorrect_logits = logits_last[torch.arange(batch_size), incorrect_index]\n",
    "    return correct_logits - incorrect_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec2e3f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e071c4",
   "metadata": {},
   "source": [
    "# Ablation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e395976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-0.0307, -0.9269, -0.4937,  2.2320,  0.6754,  4.0447, -0.1785,  1.1947,\n",
      "         1.1514,  1.7507], device='cuda:0')\n",
      "Corrupted logit difference: tensor([-0.0387, -0.9451, -0.5103,  2.2153,  0.6299, -3.2074, -0.1823,  1.1766,\n",
      "        -3.0072,  1.7392], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=10)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_ig_zero_mlp = torch.load(\"saved_results/ioi_ig_zero_mlp.pt\")\n",
    "ioi_ig_zero_attn = torch.load(\"saved_results/ioi_ig_zero_attn.pt\")\n",
    "\n",
    "ioi_ig_mlp = torch.load(\"saved_results/ioi_ig_mlp.pt\")\n",
    "ioi_ig_attn = torch.load(\"saved_results/ioi_ig_attn.pt\")\n",
    "\n",
    "ioi_ap_mlp = torch.load(\"saved_results/ioi_ap_mlp.pt\")\n",
    "ioi_ap_attn = torch.load(\"saved_results/ioi_ap_attn.pt\")\n",
    "\n",
    "ig_mlp, ap_mlp, ig_attn, ap_attn = ioi_ig_mlp[0], ioi_ap_mlp[0], ioi_ig_attn[0], ioi_ap_attn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2852cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_insignificant(results):\n",
    "    # Only keep components which are above a baseline\n",
    "    mean = torch.mean(results)\n",
    "    std = torch.std(results)\n",
    "    return torch.where(results.abs() > mean + std, results, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33a7759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablated_attn(model: HookedTransformer, layer_idx, head_idx, corrupted_cache, metric, metric_labels, *model_args, **model_kwargs):\n",
    "    # Patch in corrupted activations\n",
    "    def ablate_head_hook(act, hook):\n",
    "        act[:, :, head_idx] = corrupted_cache[hook.name][:, :, head_idx]\n",
    "        return act\n",
    "    \n",
    "    layer_name = get_act_name(\"result\", layer_idx)\n",
    "\n",
    "    logits = model.run_with_hooks(*model_args, **model_kwargs, fwd_hooks=[(layer_name, ablate_head_hook)])\n",
    "    return metric(logits, metric_labels)\n",
    "\n",
    "def run_ablated_neuron(model, layer_idx, neuron_idx, corrupted_cache, metric, metric_labels, *model_args, **model_kwargs):\n",
    "    ablate_head_hook = lambda act, hook: corrupted_cache[hook.name][:, :, neuron_idx]\n",
    "    layer_name = get_act_name(\"post\", layer_idx)\n",
    "\n",
    "    logits = model.run_with_hooks(*model_args, **model_kwargs, fwd_hooks=[(layer_name, ablate_head_hook)])\n",
    "    return metric(logits, metric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb38b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ablated_ioi_performance(layer_idx, pos, is_attn: bool, n_samples=1000):\n",
    "    test_dataset = TaskDataset(Task.IOI)\n",
    "    test_dataloader = test_dataset.to_dataloader(batch_size=n_samples)\n",
    "    clean_input, corrupted_input, labels = next(iter(test_dataloader))\n",
    "\n",
    "    clean_tokens = model.to_tokens(clean_input)\n",
    "    corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "    _, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "    \n",
    "    if is_attn:\n",
    "        performance = run_ablated_attn(model, layer_idx, pos, corrupted_cache, logit_diff_metric, labels, clean_tokens)\n",
    "    else:\n",
    "        performance = run_ablated_neuron(model, layer_idx, pos, corrupted_cache, logit_diff_metric, labels, clean_tokens)\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88ab291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "ap_attn_sig = torch.nonzero(mask_insignificant(ap_attn)) # Shape [n_indices, 2]\n",
    "\n",
    "performances = []\n",
    "for layer_idx, head_idx in ap_attn_sig:\n",
    "    performance = test_ablated_ioi_performance(layer_idx, head_idx, is_attn=True, n_samples=2)\n",
    "    performances.append(performance)\n",
    "\n",
    "performances = torch.stack(performances).mean(dim=-1)\n",
    "print(performances.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
