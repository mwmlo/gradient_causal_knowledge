{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e36db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from testing import logit_diff_metric\n",
    "from applications.pipeline import run_attribution_steps, identify_target_components, optimise_edit_components\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0e1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca4d4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The mother tongue of Danielle Darrieux is']\n",
      "['The mother tongue of Paul McCartney is']\n",
      "tensor([[24111, 15823]])\n",
      "Clean logit difference: tensor([0.1160], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "Corrupted logit difference: tensor([-1.1990], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Verify that loading works, for one example\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=1)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(counterfact_dataloader))\n",
    "\n",
    "print(clean_input)\n",
    "print(corrupted_input)\n",
    "print(labels)\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b51f055",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Baseline can be provided as a tensor for just one input and broadcasted to the batch or input and baseline must have the same shape or the baseline corresponding to each input tensor must be a scalar. Found baseline: tensor([[[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n            1.3576e-01,  1.0932e-01],\n          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n           -3.8492e-01,  2.4711e-01],\n          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n            1.2671e-01, -1.8432e-01],\n          ...,\n          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n           -1.7141e-01, -1.1625e-01],\n          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n            2.5598e-01, -7.1256e-02],\n          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n           -3.0645e-01,  3.7273e-01]],\n\n         [[ 5.6135e-02,  5.2765e-02,  8.7760e-03,  ..., -4.0619e-02,\n            1.3522e-01,  9.8149e-02],\n          [ 2.6660e-01,  6.5456e-03,  2.5709e-02,  ...,  1.8073e-01,\n            1.1111e-01,  1.3402e-01],\n          [-4.8208e-02, -1.4902e-01,  3.7220e-01,  ..., -6.3423e-02,\n            1.3125e-01, -1.6714e-01],\n          ...,\n          [-3.3139e-01, -4.4216e-01,  1.5671e-01,  ..., -5.7046e-02,\n           -1.5611e-01, -1.1741e-01],\n          [-1.2947e-01, -3.6367e-01, -2.0415e-01,  ...,  1.7347e-01,\n            2.6245e-01, -3.8326e-02],\n          [ 1.0529e-01, -3.8342e-01, -1.3472e-01,  ...,  1.8566e-01,\n           -1.9970e-01,  3.2450e-01]],\n\n         [[ 2.2397e-02,  4.0383e-02,  7.2053e-02,  ..., -1.7661e-02,\n            1.0304e-01,  4.1182e-02],\n          [ 1.6782e-01, -1.9282e-01,  9.4300e-02,  ...,  2.8860e-01,\n           -6.3679e-02,  1.7539e-01],\n          [-6.8276e-02, -1.6953e-01,  3.4273e-01,  ..., -7.1643e-02,\n            1.4635e-01, -1.4183e-01],\n          ...,\n          [-3.3865e-01, -4.1659e-01,  1.5499e-01,  ..., -5.7295e-02,\n           -1.5056e-01, -1.0963e-01],\n          [-1.2201e-01, -4.4275e-01, -1.8614e-01,  ...,  7.5768e-02,\n            2.1584e-01, -5.0138e-03],\n          [ 8.6065e-02, -2.1674e-01, -1.0654e-01,  ...,  1.5300e-01,\n           -1.4707e-01,  3.2081e-01]],\n\n         ...,\n\n         [[ 6.4724e-02, -4.8898e-03,  1.1926e-01,  ..., -3.0955e-02,\n            4.4494e-02,  5.0276e-02],\n          [-2.5488e-01, -1.5862e-01, -3.1766e-01,  ...,  1.9324e-01,\n           -2.3306e-01, -2.6534e-02],\n          [ 2.0906e-02, -1.1226e-01,  3.4931e-01,  ..., -2.2410e-02,\n            9.0138e-02, -1.0550e-01],\n          ...,\n          [-2.8575e-01, -2.9427e-01,  1.1049e-01,  ..., -8.5844e-03,\n           -6.3982e-02, -7.3881e-02],\n          [-8.0222e-02, -3.0836e-01, -9.6145e-02,  ...,  8.1953e-02,\n           -7.9909e-02, -3.2124e-02],\n          [ 8.8678e-02, -3.5062e-02, -9.9833e-02,  ...,  1.2702e-01,\n           -5.0877e-03,  3.4614e-01]],\n\n         [[ 8.0942e-02, -1.7746e-02,  1.2092e-01,  ..., -2.1856e-02,\n            4.9897e-02,  2.3710e-02],\n          [ 1.8996e-01,  2.1745e-01, -3.2437e-02,  ..., -2.5151e-02,\n           -9.7690e-02,  1.1364e-01],\n          [ 1.7757e-02, -8.9753e-02,  3.3869e-01,  ..., -4.9914e-02,\n            1.1590e-01, -9.6719e-02],\n          ...,\n          [-3.2242e-01, -2.5883e-01,  1.4814e-01,  ..., -9.8169e-03,\n           -1.3618e-01, -8.9883e-02],\n          [-1.2651e-01, -2.9261e-01, -1.3029e-01,  ..., -2.2379e-02,\n            4.8556e-03, -3.2084e-02],\n          [ 8.3148e-02, -1.4907e-01, -1.0640e-01,  ...,  1.2983e-01,\n           -2.8610e-02,  3.3631e-01]],\n\n         [[ 7.1720e-02,  6.8320e-03,  9.2763e-02,  ..., -3.3083e-02,\n            8.7794e-02,  3.8504e-02],\n          [ 8.2080e-02,  1.0938e-01, -6.7033e-02,  ...,  1.2012e-02,\n            1.2315e-01,  2.2886e-01],\n          [-3.1729e-02, -1.3783e-01,  3.9982e-01,  ..., -5.1877e-02,\n            1.4156e-01, -1.1621e-01],\n          ...,\n          [-3.2167e-01, -3.6756e-01,  1.6634e-01,  ...,  1.1999e-02,\n           -1.3713e-01, -1.2182e-01],\n          [-1.4868e-01, -3.5158e-01, -1.6904e-01,  ...,  6.7471e-02,\n            1.5907e-01, -3.8502e-03],\n          [ 1.4103e-01, -1.4993e-01, -1.8868e-01,  ...,  1.8337e-01,\n           -5.2561e-02,  3.9406e-01]]],\n\n\n        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n            1.3576e-01,  1.0932e-01],\n          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n           -3.8492e-01,  2.4711e-01],\n          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n            1.2671e-01, -1.8432e-01],\n          ...,\n          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n           -1.7141e-01, -1.1625e-01],\n          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n            2.5598e-01, -7.1256e-02],\n          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n           -3.0645e-01,  3.7273e-01]],\n\n         [[ 5.6135e-02,  5.2765e-02,  8.7760e-03,  ..., -4.0619e-02,\n            1.3522e-01,  9.8149e-02],\n          [ 2.6660e-01,  6.5456e-03,  2.5709e-02,  ...,  1.8073e-01,\n            1.1111e-01,  1.3402e-01],\n          [-4.8208e-02, -1.4902e-01,  3.7220e-01,  ..., -6.3423e-02,\n            1.3125e-01, -1.6714e-01],\n          ...,\n          [-3.3139e-01, -4.4216e-01,  1.5671e-01,  ..., -5.7046e-02,\n           -1.5611e-01, -1.1741e-01],\n          [-1.2947e-01, -3.6367e-01, -2.0415e-01,  ...,  1.7347e-01,\n            2.6245e-01, -3.8326e-02],\n          [ 1.0529e-01, -3.8342e-01, -1.3472e-01,  ...,  1.8566e-01,\n           -1.9970e-01,  3.2450e-01]],\n\n         [[ 3.7676e-02,  4.1196e-02,  6.9550e-02,  ..., -6.6748e-02,\n            1.2939e-01,  5.9925e-02],\n          [-1.6487e-02, -2.1374e-01, -2.9048e-02,  ...,  2.6674e-01,\n            1.6551e-01,  1.5811e-01],\n          [-5.7450e-02, -1.6644e-01,  3.4600e-01,  ..., -5.3515e-02,\n            1.3963e-01, -1.2742e-01],\n          ...,\n          [-3.3570e-01, -3.8015e-01,  1.6264e-01,  ..., -4.4775e-02,\n           -1.5235e-01, -8.9518e-02],\n          [-2.0001e-02, -2.8253e-01, -1.5684e-01,  ...,  1.0784e-01,\n            1.1128e-01, -6.7402e-02],\n          [ 1.2750e-01, -3.3137e-01, -1.7557e-01,  ...,  1.7779e-01,\n           -1.4017e-01,  3.3005e-01]],\n\n         ...,\n\n         [[ 4.9619e-02, -1.0999e-02,  7.5101e-02,  ..., -1.2517e-01,\n            4.7757e-02,  3.9624e-02],\n          [ 2.5300e-03,  1.7440e-01, -9.4192e-03,  ...,  3.1506e-02,\n            2.5101e-02,  2.2121e-01],\n          [ 6.8708e-03, -1.3345e-01,  3.1822e-01,  ..., -5.4231e-02,\n            1.2632e-01, -8.2346e-02],\n          ...,\n          [-3.2515e-01, -2.6622e-01,  1.2463e-01,  ..., -8.7309e-03,\n           -1.3990e-01, -9.7888e-02],\n          [ 3.2924e-03, -1.5808e-01, -1.0563e-01,  ...,  6.1098e-02,\n           -1.1146e-02, -7.0645e-02],\n          [ 1.1333e-01, -1.2933e-01, -1.1946e-01,  ...,  1.3591e-01,\n           -1.6498e-02,  3.5188e-01]],\n\n         [[ 4.3119e-02, -5.9800e-02,  1.1382e-01,  ..., -1.7368e-01,\n            1.0856e-01,  1.3357e-02],\n          [-3.0208e-01, -1.3085e-01,  2.6735e-01,  ...,  2.2674e-01,\n           -1.0058e-01, -2.0607e-01],\n          [ 4.3683e-03, -1.5895e-01,  2.7759e-01,  ..., -5.7834e-02,\n            1.2631e-01, -1.1445e-01],\n          ...,\n          [-3.1207e-01, -3.2192e-01,  1.2122e-01,  ..., -3.1067e-03,\n           -1.3828e-01, -1.0901e-01],\n          [-6.9010e-02, -3.4290e-01, -3.9674e-02,  ...,  7.8374e-02,\n            5.5379e-03,  2.7121e-02],\n          [ 9.8452e-02, -6.0071e-02, -1.3188e-01,  ...,  1.3406e-01,\n            6.8523e-02,  3.4852e-01]],\n\n         [[ 6.4684e-02, -3.2300e-02,  7.5071e-02,  ..., -1.1745e-01,\n            9.3861e-02,  2.1488e-02],\n          [ 1.6433e-01,  2.1547e-01, -4.0457e-02,  ..., -2.1143e-02,\n           -1.0321e-01,  1.1648e-01],\n          [ 5.5406e-02, -9.5885e-02,  3.2056e-01,  ..., -5.6154e-02,\n            1.2361e-01, -1.0570e-01],\n          ...,\n          [-3.4844e-01, -2.5039e-01,  1.3747e-01,  ...,  7.9164e-05,\n           -1.7358e-01, -1.0579e-01],\n          [-1.0557e-01, -2.3877e-01, -1.4631e-01,  ..., -3.3431e-02,\n           -5.0332e-02, -5.7785e-02],\n          [ 1.0105e-01, -1.6895e-01, -1.2693e-01,  ...,  1.4249e-01,\n            1.3330e-02,  3.3134e-01]]],\n\n\n        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n            1.3576e-01,  1.0932e-01],\n          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n           -3.8492e-01,  2.4711e-01],\n          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n            1.2671e-01, -1.8432e-01],\n          ...,\n          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n           -1.7141e-01, -1.1625e-01],\n          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n            2.5598e-01, -7.1256e-02],\n          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n           -3.0645e-01,  3.7273e-01]],\n\n         [[ 6.1459e-02,  6.2121e-02, -2.8548e-02,  ..., -3.7793e-02,\n            1.2494e-01,  1.0063e-01],\n          [ 2.0101e-01, -5.9048e-02, -2.1680e-02,  ...,  4.5949e-01,\n            1.9642e-01,  4.2402e-02],\n          [-4.5368e-02, -1.2999e-01,  3.5952e-01,  ..., -6.8466e-02,\n            1.1048e-01, -1.7313e-01],\n          ...,\n          [-3.2417e-01, -4.4840e-01,  1.6240e-01,  ..., -1.5891e-02,\n           -1.4142e-01, -7.4223e-02],\n          [-1.0908e-01, -3.6107e-01, -1.7821e-01,  ...,  1.3265e-01,\n            2.3327e-01, -6.7998e-02],\n          [ 1.1848e-01, -4.2342e-01, -1.5293e-01,  ...,  1.5777e-01,\n           -2.0917e-01,  3.3538e-01]],\n\n         [[ 4.8368e-02,  7.0170e-02, -5.6113e-02,  ..., -5.3152e-02,\n            8.0512e-02,  8.4481e-02],\n          [-2.2859e-01,  2.1869e-02,  3.6398e-02,  ...,  8.5735e-02,\n            1.3093e-01, -8.4808e-02],\n          [-3.9078e-02, -1.2918e-01,  3.2743e-01,  ..., -6.1834e-02,\n            1.0285e-01, -1.5874e-01],\n          ...,\n          [-3.2902e-01, -4.3611e-01,  1.6472e-01,  ..., -1.7701e-02,\n           -1.3316e-01, -6.8558e-02],\n          [-4.9892e-02, -3.5500e-01, -1.4411e-01,  ...,  1.3653e-01,\n            1.3581e-01, -1.0052e-01],\n          [ 1.1163e-01, -3.6817e-01, -1.5165e-01,  ...,  1.5272e-01,\n           -1.7209e-01,  3.2340e-01]],\n\n         ...,\n\n         [[ 6.1552e-02,  9.7966e-02, -6.2787e-02,  ..., -4.0116e-02,\n            1.4242e-01,  8.2613e-02],\n          [ 1.3697e-01,  1.0831e-01, -5.7050e-02,  ...,  2.3917e-02,\n            1.4825e-01,  2.2475e-01],\n          [-1.2342e-01, -1.4249e-01,  3.8880e-01,  ..., -7.2036e-02,\n            1.1983e-01, -1.4325e-01],\n          ...,\n          [-3.1303e-01, -4.9957e-01,  2.1792e-01,  ...,  6.5372e-02,\n           -1.3064e-01, -9.1173e-02],\n          [-1.6771e-01, -3.6374e-01, -1.9052e-01,  ...,  1.4985e-01,\n            2.3929e-01, -2.6730e-02],\n          [ 2.2840e-01, -3.4029e-01, -3.3357e-01,  ...,  2.9052e-01,\n           -2.0867e-01,  4.4150e-01]],\n\n         [[ 5.9767e-02,  1.0052e-01, -7.7298e-02,  ..., -4.1897e-02,\n            1.7159e-01,  9.3002e-02],\n          [ 1.2573e-01,  1.0747e-01, -5.5569e-02,  ...,  2.7017e-02,\n            1.5030e-01,  2.2532e-01],\n          [-1.3033e-01, -1.5881e-01,  4.3875e-01,  ..., -6.9282e-02,\n            1.3641e-01, -1.6496e-01],\n          ...,\n          [-3.1057e-01, -5.1790e-01,  2.1481e-01,  ...,  6.9270e-02,\n           -1.2908e-01, -1.0996e-01],\n          [-1.8284e-01, -3.7971e-01, -2.0292e-01,  ...,  1.3540e-01,\n            2.6760e-01, -1.1640e-02],\n          [ 2.5303e-01, -3.5602e-01, -3.6007e-01,  ...,  3.0982e-01,\n           -2.3017e-01,  4.6528e-01]],\n\n         [[ 5.7735e-02,  1.0278e-01, -8.5991e-02,  ..., -4.1738e-02,\n            1.9132e-01,  1.0020e-01],\n          [ 1.1321e-01,  1.0665e-01, -5.6380e-02,  ...,  2.9411e-02,\n            1.4955e-01,  2.2620e-01],\n          [-1.3577e-01, -1.7232e-01,  4.7817e-01,  ..., -6.6842e-02,\n            1.4873e-01, -1.8249e-01],\n          ...,\n          [-3.0780e-01, -5.3170e-01,  2.1181e-01,  ...,  7.2728e-02,\n           -1.2800e-01, -1.2404e-01],\n          [-1.9340e-01, -3.9288e-01, -2.1265e-01,  ...,  1.2336e-01,\n            2.8932e-01,  1.7411e-04],\n          [ 2.7039e-01, -3.6830e-01, -3.7865e-01,  ...,  3.2212e-01,\n           -2.4867e-01,  4.8173e-01]]]], device='cuda:0') and input: tensor([[[[ 0.0632,  0.0598, -0.0180,  ..., -0.0313,  0.1358,  0.1093],\n          [ 0.1858,  0.2344, -0.2453,  ..., -0.5324, -0.3849,  0.2471],\n          [-0.0400, -0.1362,  0.3895,  ..., -0.0678,  0.1267, -0.1843],\n          ...,\n          [-0.2830, -0.5038,  0.1617,  ..., -0.0491, -0.1714, -0.1162],\n          [-0.0565, -0.4134, -0.2071,  ...,  0.1924,  0.2560, -0.0713],\n          [ 0.1551, -0.5138, -0.1732,  ...,  0.1516, -0.3064,  0.3727]],\n\n         [[ 0.0561,  0.0528,  0.0088,  ..., -0.0406,  0.1352,  0.0981],\n          [ 0.2666,  0.0065,  0.0257,  ...,  0.1807,  0.1111,  0.1340],\n          [-0.0482, -0.1490,  0.3722,  ..., -0.0634,  0.1313, -0.1671],\n          ...,\n          [-0.3314, -0.4422,  0.1567,  ..., -0.0570, -0.1561, -0.1174],\n          [-0.1295, -0.3637, -0.2042,  ...,  0.1735,  0.2624, -0.0383],\n          [ 0.1053, -0.3834, -0.1347,  ...,  0.1857, -0.1997,  0.3245]],\n\n         [[ 0.0224,  0.0404,  0.0721,  ..., -0.0177,  0.1030,  0.0412],\n          [ 0.1678, -0.1928,  0.0943,  ...,  0.2886, -0.0637,  0.1754],\n          [-0.0683, -0.1695,  0.3427,  ..., -0.0716,  0.1464, -0.1418],\n          ...,\n          [-0.3386, -0.4166,  0.1550,  ..., -0.0573, -0.1506, -0.1096],\n          [-0.1220, -0.4427, -0.1861,  ...,  0.0758,  0.2158, -0.0050],\n          [ 0.0861, -0.2167, -0.1065,  ...,  0.1530, -0.1471,  0.3208]],\n\n         ...,\n\n         [[ 0.0390, -0.0059,  0.0447,  ..., -0.0441,  0.1086,  0.0771],\n          [ 0.1154,  0.1087, -0.0581,  ...,  0.0240,  0.1421,  0.2260],\n          [-0.0464, -0.1620,  0.4461,  ..., -0.0695,  0.1694, -0.1567],\n          ...,\n          [-0.3399, -0.4017,  0.1574,  ...,  0.0261, -0.1458, -0.1475],\n          [-0.1677, -0.3812, -0.2123,  ...,  0.0474,  0.2063, -0.0148],\n          [ 0.2072, -0.2253, -0.2902,  ...,  0.2524, -0.1484,  0.4360]],\n\n         [[ 0.0401,  0.0144,  0.0147,  ..., -0.0430,  0.1390,  0.0875],\n          [ 0.1022,  0.1072, -0.0583,  ...,  0.0282,  0.1439,  0.2270],\n          [-0.0644, -0.1743,  0.4826,  ..., -0.0669,  0.1756, -0.1744],\n          ...,\n          [-0.3308, -0.4403,  0.1651,  ...,  0.0391, -0.1413, -0.1550],\n          [-0.1791, -0.3947, -0.2208,  ...,  0.0462,  0.2348, -0.0043],\n          [ 0.2339, -0.2616, -0.3218,  ...,  0.2757, -0.1817,  0.4593]],\n\n         [[ 0.0416,  0.0292, -0.0095,  ..., -0.0427,  0.1646,  0.0950],\n          [ 0.0877,  0.1075, -0.0574,  ...,  0.0300,  0.1454,  0.2279],\n          [-0.0788, -0.1834,  0.5108,  ..., -0.0656,  0.1801, -0.1887],\n          ...,\n          [-0.3247, -0.4666,  0.1696,  ...,  0.0464, -0.1377, -0.1605],\n          [-0.1885, -0.4046, -0.2280,  ...,  0.0453,  0.2583,  0.0038],\n          [ 0.2535, -0.2840, -0.3431,  ...,  0.2916, -0.2048,  0.4742]]],\n\n\n        [[[ 0.0632,  0.0598, -0.0180,  ..., -0.0313,  0.1358,  0.1093],\n          [ 0.1858,  0.2344, -0.2453,  ..., -0.5324, -0.3849,  0.2471],\n          [-0.0400, -0.1362,  0.3895,  ..., -0.0678,  0.1267, -0.1843],\n          ...,\n          [-0.2830, -0.5038,  0.1617,  ..., -0.0491, -0.1714, -0.1162],\n          [-0.0565, -0.4134, -0.2071,  ...,  0.1924,  0.2560, -0.0713],\n          [ 0.1551, -0.5138, -0.1732,  ...,  0.1516, -0.3064,  0.3727]],\n\n         [[ 0.0561,  0.0528,  0.0088,  ..., -0.0406,  0.1352,  0.0981],\n          [ 0.2666,  0.0065,  0.0257,  ...,  0.1807,  0.1111,  0.1340],\n          [-0.0482, -0.1490,  0.3722,  ..., -0.0634,  0.1313, -0.1671],\n          ...,\n          [-0.3314, -0.4422,  0.1567,  ..., -0.0570, -0.1561, -0.1174],\n          [-0.1295, -0.3637, -0.2042,  ...,  0.1735,  0.2624, -0.0383],\n          [ 0.1053, -0.3834, -0.1347,  ...,  0.1857, -0.1997,  0.3245]],\n\n         [[ 0.0377,  0.0412,  0.0695,  ..., -0.0667,  0.1294,  0.0599],\n          [-0.0165, -0.2137, -0.0290,  ...,  0.2667,  0.1655,  0.1581],\n          [-0.0575, -0.1664,  0.3460,  ..., -0.0535,  0.1396, -0.1274],\n          ...,\n          [-0.3357, -0.3801,  0.1626,  ..., -0.0448, -0.1524, -0.0895],\n          [-0.0200, -0.2825, -0.1568,  ...,  0.1078,  0.1113, -0.0674],\n          [ 0.1275, -0.3314, -0.1756,  ...,  0.1778, -0.1402,  0.3300]],\n\n         ...,\n\n         [[-0.0097,  0.0493,  0.0675,  ...,  0.0189, -0.0203,  0.0910],\n          [ 0.0876,  0.2347, -0.3979,  ..., -0.0596,  0.1815,  0.0359],\n          [ 0.0124, -0.0730,  0.2740,  ..., -0.0552,  0.0996, -0.0772],\n          ...,\n          [-0.3377, -0.2410,  0.1608,  ..., -0.0502, -0.0979, -0.0935],\n          [-0.1080, -0.1970, -0.1752,  ...,  0.0729,  0.0166, -0.0993],\n          [ 0.0906, -0.1237, -0.1725,  ...,  0.1998, -0.0151,  0.3163]],\n\n         [[-0.0240,  0.0152,  0.0722,  ..., -0.0323,  0.0153,  0.0777],\n          [-0.0253,  0.1407, -0.1750,  ...,  0.0721, -0.2685, -0.1088],\n          [ 0.0451, -0.0695,  0.2314,  ..., -0.0395,  0.0745, -0.0457],\n          ...,\n          [-0.3236, -0.2250,  0.1637,  ..., -0.0363, -0.0888, -0.0859],\n          [-0.0778, -0.1826, -0.1689,  ...,  0.0864, -0.0321, -0.0719],\n          [ 0.0875, -0.1289, -0.1605,  ...,  0.1928, -0.0261,  0.3056]],\n\n         [[ 0.0227,  0.0337,  0.0462,  ..., -0.0308,  0.0519,  0.0793],\n          [ 0.1303,  0.2213, -0.0339,  ..., -0.0225, -0.1041,  0.1170],\n          [ 0.0395, -0.1052,  0.2418,  ..., -0.0531,  0.0944, -0.0917],\n          ...,\n          [-0.3438, -0.1803,  0.1702,  ..., -0.0141, -0.1516, -0.0745],\n          [-0.1168, -0.2029, -0.1669,  ..., -0.0287, -0.0602, -0.0956],\n          [ 0.0929, -0.1973, -0.1562,  ...,  0.1743, -0.0282,  0.3170]]],\n\n\n        [[[ 0.0632,  0.0598, -0.0180,  ..., -0.0313,  0.1358,  0.1093],\n          [ 0.1858,  0.2344, -0.2453,  ..., -0.5324, -0.3849,  0.2471],\n          [-0.0400, -0.1362,  0.3895,  ..., -0.0678,  0.1267, -0.1843],\n          ...,\n          [-0.2830, -0.5038,  0.1617,  ..., -0.0491, -0.1714, -0.1162],\n          [-0.0565, -0.4134, -0.2071,  ...,  0.1924,  0.2560, -0.0713],\n          [ 0.1551, -0.5138, -0.1732,  ...,  0.1516, -0.3064,  0.3727]],\n\n         [[ 0.0626,  0.0688, -0.0081,  ..., -0.0430,  0.1256,  0.0931],\n          [ 0.0056, -0.0056, -0.0125,  ...,  0.3735, -0.0373,  0.1431],\n          [-0.0324, -0.1349,  0.3851,  ..., -0.0637,  0.1241, -0.1818],\n          ...,\n          [-0.3141, -0.4468,  0.1448,  ..., -0.0364, -0.1514, -0.1081],\n          [-0.0845, -0.3957, -0.1217,  ...,  0.1417,  0.2262, -0.0501],\n          [ 0.1276, -0.4066, -0.1258,  ...,  0.2022, -0.2168,  0.3499]],\n\n         [[ 0.0538,  0.0695,  0.0071,  ..., -0.0680,  0.0800,  0.0669],\n          [ 0.1445, -0.0442, -0.1842,  ...,  0.3799,  0.1650,  0.0277],\n          [-0.0213, -0.1298,  0.3684,  ..., -0.0589,  0.1145, -0.1673],\n          ...,\n          [-0.3241, -0.3642,  0.1712,  ..., -0.0419, -0.1588, -0.1205],\n          [-0.0851, -0.3901, -0.1720,  ...,  0.0625,  0.1336, -0.0081],\n          [ 0.1281, -0.4012, -0.1481,  ...,  0.1854, -0.2014,  0.3665]],\n\n         ...,\n\n         [[ 0.0579,  0.0904, -0.0220,  ..., -0.0799,  0.1815,  0.0354],\n          [ 0.1256,  0.1074, -0.0555,  ...,  0.0274,  0.1500,  0.2253],\n          [-0.1137, -0.1721,  0.4689,  ..., -0.0657,  0.1387, -0.1832],\n          ...,\n          [-0.3110, -0.5115,  0.1993,  ...,  0.0593, -0.1486, -0.1371],\n          [-0.1789, -0.3888, -0.1912,  ...,  0.1167,  0.2705, -0.0022],\n          [ 0.2536, -0.3351, -0.3439,  ...,  0.3073, -0.2030,  0.4756]],\n\n         [[ 0.0567,  0.0939, -0.0378,  ..., -0.0741,  0.1982,  0.0499],\n          [ 0.1131,  0.1066, -0.0563,  ...,  0.0297,  0.1494,  0.2262],\n          [-0.1213, -0.1830,  0.5026,  ..., -0.0639,  0.1501, -0.1974],\n          ...,\n          [-0.3082, -0.5263,  0.1989,  ...,  0.0647, -0.1440, -0.1461],\n          [-0.1899, -0.4000, -0.2034,  ...,  0.1079,  0.2919,  0.0076],\n          [ 0.2701, -0.3495, -0.3639,  ...,  0.3195, -0.2242,  0.4902]],\n\n         [[ 0.0553,  0.0963, -0.0522,  ..., -0.0694,  0.2126,  0.0628],\n          [ 0.0992,  0.1069, -0.0560,  ...,  0.0309,  0.1494,  0.2271],\n          [-0.1276, -0.1907,  0.5278,  ..., -0.0630,  0.1581, -0.2081],\n          ...,\n          [-0.3064, -0.5369,  0.1978,  ...,  0.0672, -0.1403, -0.1529],\n          [-0.1987, -0.4083, -0.2131,  ...,  0.1004,  0.3091,  0.0149],\n          [ 0.2830, -0.3579, -0.3774,  ...,  0.3279, -0.2381,  0.4997]]]],\n       device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m rewrite_logit_diff = logit_diff_metric(rewrite_logits, labels)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# LOCALISATION STAGE\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m mlp_highlighted, attn_highlighted = \u001b[43mrun_attribution_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrewrite_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrewrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_logit_diff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrewrite_logit_diff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m target_mlp = identify_target_components(model, mlp_highlighted)\n\u001b[32m     26\u001b[39m target_attn = identify_target_components(model, attn_highlighted)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradient_causal_knowledge/applications/pipeline.py:44\u001b[39m, in \u001b[36mrun_attribution_steps\u001b[39m\u001b[34m(model, original_tokens, rewrite_tokens, answer_labels, original_cache, rewrite_cache, original_logit_diff, rewrite_logit_diff)\u001b[39m\n\u001b[32m     41\u001b[39m attn_attribution_highlights = \u001b[38;5;28mdict\u001b[39m()\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Run integrated gradients with original baseline and rewrite input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m ig_original_rewrite_mlp, ig_original_rewrite_attn = \u001b[43mintegrated_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrewrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogit_diff_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m mlp_attribution_highlights[AttributionMethod.IG_ORIGINAL_REWRITE], _ = (\n\u001b[32m     53\u001b[39m     highlight_components(ig_original_rewrite_mlp)\n\u001b[32m     54\u001b[39m )\n\u001b[32m     55\u001b[39m attn_attribution_highlights[AttributionMethod.IG_ORIGINAL_REWRITE], _ = (\n\u001b[32m     56\u001b[39m     highlight_components(ig_original_rewrite_attn)\n\u001b[32m     57\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradient_causal_knowledge/attribution_methods.py:113\u001b[39m, in \u001b[36mintegrated_gradients\u001b[39m\u001b[34m(model, baseline_tokens, baseline_cache, input_cache, metric, metric_labels)\u001b[39m\n\u001b[32m    110\u001b[39m layer_baseline = baseline_cache[prev_layer_hook]\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Shape [batch, seq_len, d_head, d_model]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m attributions = \u001b[43mcompute_layer_to_output_attributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaseline_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Calculate score based on mean over each embedding, for each token\u001b[39;00m\n\u001b[32m    125\u001b[39m per_token_score = attributions.mean(dim=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradient_causal_knowledge/attribution_methods.py:69\u001b[39m, in \u001b[36mcompute_layer_to_output_attributions\u001b[39m\u001b[34m(model, original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, metric_labels)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Attribute to the target_layer's output\u001b[39;00m\n\u001b[32m     66\u001b[39m ig_embed = LayerIntegratedGradients(\n\u001b[32m     67\u001b[39m     forward_fn, target_layer, multiply_by_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     68\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m attributions, error = \u001b[43mig_embed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mError (delta) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/log/dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:493\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[33;03mThis method attributes the output of the model with given target index\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m(in case it is provided, otherwise it assumes that output is a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m \u001b[33;03m    >>> attribution = lig.attribute(input, target=3)\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    492\u001b[39m inps, baselines = _format_input_baseline(inputs, baselines)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m baselines = _tensorize_baseline(inps, baselines)\n\u001b[32m    496\u001b[39m additional_forward_args = _format_additional_forward_args(\n\u001b[32m    497\u001b[39m     additional_forward_args\n\u001b[32m    498\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/attr/_utils/common.py:46\u001b[39m, in \u001b[36m_validate_input\u001b[39m\u001b[34m(inputs, baselines, n_steps, method, draw_baseline_from_distrib)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_input\u001b[39m(\n\u001b[32m     40\u001b[39m     inputs: Tuple[Tensor, ...],\n\u001b[32m     41\u001b[39m     baselines: Tuple[Union[Tensor, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], ...],\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     draw_baseline_from_distrib: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     45\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43m_validate_input_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw_baseline_from_distrib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     48\u001b[39m         n_steps >= \u001b[32m0\u001b[39m\n\u001b[32m     49\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33mThe number of steps must be a positive integer. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mGiven: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_steps)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     52\u001b[39m         method \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_METHODS\n\u001b[32m     53\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33mApproximation method must be one for the following \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mGiven \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m     54\u001b[39m         SUPPORTED_METHODS, method\n\u001b[32m     55\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/captum/_utils/common.py:139\u001b[39m, in \u001b[36m_validate_input\u001b[39m\u001b[34m(inputs, baselines, draw_baseline_from_distrib)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    129\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(baseline, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m))\n\u001b[32m    130\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m.shape[\u001b[32m1\u001b[39m:] == baseline.shape[\u001b[32m1\u001b[39m:]\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Found baseline: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.format(baseline, \u001b[38;5;28minput\u001b[39m)\n\u001b[32m    136\u001b[39m     )\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(baseline, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m))\n\u001b[32m    140\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m.shape == baseline.shape\n\u001b[32m    141\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m baseline.shape[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m\n\u001b[32m    142\u001b[39m     ), (\n\u001b[32m    143\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBaseline can be provided as a tensor for just one input and\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    144\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m broadcasted to the batch or input and baseline must have the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m same shape or the baseline corresponding to each input tensor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m must be a scalar. Found baseline: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    147\u001b[39m             baseline, \u001b[38;5;28minput\u001b[39m\n\u001b[32m    148\u001b[39m         )\n\u001b[32m    149\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Baseline can be provided as a tensor for just one input and broadcasted to the batch or input and baseline must have the same shape or the baseline corresponding to each input tensor must be a scalar. Found baseline: tensor([[[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n            1.3576e-01,  1.0932e-01],\n          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n           -3.8492e-01,  2.4711e-01],\n          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n            1.2671e-01, -1.8432e-01],\n          ...,\n          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n           -1.7141e-01, -1.1625e-01],\n          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n            2.5598e-01, -7.1256e-02],\n          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n           -3.0645e-01,  3.7273e-01]],\n\n         [[ 5.6135e-02,  5.2765e-02,  8.7760e-03,  ..., -4.0619e-02,\n            1.3522e-01,  9.8149e-02],\n          [ 2.6660e-01,  6.5456e-03,  2.5709e-02,  ...,  1.8073e-01,\n            1.1111e-01,  1.3402e-01],\n          [-4.8208e-02, -1.4902e-01,  3.7220e-01,  ..., -6.3423e-02,\n            1.3125e-01, -1.6714e-01],\n          ...,\n          [-3.3139e-01, -4.4216e-01,  1.5671e-01,  ..., -5.7046e-02,\n           -1.5611e-01, -1.1741e-01],\n          [-1.2947e-01, -3.6367e-01, -2.0415e-01,  ...,  1.7347e-01,\n            2.6245e-01, -3.8326e-02],\n          [ 1.0529e-01, -3.8342e-01, -1.3472e-01,  ...,  1.8566e-01,\n           -1.9970e-01,  3.2450e-01]],\n\n         [[ 2.2397e-02,  4.0383e-02,  7.2053e-02,  ..., -1.7661e-02,\n            1.0304e-01,  4.1182e-02],\n          [ 1.6782e-01, -1.9282e-01,  9.4300e-02,  ...,  2.8860e-01,\n           -6.3679e-02,  1.7539e-01],\n          [-6.8276e-02, -1.6953e-01,  3.4273e-01,  ..., -7.1643e-02,\n            1.4635e-01, -1.4183e-01],\n          ...,\n          [-3.3865e-01, -4.1659e-01,  1.5499e-01,  ..., -5.7295e-02,\n           -1.5056e-01, -1.0963e-01],\n          [-1.2201e-01, -4.4275e-01, -1.8614e-01,  ...,  7.5768e-02,\n            2.1584e-01, -5.0138e-03],\n          [ 8.6065e-02, -2.1674e-01, -1.0654e-01,  ...,  1.5300e-01,\n           -1.4707e-01,  3.2081e-01]],\n\n         ...,\n\n         [[ 6.4724e-02, -4.8898e-03,  1.1926e-01,  ..., -3.0955e-02,\n            4.4494e-02,  5.0276e-02],\n          [-2.5488e-01, -1.5862e-01, -3.1766e-01,  ...,  1.9324e-01,\n           -2.3306e-01, -2.6534e-02],\n          [ 2.0906e-02, -1.1226e-01,  3.4931e-01,  ..., -2.2410e-02,\n            9.0138e-02, -1.0550e-01],\n          ...,\n          [-2.8575e-01, -2.9427e-01,  1.1049e-01,  ..., -8.5844e-03,\n           -6.3982e-02, -7.3881e-02],\n          [-8.0222e-02, -3.0836e-01, -9.6145e-02,  ...,  8.1953e-02,\n           -7.9909e-02, -3.2124e-02],\n          [ 8.8678e-02, -3.5062e-02, -9.9833e-02,  ...,  1.2702e-01,\n           -5.0877e-03,  3.4614e-01]],\n\n         [[ 8.0942e-02, -1.7746e-02,  1.2092e-01,  ..., -2.1856e-02,\n            4.9897e-02,  2.3710e-02],\n          [ 1.8996e-01,  2.1745e-01, -3.2437e-02,  ..., -2.5151e-02,\n           -9.7690e-02,  1.1364e-01],\n          [ 1.7757e-02, -8.9753e-02,  3.3869e-01,  ..., -4.9914e-02,\n            1.1590e-01, -9.6719e-02],\n          ...,\n          [-3.2242e-01, -2.5883e-01,  1.4814e-01,  ..., -9.8169e-03,\n           -1.3618e-01, -8.9883e-02],\n          [-1.2651e-01, -2.9261e-01, -1.3029e-01,  ..., -2.2379e-02,\n            4.8556e-03, -3.2084e-02],\n          [ 8.3148e-02, -1.4907e-01, -1.0640e-01,  ...,  1.2983e-01,\n           -2.8610e-02,  3.3631e-01]],\n\n         [[ 7.1720e-02,  6.8320e-03,  9.2763e-02,  ..., -3.3083e-02,\n            8.7794e-02,  3.8504e-02],\n          [ 8.2080e-02,  1.0938e-01, -6.7033e-02,  ...,  1.2012e-02,\n            1.2315e-01,  2.2886e-01],\n          [-3.1729e-02, -1.3783e-01,  3.9982e-01,  ..., -5.1877e-02,\n            1.4156e-01, -1.1621e-01],\n          ...,\n          [-3.2167e-01, -3.6756e-01,  1.6634e-01,  ...,  1.1999e-02,\n           -1.3713e-01, -1.2182e-01],\n          [-1.4868e-01, -3.5158e-01, -1.6904e-01,  ...,  6.7471e-02,\n            1.5907e-01, -3.8502e-03],\n          [ 1.4103e-01, -1.4993e-01, -1.8868e-01,  ...,  1.8337e-01,\n           -5.2561e-02,  3.9406e-01]]],\n\n\n        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n            1.3576e-01,  1.0932e-01],\n          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n           -3.8492e-01,  2.4711e-01],\n          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n            1.2671e-01, -1.8432e-01],\n          ...,\n          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n           -1.7141e-01, -1.1625e-01],\n          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n            2.5598e-01, -7.1256e-02],\n          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n           -3.0645e-01,  3.7273e-01]],\n\n         [[ 5.6135e-02,  5.2765e-02,  8.7760e-03,  ..., -4.0619e-02,\n            1.3522e-01,  9.8149e-02],\n          [ 2.6660e-01,  6.5456e-03,  2.5709e-02,  ...,  1.8073e-01,\n            1.1111e-01,  1.3402e-01],\n          [-4.8208e-02, -1.4902e-01,  3.7220e-01,  ..., -6.3423e-02,\n            1.3125e-01, -1.6714e-01],\n          ...,\n          [-3.3139e-01, -4.4216e-01,  1.5671e-01,  ..., -5.7046e-02,\n           -1.5611e-01, -1.1741e-01],\n          [-1.2947e-01, -3.6367e-01, -2.0415e-01,  ...,  1.7347e-01,\n            2.6245e-01, -3.8326e-02],\n          [ 1.0529e-01, -3.8342e-01, -1.3472e-01,  ...,  1.8566e-01,\n           -1.9970e-01,  3.2450e-01]],\n\n         [[ 3.7676e-02,  4.1196e-02,  6.9550e-02,  ..., -6.6748e-02,\n            1.2939e-01,  5.9925e-02],\n          [-1.6487e-02, -2.1374e-01, -2.9048e-02,  ...,  2.6674e-01,\n            1.6551e-01,  1.5811e-01],\n          [-5.7450e-02, -1.6644e-01,  3.4600e-01,  ..., -5.3515e-02,\n            1.3963e-01, -1.2742e-01],\n          ...,\n          [-3.3570e-01, -3.8015e-01,  1.6264e-01,  ..., -4.4775e-02,\n           -1.5235e-01, -8.9518e-02],\n          [-2.0001e-02, -2.8253e-01, -1.5684e-01,  ...,  1.0784e-01,\n            1.1128e-01, -6.7402e-02],\n          [ 1.2750e-01, -3.3137e-01, -1.7557e-01,  ...,  1.7779e-01,\n           -1.4017e-01,  3.3005e-01]],\n\n         ...,\n\n         [[ 4.9619e-02, -1.0999e-02,  7.5101e-02,  ..., -1.2517e-01,\n            4.7757e-02,  3.9624e-02],\n          [ 2.5300e-03,  1.7440e-01, -9.4192e-03,  ...,  3.1506e-02,\n            2.5101e-02,  2.2121e-01],\n          [ 6.8708e-03, -1.3345e-01,  3.1822e-01,  ..., -5.4231e-02,\n            1.2632e-01, -8.2346e-02],\n          ...,\n          [-3.2515e-01, -2.6622e-01,  1.2463e-01,  ..., -8.7309e-03,\n           -1.3990e-01, -9.7888e-02],\n          [ 3.2924e-03, -1.5808e-01, -1.0563e-01,  ...,  6.1098e-02,\n           -1.1146e-02, -7.0645e-02],\n          [ 1.1333e-01, -1.2933e-01, -1.1946e-01,  ...,  1.3591e-01,\n           -1.6498e-02,  3.5188e-01]],\n\n         [[ 4.3119e-02, -5.9800e-02,  1.1382e-01,  ..., -1.7368e-01,\n            1.0856e-01,  1.3357e-02],\n          [-3.0208e-01, -1.3085e-01,  2.6735e-01,  ...,  2.2674e-01,\n           -1.0058e-01, -2.0607e-01],\n          [ 4.3683e-03, -1.5895e-01,  2.7759e-01,  ..., -5.7834e-02,\n            1.2631e-01, -1.1445e-01],\n          ...,\n          [-3.1207e-01, -3.2192e-01,  1.2122e-01,  ..., -3.1067e-03,\n           -1.3828e-01, -1.0901e-01],\n          [-6.9010e-02, -3.4290e-01, -3.9674e-02,  ...,  7.8374e-02,\n            5.5379e-03,  2.7121e-02],\n          [ 9.8452e-02, -6.0071e-02, -1.3188e-01,  ...,  1.3406e-01,\n            6.8523e-02,  3.4852e-01]],\n\n         [[ 6.4684e-02, -3.2300e-02,  7.5071e-02,  ..., -1.1745e-01,\n            9.3861e-02,  2.1488e-02],\n          [ 1.6433e-01,  2.1547e-01, -4.0457e-02,  ..., -2.1143e-02,\n           -1.0321e-01,  1.1648e-01],\n          [ 5.5406e-02, -9.5885e-02,  3.2056e-01,  ..., -5.6154e-02,\n            1.2361e-01, -1.0570e-01],\n          ...,\n          [-3.4844e-01, -2.5039e-01,  1.3747e-01,  ...,  7.9164e-05,\n           -1.7358e-01, -1.0579e-01],\n          [-1.0557e-01, -2.3877e-01, -1.4631e-01,  ..., -3.3431e-02,\n           -5.0332e-02, -5.7785e-02],\n          [ 1.0105e-01, -1.6895e-01, -1.2693e-01,  ...,  1.4249e-01,\n            1.3330e-02,  3.3134e-01]]],\n\n\n        [[[ 6.3183e-02,  5.9828e-02, -1.8017e-02,  ..., -3.1300e-02,\n            1.3576e-01,  1.0932e-01],\n          [ 1.8581e-01,  2.3438e-01, -2.4526e-01,  ..., -5.3240e-01,\n           -3.8492e-01,  2.4711e-01],\n          [-4.0007e-02, -1.3616e-01,  3.8954e-01,  ..., -6.7756e-02,\n            1.2671e-01, -1.8432e-01],\n          ...,\n          [-2.8296e-01, -5.0378e-01,  1.6165e-01,  ..., -4.9117e-02,\n           -1.7141e-01, -1.1625e-01],\n          [-5.6480e-02, -4.1342e-01, -2.0710e-01,  ...,  1.9242e-01,\n            2.5598e-01, -7.1256e-02],\n          [ 1.5511e-01, -5.1378e-01, -1.7323e-01,  ...,  1.5156e-01,\n           -3.0645e-01,  3.7273e-01]],\n\n         [[ 6.1459e-02,  6.2121e-02, -2.8548e-02,  ..., -3.7793e-02,\n            1.2494e-01,  1.0063e-01],\n          [ 2.0101e-01, -5.9048e-02, -2.1680e-02,  ...,  4.5949e-01,\n            1.9642e-01,  4.2402e-02],\n          [-4.5368e-02, -1.2999e-01,  3.5952e-01,  ..., -6.8466e-02,\n            1.1048e-01, -1.7313e-01],\n          ...,\n          [-3.2417e-01, -4.4840e-01,  1.6240e-01,  ..., -1.5891e-02,\n           -1.4142e-01, -7.4223e-02],\n          [-1.0908e-01, -3.6107e-01, -1.7821e-01,  ...,  1.3265e-01,\n            2.3327e-01, -6.7998e-02],\n          [ 1.1848e-01, -4.2342e-01, -1.5293e-01,  ...,  1.5777e-01,\n           -2.0917e-01,  3.3538e-01]],\n\n         [[ 4.8368e-02,  7.0170e-02, -5.6113e-02,  ..., -5.3152e-02,\n            8.0512e-02,  8.4481e-02],\n          [-2.2859e-01,  2.1869e-02,  3.6398e-02,  ...,  8.5735e-02,\n            1.3093e-01, -8.4808e-02],\n          [-3.9078e-02, -1.2918e-01,  3.2743e-01,  ..., -6.1834e-02,\n            1.0285e-01, -1.5874e-01],\n          ...,\n          [-3.2902e-01, -4.3611e-01,  1.6472e-01,  ..., -1.7701e-02,\n           -1.3316e-01, -6.8558e-02],\n          [-4.9892e-02, -3.5500e-01, -1.4411e-01,  ...,  1.3653e-01,\n            1.3581e-01, -1.0052e-01],\n          [ 1.1163e-01, -3.6817e-01, -1.5165e-01,  ...,  1.5272e-01,\n           -1.7209e-01,  3.2340e-01]],\n\n         ...,\n\n         [[ 6.1552e-02,  9.7966e-02, -6.2787e-02,  ..., -4.0116e-02,\n            1.4242e-01,  8.2613e-02],\n          [ 1.3697e-01,  1.0831e-01, -5.7050e-02,  ...,  2.3917e-02,\n            1.4825e-01,  2.2475e-01],\n          [-1.2342e-01, -1.4249e-01,  3.8880e-01,  ..., -7.2036e-02,\n            1.1983e-01, -1.4325e-01],\n          ...,\n          [-3.1303e-01, -4.9957e-01,  2.1792e-01,  ...,  6.5372e-02,\n           -1.3064e-01, -9.1173e-02],\n          [-1.6771e-01, -3.6374e-01, -1.9052e-01,  ...,  1.4985e-01,\n            2.3929e-01, -2.6730e-02],\n          [ 2.2840e-01, -3.4029e-01, -3.3357e-01,  ...,  2.9052e-01,\n           -2.0867e-01,  4.4150e-01]],\n\n         [[ 5.9767e-02,  1.0052e-01, -7.7298e-02,  ..., -4.1897e-02,\n            1.7159e-01,  9.3002e-02],\n          [ 1.2573e-01,  1.0747e-01, -5.5569e-02,  ...,  2.7017e-02,\n            1.5030e-01,  2.2532e-01],\n          [-1.3033e-01, -1.5881e-01,  4.3875e-01,  ..., -6.9282e-02,\n            1.3641e-01, -1.6496e-01],\n          ...,\n          [-3.1057e-01, -5.1790e-01,  2.1481e-01,  ...,  6.9270e-02,\n           -1.2908e-01, -1.0996e-01],\n          [-1.8284e-01, -3.7971e-01, -2.0292e-01,  ...,  1.3540e-01,\n            2.6760e-01, -1.1640e-02],\n          [ 2.5303e-01, -3.5602e-01, -3.6007e-01,  ...,  3.0982e-01,\n           -2.3017e-01,  4.6528e-01]],\n\n         [[ 5.7735e-02,  1.0278e-01, -8.5991e-02,  ..., -4.1738e-02,\n            1.9132e-01,  1.0020e-01],\n          [ 1.1321e-01,  1.0665e-01, -5.6380e-02,  ...,  2.9411e-02,\n            1.4955e-01,  2.2620e-01],\n          [-1.3577e-01, -1.7232e-01,  4.7817e-01,  ..., -6.6842e-02,\n            1.4873e-01, -1.8249e-01],\n          ...,\n          [-3.0780e-01, -5.3170e-01,  2.1181e-01,  ...,  7.2728e-02,\n           -1.2800e-01, -1.2404e-01],\n          [-1.9340e-01, -3.9288e-01, -2.1265e-01,  ...,  1.2336e-01,\n            2.8932e-01,  1.7411e-04],\n          [ 2.7039e-01, -3.6830e-01, -3.7865e-01,  ...,  3.2212e-01,\n           -2.4867e-01,  4.8173e-01]]]], device='cuda:0') and input: tensor([[[[ 0.0632,  0.0598, -0.0180,  ..., -0.0313,  0.1358,  0.1093],\n          [ 0.1858,  0.2344, -0.2453,  ..., -0.5324, -0.3849,  0.2471],\n          [-0.0400, -0.1362,  0.3895,  ..., -0.0678,  0.1267, -0.1843],\n          ...,\n          [-0.2830, -0.5038,  0.1617,  ..., -0.0491, -0.1714, -0.1162],\n          [-0.0565, -0.4134, -0.2071,  ...,  0.1924,  0.2560, -0.0713],\n          [ 0.1551, -0.5138, -0.1732,  ...,  0.1516, -0.3064,  0.3727]],\n\n         [[ 0.0561,  0.0528,  0.0088,  ..., -0.0406,  0.1352,  0.0981],\n          [ 0.2666,  0.0065,  0.0257,  ...,  0.1807,  0.1111,  0.1340],\n          [-0.0482, -0.1490,  0.3722,  ..., -0.0634,  0.1313, -0.1671],\n          ...,\n          [-0.3314, -0.4422,  0.1567,  ..., -0.0570, -0.1561, -0.1174],\n          [-0.1295, -0.3637, -0.2042,  ...,  0.1735,  0.2624, -0.0383],\n          [ 0.1053, -0.3834, -0.1347,  ...,  0.1857, -0.1997,  0.3245]],\n\n         [[ 0.0224,  0.0404,  0.0721,  ..., -0.0177,  0.1030,  0.0412],\n          [ 0.1678, -0.1928,  0.0943,  ...,  0.2886, -0.0637,  0.1754],\n          [-0.0683, -0.1695,  0.3427,  ..., -0.0716,  0.1464, -0.1418],\n          ...,\n          [-0.3386, -0.4166,  0.1550,  ..., -0.0573, -0.1506, -0.1096],\n          [-0.1220, -0.4427, -0.1861,  ...,  0.0758,  0.2158, -0.0050],\n          [ 0.0861, -0.2167, -0.1065,  ...,  0.1530, -0.1471,  0.3208]],\n\n         ...,\n\n         [[ 0.0390, -0.0059,  0.0447,  ..., -0.0441,  0.1086,  0.0771],\n          [ 0.1154,  0.1087, -0.0581,  ...,  0.0240,  0.1421,  0.2260],\n          [-0.0464, -0.1620,  0.4461,  ..., -0.0695,  0.1694, -0.1567],\n          ...,\n          [-0.3399, -0.4017,  0.1574,  ...,  0.0261, -0.1458, -0.1475],\n          [-0.1677, -0.3812, -0.2123,  ...,  0.0474,  0.2063, -0.0148],\n          [ 0.2072, -0.2253, -0.2902,  ...,  0.2524, -0.1484,  0.4360]],\n\n         [[ 0.0401,  0.0144,  0.0147,  ..., -0.0430,  0.1390,  0.0875],\n          [ 0.1022,  0.1072, -0.0583,  ...,  0.0282,  0.1439,  0.2270],\n          [-0.0644, -0.1743,  0.4826,  ..., -0.0669,  0.1756, -0.1744],\n          ...,\n          [-0.3308, -0.4403,  0.1651,  ...,  0.0391, -0.1413, -0.1550],\n          [-0.1791, -0.3947, -0.2208,  ...,  0.0462,  0.2348, -0.0043],\n          [ 0.2339, -0.2616, -0.3218,  ...,  0.2757, -0.1817,  0.4593]],\n\n         [[ 0.0416,  0.0292, -0.0095,  ..., -0.0427,  0.1646,  0.0950],\n          [ 0.0877,  0.1075, -0.0574,  ...,  0.0300,  0.1454,  0.2279],\n          [-0.0788, -0.1834,  0.5108,  ..., -0.0656,  0.1801, -0.1887],\n          ...,\n          [-0.3247, -0.4666,  0.1696,  ...,  0.0464, -0.1377, -0.1605],\n          [-0.1885, -0.4046, -0.2280,  ...,  0.0453,  0.2583,  0.0038],\n          [ 0.2535, -0.2840, -0.3431,  ...,  0.2916, -0.2048,  0.4742]]],\n\n\n        [[[ 0.0632,  0.0598, -0.0180,  ..., -0.0313,  0.1358,  0.1093],\n          [ 0.1858,  0.2344, -0.2453,  ..., -0.5324, -0.3849,  0.2471],\n          [-0.0400, -0.1362,  0.3895,  ..., -0.0678,  0.1267, -0.1843],\n          ...,\n          [-0.2830, -0.5038,  0.1617,  ..., -0.0491, -0.1714, -0.1162],\n          [-0.0565, -0.4134, -0.2071,  ...,  0.1924,  0.2560, -0.0713],\n          [ 0.1551, -0.5138, -0.1732,  ...,  0.1516, -0.3064,  0.3727]],\n\n         [[ 0.0561,  0.0528,  0.0088,  ..., -0.0406,  0.1352,  0.0981],\n          [ 0.2666,  0.0065,  0.0257,  ...,  0.1807,  0.1111,  0.1340],\n          [-0.0482, -0.1490,  0.3722,  ..., -0.0634,  0.1313, -0.1671],\n          ...,\n          [-0.3314, -0.4422,  0.1567,  ..., -0.0570, -0.1561, -0.1174],\n          [-0.1295, -0.3637, -0.2042,  ...,  0.1735,  0.2624, -0.0383],\n          [ 0.1053, -0.3834, -0.1347,  ...,  0.1857, -0.1997,  0.3245]],\n\n         [[ 0.0377,  0.0412,  0.0695,  ..., -0.0667,  0.1294,  0.0599],\n          [-0.0165, -0.2137, -0.0290,  ...,  0.2667,  0.1655,  0.1581],\n          [-0.0575, -0.1664,  0.3460,  ..., -0.0535,  0.1396, -0.1274],\n          ...,\n          [-0.3357, -0.3801,  0.1626,  ..., -0.0448, -0.1524, -0.0895],\n          [-0.0200, -0.2825, -0.1568,  ...,  0.1078,  0.1113, -0.0674],\n          [ 0.1275, -0.3314, -0.1756,  ...,  0.1778, -0.1402,  0.3300]],\n\n         ...,\n\n         [[-0.0097,  0.0493,  0.0675,  ...,  0.0189, -0.0203,  0.0910],\n          [ 0.0876,  0.2347, -0.3979,  ..., -0.0596,  0.1815,  0.0359],\n          [ 0.0124, -0.0730,  0.2740,  ..., -0.0552,  0.0996, -0.0772],\n          ...,\n          [-0.3377, -0.2410,  0.1608,  ..., -0.0502, -0.0979, -0.0935],\n          [-0.1080, -0.1970, -0.1752,  ...,  0.0729,  0.0166, -0.0993],\n          [ 0.0906, -0.1237, -0.1725,  ...,  0.1998, -0.0151,  0.3163]],\n\n         [[-0.0240,  0.0152,  0.0722,  ..., -0.0323,  0.0153,  0.0777],\n          [-0.0253,  0.1407, -0.1750,  ...,  0.0721, -0.2685, -0.1088],\n          [ 0.0451, -0.0695,  0.2314,  ..., -0.0395,  0.0745, -0.0457],\n          ...,\n          [-0.3236, -0.2250,  0.1637,  ..., -0.0363, -0.0888, -0.0859],\n          [-0.0778, -0.1826, -0.1689,  ...,  0.0864, -0.0321, -0.0719],\n          [ 0.0875, -0.1289, -0.1605,  ...,  0.1928, -0.0261,  0.3056]],\n\n         [[ 0.0227,  0.0337,  0.0462,  ..., -0.0308,  0.0519,  0.0793],\n          [ 0.1303,  0.2213, -0.0339,  ..., -0.0225, -0.1041,  0.1170],\n          [ 0.0395, -0.1052,  0.2418,  ..., -0.0531,  0.0944, -0.0917],\n          ...,\n          [-0.3438, -0.1803,  0.1702,  ..., -0.0141, -0.1516, -0.0745],\n          [-0.1168, -0.2029, -0.1669,  ..., -0.0287, -0.0602, -0.0956],\n          [ 0.0929, -0.1973, -0.1562,  ...,  0.1743, -0.0282,  0.3170]]],\n\n\n        [[[ 0.0632,  0.0598, -0.0180,  ..., -0.0313,  0.1358,  0.1093],\n          [ 0.1858,  0.2344, -0.2453,  ..., -0.5324, -0.3849,  0.2471],\n          [-0.0400, -0.1362,  0.3895,  ..., -0.0678,  0.1267, -0.1843],\n          ...,\n          [-0.2830, -0.5038,  0.1617,  ..., -0.0491, -0.1714, -0.1162],\n          [-0.0565, -0.4134, -0.2071,  ...,  0.1924,  0.2560, -0.0713],\n          [ 0.1551, -0.5138, -0.1732,  ...,  0.1516, -0.3064,  0.3727]],\n\n         [[ 0.0626,  0.0688, -0.0081,  ..., -0.0430,  0.1256,  0.0931],\n          [ 0.0056, -0.0056, -0.0125,  ...,  0.3735, -0.0373,  0.1431],\n          [-0.0324, -0.1349,  0.3851,  ..., -0.0637,  0.1241, -0.1818],\n          ...,\n          [-0.3141, -0.4468,  0.1448,  ..., -0.0364, -0.1514, -0.1081],\n          [-0.0845, -0.3957, -0.1217,  ...,  0.1417,  0.2262, -0.0501],\n          [ 0.1276, -0.4066, -0.1258,  ...,  0.2022, -0.2168,  0.3499]],\n\n         [[ 0.0538,  0.0695,  0.0071,  ..., -0.0680,  0.0800,  0.0669],\n          [ 0.1445, -0.0442, -0.1842,  ...,  0.3799,  0.1650,  0.0277],\n          [-0.0213, -0.1298,  0.3684,  ..., -0.0589,  0.1145, -0.1673],\n          ...,\n          [-0.3241, -0.3642,  0.1712,  ..., -0.0419, -0.1588, -0.1205],\n          [-0.0851, -0.3901, -0.1720,  ...,  0.0625,  0.1336, -0.0081],\n          [ 0.1281, -0.4012, -0.1481,  ...,  0.1854, -0.2014,  0.3665]],\n\n         ...,\n\n         [[ 0.0579,  0.0904, -0.0220,  ..., -0.0799,  0.1815,  0.0354],\n          [ 0.1256,  0.1074, -0.0555,  ...,  0.0274,  0.1500,  0.2253],\n          [-0.1137, -0.1721,  0.4689,  ..., -0.0657,  0.1387, -0.1832],\n          ...,\n          [-0.3110, -0.5115,  0.1993,  ...,  0.0593, -0.1486, -0.1371],\n          [-0.1789, -0.3888, -0.1912,  ...,  0.1167,  0.2705, -0.0022],\n          [ 0.2536, -0.3351, -0.3439,  ...,  0.3073, -0.2030,  0.4756]],\n\n         [[ 0.0567,  0.0939, -0.0378,  ..., -0.0741,  0.1982,  0.0499],\n          [ 0.1131,  0.1066, -0.0563,  ...,  0.0297,  0.1494,  0.2262],\n          [-0.1213, -0.1830,  0.5026,  ..., -0.0639,  0.1501, -0.1974],\n          ...,\n          [-0.3082, -0.5263,  0.1989,  ...,  0.0647, -0.1440, -0.1461],\n          [-0.1899, -0.4000, -0.2034,  ...,  0.1079,  0.2919,  0.0076],\n          [ 0.2701, -0.3495, -0.3639,  ...,  0.3195, -0.2242,  0.4902]],\n\n         [[ 0.0553,  0.0963, -0.0522,  ..., -0.0694,  0.2126,  0.0628],\n          [ 0.0992,  0.1069, -0.0560,  ...,  0.0309,  0.1494,  0.2271],\n          [-0.1276, -0.1907,  0.5278,  ..., -0.0630,  0.1581, -0.2081],\n          ...,\n          [-0.3064, -0.5369,  0.1978,  ...,  0.0672, -0.1403, -0.1529],\n          [-0.1987, -0.4083, -0.2131,  ...,  0.1004,  0.3091,  0.0149],\n          [ 0.2830, -0.3579, -0.3774,  ...,  0.3279, -0.2381,  0.4997]]]],\n       device='cuda:0')"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "original_tokens = model.to_tokens(clean_input)\n",
    "rewrite_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "original_logits, original_cache = model.run_with_cache(original_tokens)\n",
    "original_logit_diff = logit_diff_metric(original_logits, labels)\n",
    "\n",
    "rewrite_logits, rewrite_cache = model.run_with_cache(rewrite_tokens)\n",
    "rewrite_logit_diff = logit_diff_metric(rewrite_logits, labels)\n",
    "\n",
    "# LOCALISATION STAGE\n",
    "\n",
    "mlp_highlighted, attn_highlighted = run_attribution_steps(\n",
    "    model,\n",
    "    original_tokens,\n",
    "    rewrite_tokens,\n",
    "    labels,\n",
    "    original_cache,\n",
    "    rewrite_cache,\n",
    "    original_logit_diff,\n",
    "    rewrite_logit_diff,\n",
    ")\n",
    "\n",
    "target_mlp = identify_target_components(model, mlp_highlighted)\n",
    "target_attn = identify_target_components(model, attn_highlighted)\n",
    "\n",
    "# EDITING STAGE\n",
    "\n",
    "relevant_parameters = [\n",
    "    p for name, p in model.named_parameters() if \"attn\" in name or \"mlp\" in name\n",
    "]\n",
    "optimiser = optim.Adam(relevant_parameters, lr=2e-4)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    logits = model(original_tokens)\n",
    "    answer_index = labels[:, 1]  # Aim for rewritten answer\n",
    "    optimise_edit_components(\n",
    "        model, logits, answer_index, target_mlp, target_attn, optimiser\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
