{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e36db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from testing import logit_diff_metric\n",
    "from applications.pipeline import run_attribution_steps, identify_target_components, optimise_edit_components\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0e1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c65bbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The mother tongue of Danielle Darrieux is',\n",
       " 'The official religion of Edwin of Northumbria is',\n",
       " 'The mother tongue of Paul McCartney is',\n",
       " 'The official religion of Rasul Gamzatov is']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_input + corrupted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1f858b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  464,  2802, 11880,   286, 39808,  7491,  5034,  2821,   318, 50256,\n",
       "          50256],\n",
       "         [  464,  1743,  5737,   286, 37016,   286,  2258,  2178,  7496,   318,\n",
       "          50256]], device='cuda:0'),\n",
       " tensor([[  464,  2802, 11880,   286,  3362, 44677,   318, 50256, 50256, 50256,\n",
       "          50256],\n",
       "         [  464,  1743,  5737,   286, 28513,   377, 14014,    89,   265,   709,\n",
       "            318]], device='cuda:0')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised = model.to_tokens(clean_input + corrupted_input, prepend_bos=False)\n",
    "\n",
    "[tokenised[i:i + 2] for i in range(0, len(tokenised), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca4d4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The mother tongue of Danielle Darrieux is', 'The official religion of Edwin of Northumbria is']\n",
      "['The mother tongue of Paul McCartney is', 'The official religion of Rasul Gamzatov is']\n",
      "tensor([[24111, 15823],\n",
      "        [20298, 16991]])\n"
     ]
    }
   ],
   "source": [
    "# Verify that loading works, for one example\n",
    "n_samples = 2\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(counterfact_dataloader))\n",
    "\n",
    "print(clean_input)\n",
    "print(corrupted_input)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11]) torch.Size([2, 11])\n",
      "Original logit difference: tensor([0.0337, 0.5005], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "Rewrite logit difference: tensor([ 0.1232, -3.2095], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.0.attn.hook_result attribution: tensor([-2.0787e-06, -1.6144e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.0.mlp.hook_post attribution: tensor([ 0.0005, -0.0800], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.1.attn.hook_result attribution: tensor([ 4.0978e-07, -5.4576e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.1.mlp.hook_post attribution: tensor([ 3.3155e-06, -9.3132e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.2.attn.hook_result attribution: tensor([-1.6969e-06, -2.4214e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.2.mlp.hook_post attribution: tensor([-1.9595e-06,  1.5646e-06], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.3.attn.hook_result attribution: tensor([ 2.6077e-07, -2.9318e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.3.mlp.hook_post attribution: tensor([ 1.8203e-06, -6.0163e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.4.attn.hook_result attribution: tensor([-4.5449e-07, -3.5055e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.4.mlp.hook_post attribution: tensor([-2.7660e-06,  5.0664e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.5.attn.hook_result attribution: tensor([-2.2175e-06, -1.4864e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.5.mlp.hook_post attribution: tensor([ 1.2852e-06, -5.2899e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.6.attn.hook_result attribution: tensor([-2.6971e-06, -4.3232e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.6.mlp.hook_post attribution: tensor([2.3581e-06, 2.1160e-06], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.7.attn.hook_result attribution: tensor([-4.0000e-07, -3.7737e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.7.mlp.hook_post attribution: tensor([-4.3698e-06, -7.4506e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.8.attn.hook_result attribution: tensor([-1.2047e-06,  5.0664e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.8.mlp.hook_post attribution: tensor([-3.7290e-06,  1.1921e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.9.attn.hook_result attribution: tensor([1.0682e-06, 2.3842e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.9.mlp.hook_post attribution: tensor([-3.1739e-06, -2.9802e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.10.attn.hook_result attribution: tensor([ 8.6357e-07, -1.6987e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.10.mlp.hook_post attribution: tensor([-1.2359e-06, -3.9935e-06], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.11.attn.hook_result attribution: tensor([-1.5795e-06, -4.1723e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.11.mlp.hook_post attribution: tensor([-3.6173e-06, -8.9407e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.0.attn.hook_result attribution: tensor([ 6.3404e-06, -9.5367e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.0.mlp.hook_post attribution: tensor([-0.0028, -0.0242], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.1.attn.hook_result attribution: tensor([3.6769e-06, 1.9744e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.1.mlp.hook_post attribution: tensor([1.6652e-06, 9.5367e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.2.attn.hook_result attribution: tensor([4.8820e-06, 2.8610e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.2.mlp.hook_post attribution: tensor([1.6429e-06, 3.2783e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.3.attn.hook_result attribution: tensor([ 4.9351e-06, -5.3644e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.3.mlp.hook_post attribution: tensor([3.0403e-06, 7.1526e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.4.attn.hook_result attribution: tensor([-6.2352e-07,  2.3991e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.4.mlp.hook_post attribution: tensor([ 7.1898e-07, -1.3411e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.5.attn.hook_result attribution: tensor([ 2.0731e-06, -2.3842e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.5.mlp.hook_post attribution: tensor([4.1761e-06, 5.9605e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.6.attn.hook_result attribution: tensor([ 3.0547e-07, -6.5565e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.6.mlp.hook_post attribution: tensor([ 1.7285e-06, -1.4156e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.7.attn.hook_result attribution: tensor([-8.5915e-07,  4.1723e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.7.mlp.hook_post attribution: tensor([ 8.5682e-07, -5.9605e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.8.attn.hook_result attribution: tensor([5.4296e-06, 1.2815e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.8.mlp.hook_post attribution: tensor([2.0005e-06, 9.5367e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.9.attn.hook_result attribution: tensor([1.9148e-06, 0.0000e+00], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.9.mlp.hook_post attribution: tensor([4.6864e-06, 7.7486e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.10.attn.hook_result attribution: tensor([5.3076e-06, 9.6112e-07], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.10.mlp.hook_post attribution: tensor([ 2.4140e-06, -7.1526e-07], device='cuda:0')\n",
      "torch.Size([2, 11, 12, 64])\n",
      "torch.Size([2, 11, 12, 64])\n",
      "\n",
      "Error (delta) for blocks.11.attn.hook_result attribution: tensor([4.4368e-06, 2.3842e-06], device='cuda:0')\n",
      "\n",
      "Error (delta) for blocks.11.mlp.hook_post attribution: tensor([3.1590e-06, 1.1921e-06], device='cuda:0')\n",
      "Activation patching on attention heads in layer 0\n",
      "Activation patching on MLP in layer 0\n",
      "Activation patching on attention heads in layer 1\n",
      "Activation patching on MLP in layer 1\n",
      "Activation patching on attention heads in layer 2\n",
      "Activation patching on MLP in layer 2\n",
      "Activation patching on attention heads in layer 3\n",
      "Activation patching on MLP in layer 3\n",
      "Activation patching on attention heads in layer 4\n",
      "Activation patching on MLP in layer 4\n",
      "Activation patching on attention heads in layer 5\n",
      "Activation patching on MLP in layer 5\n",
      "Activation patching on attention heads in layer 6\n",
      "Activation patching on MLP in layer 6\n",
      "Activation patching on attention heads in layer 7\n",
      "Activation patching on MLP in layer 7\n",
      "Activation patching on attention heads in layer 8\n",
      "Activation patching on MLP in layer 8\n",
      "Activation patching on attention heads in layer 9\n",
      "Activation patching on MLP in layer 9\n",
      "Activation patching on attention heads in layer 10\n",
      "Activation patching on MLP in layer 10\n",
      "Activation patching on attention heads in layer 11\n",
      "Activation patching on MLP in layer 11\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "identify_target_components() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# LOCALISATION STAGE\u001b[39;00m\n\u001b[32m     18\u001b[39m mlp_highlighted, attn_highlighted = run_attribution_steps(\n\u001b[32m     19\u001b[39m     model,\n\u001b[32m     20\u001b[39m     original_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     rewrite_logit_diff,\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m target_mlp = \u001b[43midentify_target_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_highlighted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m target_attn = identify_target_components(model, attn_highlighted)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# EDITING STAGE\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: identify_target_components() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "# Tokenise all together to ensure shapes stay the same\n",
    "tokenised = model.to_tokens(clean_input + corrupted_input, prepend_bos=False)\n",
    "original_tokens, rewrite_tokens = [tokenised[i:i + n_samples] for i in range(0, len(tokenised), n_samples)]\n",
    "print(original_tokens.shape, rewrite_tokens.shape)\n",
    "\n",
    "original_logits, original_cache = model.run_with_cache(original_tokens)\n",
    "original_logit_diff = logit_diff_metric(original_logits, labels)\n",
    "print(f\"Original logit difference: {original_logit_diff}\")\n",
    "\n",
    "rewrite_logits, rewrite_cache = model.run_with_cache(rewrite_tokens)\n",
    "rewrite_logit_diff = logit_diff_metric(rewrite_logits, labels)\n",
    "print(f\"Rewrite logit difference: {rewrite_logit_diff}\")\n",
    "\n",
    "# LOCALISATION STAGE\n",
    "\n",
    "mlp_highlighted, attn_highlighted = run_attribution_steps(\n",
    "    model,\n",
    "    original_tokens,\n",
    "    rewrite_tokens,\n",
    "    labels,\n",
    "    original_cache,\n",
    "    rewrite_cache,\n",
    "    original_logit_diff,\n",
    "    rewrite_logit_diff,\n",
    ")\n",
    "\n",
    "target_mlp = identify_target_components(mlp_highlighted)\n",
    "target_attn = identify_target_components(attn_highlighted)\n",
    "\n",
    "# EDITING STAGE\n",
    "\n",
    "relevant_parameters = [\n",
    "    p for name, p in model.named_parameters() if \"attn\" in name or \"mlp\" in name\n",
    "]\n",
    "optimiser = optim.Adam(relevant_parameters, lr=2e-4)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    logits = model(original_tokens)\n",
    "    answer_index = labels[:, 1]  # Aim for rewritten answer\n",
    "    optimise_edit_components(\n",
    "        model, logits, answer_index, target_mlp, target_attn, optimiser\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
