{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087a33ab",
   "metadata": {},
   "source": [
    "# Model Editing\n",
    "\n",
    "We use our IG and AP pipeline to localise important components. These components are edited using gradient descent to \"unlearn\" information. We evaluate our results on the CounterFact dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e36db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b56745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from testing import logit_diff_metric\n",
    "from applications.pipeline import run_attribution_steps, identify_target_components, optimise_edit_components, AttributionMethod\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0e1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# device = get_device()\n",
    "device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78dcf8",
   "metadata": {},
   "source": [
    "## Editing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4d4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The mother tongue of Danielle Darrieux is']\n",
      "['The mother tongue of Paul McCartney is']\n",
      "tensor([[24111, 15823]])\n"
     ]
    }
   ],
   "source": [
    "# Verify that loading works, for one example\n",
    "n_samples = 1\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n",
    "\n",
    "# clean_input, corrupted_input, labels = next(iter(counterfact_dataloader))\n",
    "clean_input, corrupted_input, labels = counterfact_dataset.get_single_sample(0)\n",
    "\n",
    "print(clean_input)\n",
    "print(corrupted_input)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37beafb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9]) torch.Size([1, 9])\n",
      "Original logit difference: tensor([0.3067], grad_fn=<SubBackward0>)\n",
      "Rewrite logit difference: tensor([0.0737], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tokenise all together to ensure shapes stay the same\n",
    "tokenised = model.to_tokens(clean_input + corrupted_input, prepend_bos=False)\n",
    "original_tokens, rewrite_tokens = [tokenised[i:i + n_samples] for i in range(0, len(tokenised), n_samples)]\n",
    "print(original_tokens.shape, rewrite_tokens.shape)\n",
    "\n",
    "original_logits, original_cache = model.run_with_cache(original_tokens)\n",
    "original_logit_diff = logit_diff_metric(original_logits, labels)\n",
    "print(f\"Original logit difference: {original_logit_diff}\")\n",
    "\n",
    "rewrite_logits, rewrite_cache = model.run_with_cache(rewrite_tokens)\n",
    "rewrite_logit_diff = logit_diff_metric(rewrite_logits, labels)\n",
    "print(f\"Rewrite logit difference: {rewrite_logit_diff}\")\n",
    "\n",
    "# LOCALISATION STAGE\n",
    "\n",
    "mlp_highlighted, attn_highlighted = run_attribution_steps(\n",
    "    model,\n",
    "    original_tokens,\n",
    "    rewrite_tokens,\n",
    "    labels,\n",
    "    original_cache,\n",
    "    rewrite_cache,\n",
    "    original_logit_diff,\n",
    "    rewrite_logit_diff,\n",
    ")\n",
    "\n",
    "target_mlp = identify_target_components(mlp_highlighted)\n",
    "target_attn = identify_target_components(attn_highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b51f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.76655387878418\n",
      "Loss: 7.237244606018066\n",
      "Loss: 2.146595001220703\n",
      "Loss: 1.9837698936462402\n",
      "Loss: 1.9987032413482666\n"
     ]
    }
   ],
   "source": [
    "# EDITING STAGE\n",
    "n_epochs = 5\n",
    "\n",
    "relevant_parameters = [\n",
    "    p for name, p in model.named_parameters() if \"attn\" in name or \"mlp\" in name\n",
    "]\n",
    "optimiser = optim.Adam(relevant_parameters, lr=2e-4)\n",
    "\n",
    "# TODO: the issue is that you can only refine a model for a specific data sample\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    forget_logits = model(clean_input)[:, -1, :]\n",
    "    retain_logits = model(corrupted_input)[:, -1, :]\n",
    "    answer_index = labels[:, 1]  # Aim for rewritten answer\n",
    "    optimise_edit_components(\n",
    "        model, forget_logits, retain_logits, answer_index, target_mlp[0], target_attn[0], optimiser\n",
    "    )\n",
    "    # TODO: instead of using the first sample of the tensors, can we fine tune on more than one???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc4b86",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c3416e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "tensor([[24111, 15823]])\n"
     ]
    }
   ],
   "source": [
    "from applications.datasets import CounterFactEvaluation\n",
    "\n",
    "counterfact_dataset = CounterFactEvaluation(model, \"generation_prompts\")\n",
    "original_input, labels = counterfact_dataset.get_single_sample(0)\n",
    "\n",
    "print(original_input)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a454bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts: [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'English', 'The', 'The', 'The', 'The', 'The', 'English', 'The', 'The']\n",
      "Efficacy score 1.0. Efficacy magnitude 0.06799386441707611.\n",
      "Prompts: ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: [' of', 'The']\n",
      "Generalisation score 1.0. Generalisation magnitude 0.004862282425165176.\n",
      "Prompts: ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'The', 'The', 'The', 'The', 'English', 'The', 'The', 'The', 'The']\n",
      "Specificity score 1.0. Specificity magnitude 0.037703223526477814.\n"
     ]
    }
   ],
   "source": [
    "from applications.metrics import evaluate_counterfact_efficacy, evaluate_counterfact_paraphrased, evaluate_counterfact_neighborhood\n",
    "\n",
    "score, magnitude = evaluate_counterfact_efficacy(model, 0, verbose=True)\n",
    "print(f\"Efficacy score {score}. Efficacy magnitude {magnitude}.\")\n",
    "\n",
    "\n",
    "score, magnitude = evaluate_counterfact_paraphrased(model, 0, verbose=True)\n",
    "print(f\"Generalisation score {score}. Generalisation magnitude {magnitude}.\")\n",
    "\n",
    "\n",
    "score, magnitude = evaluate_counterfact_neighborhood(model, 0, verbose=True)\n",
    "print(f\"Specificity score {score}. Specificity magnitude {magnitude}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
