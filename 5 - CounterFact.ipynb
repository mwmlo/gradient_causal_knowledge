{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087a33ab",
   "metadata": {},
   "source": [
    "# Model Editing\n",
    "\n",
    "We use our IG and AP pipeline to localise important components. These components are edited using gradient descent to \"unlearn\" information. We evaluate our results on the CounterFact dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e36db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from testing import logit_diff_metric\n",
    "from applications.pipeline import run_attribution_steps, identify_target_components, optimise_edit_components, AttributionMethod\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e0e1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# device = get_device()\n",
    "device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78dcf8",
   "metadata": {},
   "source": [
    "## Editing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The mother tongue of Danielle Darrieux is', 'The official religion of Edwin of Northumbria is', 'Toko Yasuda, the', 'Autonomous University of Madrid, which is located in', 'What is the twin city of Lyon? It is']\n",
      "['The mother tongue of Paul McCartney is', 'The official religion of Rasul Gamzatov is', 'Justus Frantz, the', 'IKEA, which is located in', 'What is the twin city of Bucharest? It is']\n",
      "tensor([[24111, 15823],\n",
      "        [20298, 16991],\n",
      "        [   70,    79],\n",
      "        [45355, 10462],\n",
      "        [ 3856,  5124]])\n"
     ]
    }
   ],
   "source": [
    "# Verify that loading works, for one example\n",
    "n_samples = 5\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(counterfact_dataloader))\n",
    "# clean_input, corrupted_input, labels = counterfact_dataset.get_single_sample(0)\n",
    "\n",
    "print(clean_input)\n",
    "print(corrupted_input)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c09bf2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux is French.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official religion of Edwin of Northumbria is the Christian religion of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toko Yasuda, the former president of the Japanese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous University of Madrid, which is located in Madrid, Spain, is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the twin city of Lyon? It is a city of the French\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample generation\n",
    "for i in range(n_samples):\n",
    "    output = model.generate(clean_input[i], max_new_tokens=5, do_sample=False)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37beafb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 11]) torch.Size([5, 11])\n",
      "Original logit difference: tensor([ 0.0337,  0.5005, -1.3812, -1.2258,  0.1026], grad_fn=<SubBackward0>)\n",
      "Rewrite logit difference: tensor([ 0.1232, -3.2095, -1.3924, -1.4354,  1.8098], grad_fn=<SubBackward0>)\n",
      "\n",
      "Error (delta) for blocks.0.attn.hook_result attribution: tensor([ 6.5193e-06, -3.5609e-06, -1.0366e-06,  3.3826e-06, -2.9672e-06])\n",
      "\n",
      "Error (delta) for blocks.0.mlp.hook_post attribution: tensor([ 4.8194e-04, -8.0000e-02, -2.8666e-06,  1.1356e-03, -5.1265e-03])\n",
      "\n",
      "Error (delta) for blocks.1.attn.hook_result attribution: tensor([ 7.0706e-06, -4.3679e-07, -1.3188e-06,  1.4622e-07,  2.2762e-06])\n",
      "\n",
      "Error (delta) for blocks.1.mlp.hook_post attribution: tensor([ 7.2028e-06,  1.0543e-06, -1.6987e-06,  1.3337e-06,  7.5251e-07])\n",
      "\n",
      "Error (delta) for blocks.2.attn.hook_result attribution: tensor([ 3.1777e-06, -1.5497e-06,  9.9465e-07, -3.4645e-07, -1.7956e-06])\n",
      "\n",
      "Error (delta) for blocks.2.mlp.hook_post attribution: tensor([ 5.7742e-06, -3.7253e-07, -1.4231e-06,  4.9546e-07, -1.0580e-06])\n",
      "\n",
      "Error (delta) for blocks.3.attn.hook_result attribution: tensor([ 4.0932e-06, -2.9132e-06,  5.0664e-07,  3.6336e-06,  2.9206e-06])\n",
      "\n",
      "Error (delta) for blocks.3.mlp.hook_post attribution: tensor([ 1.7174e-06, -7.5251e-07, -4.0848e-06,  4.4461e-06,  4.3213e-06])\n",
      "\n",
      "Error (delta) for blocks.4.attn.hook_result attribution: tensor([ 5.8673e-07, -1.6708e-06,  1.2387e-06, -3.2037e-06,  6.4187e-06])\n",
      "\n",
      "Error (delta) for blocks.4.mlp.hook_post attribution: tensor([ 4.9483e-06, -4.9919e-07, -4.2506e-06,  4.2599e-06, -2.6822e-07])\n",
      "\n",
      "Error (delta) for blocks.5.attn.hook_result attribution: tensor([ 4.4834e-06, -1.6019e-06, -4.7330e-06, -1.8626e-07, -5.0664e-07])\n",
      "\n",
      "Error (delta) for blocks.5.mlp.hook_post attribution: tensor([ 5.6773e-06, -7.3016e-07, -1.9013e-06,  3.7160e-07,  2.2650e-06])\n",
      "\n",
      "Error (delta) for blocks.6.attn.hook_result attribution: tensor([ 7.0557e-06, -2.5742e-06, -3.8063e-06,  1.6950e-06,  3.2568e-06])\n",
      "\n",
      "Error (delta) for blocks.6.mlp.hook_post attribution: tensor([ 7.1209e-06, -1.8738e-06,  4.0932e-07,  4.6836e-06,  2.8461e-06])\n",
      "\n",
      "Error (delta) for blocks.7.attn.hook_result attribution: tensor([ 4.3688e-06, -2.8126e-06,  1.2740e-06, -1.8012e-06,  2.2650e-06])\n",
      "\n",
      "Error (delta) for blocks.7.mlp.hook_post attribution: tensor([ 1.0673e-06,  4.3511e-06, -3.7383e-06,  4.0829e-06,  5.3346e-06])\n",
      "\n",
      "Error (delta) for blocks.8.attn.hook_result attribution: tensor([ 2.4727e-06,  1.8328e-06, -1.6391e-06,  4.0513e-06, -8.9407e-07])\n",
      "\n",
      "Error (delta) for blocks.8.mlp.hook_post attribution: tensor([ 6.5565e-07,  5.3644e-07, -4.4261e-07,  5.5032e-06, -2.1160e-06])\n",
      "\n",
      "Error (delta) for blocks.9.attn.hook_result attribution: tensor([ 3.9209e-06, -6.5565e-07,  2.1723e-06,  1.7211e-06,  5.9605e-06])\n",
      "\n",
      "Error (delta) for blocks.9.mlp.hook_post attribution: tensor([ 2.5332e-06, -1.7881e-07, -2.0277e-06, -9.9093e-07,  2.6673e-06])\n",
      "\n",
      "Error (delta) for blocks.10.attn.hook_result attribution: tensor([-1.6019e-07, -2.4140e-06, -2.5891e-06,  4.8056e-07,  2.6822e-06])\n",
      "\n",
      "Error (delta) for blocks.10.mlp.hook_post attribution: tensor([1.3337e-06, 1.8179e-06, 1.7136e-07, 4.2096e-07, 3.5167e-06])\n",
      "\n",
      "Error (delta) for blocks.11.attn.hook_result attribution: tensor([ 1.4193e-06, -2.3246e-06, -2.5928e-06,  1.2591e-06,  6.9886e-06])\n",
      "\n",
      "Error (delta) for blocks.11.mlp.hook_post attribution: tensor([ 4.4964e-06, -7.7486e-07,  1.0026e-06,  4.2832e-06,  1.7881e-06])\n",
      "\n",
      "Error (delta) for blocks.0.attn.hook_result attribution: tensor([-1.3560e-06,  1.7285e-06,  3.3388e-06, -1.4156e-06, -1.9372e-06])\n",
      "\n",
      "Error (delta) for blocks.0.mlp.hook_post attribution: tensor([-2.8016e-03, -2.4240e-02,  2.0675e-07,  2.9139e-05, -7.6551e-03])\n",
      "\n",
      "Error (delta) for blocks.1.attn.hook_result attribution: tensor([ 7.9721e-07, -1.2144e-06, -5.6252e-07, -1.7248e-06, -1.1139e-06])\n",
      "\n",
      "Error (delta) for blocks.1.mlp.hook_post attribution: tensor([-2.6450e-07,  8.3447e-07,  2.3283e-06,  4.8336e-06, -9.8348e-07])\n",
      "\n",
      "Error (delta) for blocks.2.attn.hook_result attribution: tensor([ 4.8429e-08,  1.1027e-06,  4.3828e-06,  2.3376e-06, -1.8477e-06])\n",
      "\n",
      "Error (delta) for blocks.2.mlp.hook_post attribution: tensor([-2.7567e-07,  5.5432e-06,  6.5081e-06,  4.4852e-06, -1.3113e-06])\n",
      "\n",
      "Error (delta) for blocks.3.attn.hook_result attribution: tensor([-7.9535e-07,  8.6427e-07, -2.4848e-06,  1.6447e-06, -3.1851e-06])\n",
      "\n",
      "Error (delta) for blocks.3.mlp.hook_post attribution: tensor([ 1.3299e-06,  1.9073e-06, -1.2573e-07, -6.8173e-07,  5.9605e-08])\n",
      "\n",
      "Error (delta) for blocks.4.attn.hook_result attribution: tensor([-2.5751e-06,  1.5646e-06, -1.3411e-07,  3.9488e-06, -1.5795e-06])\n",
      "\n",
      "Error (delta) for blocks.4.mlp.hook_post attribution: tensor([-2.6263e-07,  2.5928e-06,  8.3633e-07,  1.8328e-06,  3.1665e-07])\n",
      "\n",
      "Error (delta) for blocks.5.attn.hook_result attribution: tensor([ 2.0675e-06,  2.8610e-06,  2.7046e-06,  2.0191e-06, -1.1623e-06])\n",
      "\n",
      "Error (delta) for blocks.5.mlp.hook_post attribution: tensor([-1.2815e-06,  1.7881e-06,  1.4557e-06,  3.0026e-06, -1.5497e-06])\n",
      "\n",
      "Error (delta) for blocks.6.attn.hook_result attribution: tensor([-4.7386e-06,  5.9605e-07,  3.3691e-06,  2.5164e-06,  1.7881e-07])\n",
      "\n",
      "Error (delta) for blocks.6.mlp.hook_post attribution: tensor([-3.0287e-06,  2.4065e-06,  2.1965e-06,  3.3751e-06, -1.0133e-06])\n",
      "\n",
      "Error (delta) for blocks.7.attn.hook_result attribution: tensor([-8.4471e-07,  2.0564e-06,  5.7514e-06,  3.4878e-06, -1.1139e-06])\n",
      "\n",
      "Error (delta) for blocks.7.mlp.hook_post attribution: tensor([-1.7062e-06,  3.9637e-06,  1.5730e-06,  2.8126e-06, -1.1697e-06])\n",
      "\n",
      "Error (delta) for blocks.8.attn.hook_result attribution: tensor([ 2.7046e-06,  5.9605e-07,  1.7649e-06, -3.1162e-06,  4.4703e-07])\n",
      "\n",
      "Error (delta) for blocks.8.mlp.hook_post attribution: tensor([ 5.1782e-07,  2.9802e-06, -6.2166e-07,  1.9073e-06, -1.9968e-06])\n",
      "\n",
      "Error (delta) for blocks.9.attn.hook_result attribution: tensor([-8.6427e-07, -7.1526e-07,  3.1227e-06, -1.3858e-06, -7.0035e-07])\n",
      "\n",
      "Error (delta) for blocks.9.mlp.hook_post attribution: tensor([ 9.0152e-07, -1.1325e-06,  1.5972e-07,  3.6471e-06, -4.7684e-07])\n",
      "\n",
      "Error (delta) for blocks.10.attn.hook_result attribution: tensor([ 6.1933e-07, -1.3113e-06,  2.2426e-06, -1.5870e-06, -5.8115e-07])\n",
      "\n",
      "Error (delta) for blocks.10.mlp.hook_post attribution: tensor([ 7.3761e-07,  2.7418e-06,  1.1176e-06,  2.4587e-06, -1.6093e-06])\n",
      "\n",
      "Error (delta) for blocks.11.attn.hook_result attribution: tensor([-1.4007e-06, -1.6689e-06, -1.8440e-06,  2.3097e-07, -7.8231e-07])\n",
      "\n",
      "Error (delta) for blocks.11.mlp.hook_post attribution: tensor([-2.1160e-06, -1.6689e-06,  1.4657e-06, -1.6196e-06, -2.8014e-06])\n",
      "Activation patching on attention heads in layer 0\n",
      "Activation patching on MLP in layer 0\n",
      "Activation patching on attention heads in layer 1\n",
      "Activation patching on MLP in layer 1\n",
      "Activation patching on attention heads in layer 2\n",
      "Activation patching on MLP in layer 2\n",
      "Activation patching on attention heads in layer 3\n",
      "Activation patching on MLP in layer 3\n",
      "Activation patching on attention heads in layer 4\n",
      "Activation patching on MLP in layer 4\n",
      "Activation patching on attention heads in layer 5\n",
      "Activation patching on MLP in layer 5\n",
      "Activation patching on attention heads in layer 6\n",
      "Activation patching on MLP in layer 6\n",
      "Activation patching on attention heads in layer 7\n",
      "Activation patching on MLP in layer 7\n",
      "Activation patching on attention heads in layer 8\n",
      "Activation patching on MLP in layer 8\n",
      "Activation patching on attention heads in layer 9\n",
      "Activation patching on MLP in layer 9\n",
      "Activation patching on attention heads in layer 10\n",
      "Activation patching on MLP in layer 10\n",
      "Activation patching on attention heads in layer 11\n",
      "Activation patching on MLP in layer 11\n"
     ]
    }
   ],
   "source": [
    "# Tokenise all together to ensure shapes stay the same\n",
    "tokenised = model.to_tokens(clean_input + corrupted_input, prepend_bos=False)\n",
    "original_tokens, rewrite_tokens = [tokenised[i:i + n_samples] for i in range(0, len(tokenised), n_samples)]\n",
    "print(original_tokens.shape, rewrite_tokens.shape)\n",
    "\n",
    "original_logits, original_cache = model.run_with_cache(original_tokens)\n",
    "original_logit_diff = logit_diff_metric(original_logits, labels)\n",
    "print(f\"Original logit difference: {original_logit_diff}\")\n",
    "\n",
    "rewrite_logits, rewrite_cache = model.run_with_cache(rewrite_tokens)\n",
    "rewrite_logit_diff = logit_diff_metric(rewrite_logits, labels)\n",
    "print(f\"Rewrite logit difference: {rewrite_logit_diff}\")\n",
    "\n",
    "# LOCALISATION STAGE\n",
    "\n",
    "mlp_highlighted, attn_highlighted = run_attribution_steps(\n",
    "    model,\n",
    "    original_tokens,\n",
    "    rewrite_tokens,\n",
    "    labels,\n",
    "    original_cache,\n",
    "    rewrite_cache,\n",
    "    original_logit_diff,\n",
    "    rewrite_logit_diff,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "target_mlp = identify_target_components(mlp_highlighted)\n",
    "target_attn = identify_target_components(attn_highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b51f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine tuning model on sample 0...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 13.766546249389648\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 7.2374587059021\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 2.1456494331359863\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 1.983017086982727\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 1.9986073970794678\n",
      "\n",
      "Fine tuning model on sample 1...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 12.012432098388672\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 4.472603797912598\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 1.4108072519302368\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 0.9813597202301025\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 1.1624717712402344\n",
      "\n",
      "Fine tuning model on sample 2...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([79])\n",
      "Loss: 16.857669830322266\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([79])\n",
      "Loss: 10.372397422790527\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([79])\n",
      "Loss: 5.033481597900391\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([79])\n",
      "Loss: 1.4878751039505005\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([79])\n",
      "Loss: 1.479361891746521\n",
      "\n",
      "Fine tuning model on sample 3...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([10462])\n",
      "Loss: 16.64475440979004\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([10462])\n",
      "Loss: 9.003761291503906\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([10462])\n",
      "Loss: 2.070594072341919\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([10462])\n",
      "Loss: 0.013360492885112762\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([10462])\n",
      "Loss: 0.018198512494564056\n",
      "\n",
      "Fine tuning model on sample 4...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([5124])\n",
      "Loss: 20.04077911376953\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([5124])\n",
      "Loss: 10.676115036010742\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([5124])\n",
      "Loss: 3.394716739654541\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([5124])\n",
      "Loss: 1.888906478881836\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([5124])\n",
      "Loss: 1.960780382156372\n"
     ]
    }
   ],
   "source": [
    "# EDITING STAGE\n",
    "n_epochs = 5\n",
    "\n",
    "edited_models = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    print(f\"\\nFine tuning model on sample {i}...\")\n",
    "\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    relevant_parameters = [\n",
    "        p for name, p in model_copy.named_parameters() if \"attn\" in name or \"mlp\" in name\n",
    "    ]\n",
    "    optimiser = optim.Adam(relevant_parameters, lr=2e-4)\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        forget_logits = model_copy(clean_input[i])[:, -1, :]\n",
    "        retain_logits = model_copy(corrupted_input[i])[:, -1, :]\n",
    "        answer_index = labels[i, 1].unsqueeze(0)  # Aim for rewritten answer\n",
    "        print(forget_logits.shape, retain_logits.shape, answer_index)\n",
    "        optimise_edit_components(\n",
    "            model_copy, forget_logits, retain_logits, answer_index, target_mlp[i], target_attn[i], optimiser\n",
    "        )\n",
    "    edited_models.append(model_copy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc4b86",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a454bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The mother tongue of Danielle Darrieux is\n",
      "Original answer: tensor([24111, 20298,    70, 45355,  3856]). Target answer: tensor([15823, 16991,    79, 10462,  5124])\n",
      "Prompts: [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'English', 'The', 'The', 'The', 'The', 'The', 'English', 'The', 'The']\n",
      "Efficacy score 1.0. Efficacy magnitude 0.0652724951505661.\n",
      "Prompts: ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: [' of', 'The']\n",
      "Generalisation score 1.0. Generalisation magnitude 0.004716550465673208.\n",
      "Prompts: ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'The', 'The', 'The', 'The', 'English', 'The', 'The', 'The', 'The']\n",
      "Specificity score 1.0. Specificity magnitude 0.036850351840257645.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.55it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers: ['The U.S.', ' the city.\\n\\n', 'The U.S.', 'The UESPWiki', 'The U.S.', 'The U.S.', 'The UESPWiki', ' the city.\\n\\n', 'The UESPWiki', 'The UESPWiki']\n",
      "Reference answers: [' English and has been a', 'The UESPWiki', 'The UESPWiki', 'The UESPWiki', 'The first time I saw', 'The UESPWiki', 'The first time I saw', 'The U.S.', 'The first time I saw', 'The U.S.']\n",
      "Consistency score 0.44447002956810755\n",
      "Prompt: The official religion of Edwin of Northumbria is\n",
      "Original answer: tensor([24111, 20298,    70, 45355,  3856]). Target answer: tensor([15823, 16991,    79, 10462,  5124])\n",
      "Prompts: [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', ' the', 'The', 'The', 'The', 'The', 'The', ' the', 'The', 'The']\n",
      "Efficacy score 0.0. Efficacy magnitude -4.569550583255477e-05.\n",
      "Prompts: ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: [' of', 'The']\n",
      "Generalisation score 0.5. Generalisation magnitude -1.8543438272899948e-05.\n",
      "Prompts: ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'The', 'The', 'The', 'The', ' French', 'The', 'The', 'The', 'The']\n",
      "Specificity score 0.0. Specificity magnitude -4.4584288843907416e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.68it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers: ['The U.S.', ' the city.\\n\\n', 'The U.S.', 'The UESPWiki', 'The U.S.', 'The U.S.', 'The UESPWiki', ' the city.\\n\\n', 'The UESPWiki', 'The UESPWiki']\n",
      "Reference answers: [' English and has been a', 'The UESPWiki', 'The UESPWiki', 'The UESPWiki', 'The first time I saw', 'The UESPWiki', 'The first time I saw', 'The U.S.', 'The first time I saw', 'The U.S.']\n",
      "Consistency score 0.44447002956810755\n",
      "Prompt: Toko Yasuda, the\n",
      "Original answer: tensor([24111, 20298,    70, 45355,  3856]). Target answer: tensor([15823, 16991,    79, 10462,  5124])\n",
      "Prompts: [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', ' the', 'The', 'The', 'The', 'The', 'The', ' the', 'The', 'The']\n",
      "Efficacy score 1.0. Efficacy magnitude 1.6151960153365508e-05.\n",
      "Prompts: ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: [' of', 'The']\n",
      "Generalisation score 1.0. Generalisation magnitude 1.689217970124446e-05.\n",
      "Prompts: ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'The', 'The', 'The', 'The', ' the', 'The', 'The', 'The', 'The']\n",
      "Specificity score 1.0. Specificity magnitude 2.2975538740865886e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.65it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers: ['The U.S.', ' the city.\\n\\n', 'The U.S.', 'The UESPWiki', 'The U.S.', 'The U.S.', 'The UESPWiki', ' the city.\\n\\n', 'The UESPWiki', 'The UESPWiki']\n",
      "Reference answers: [' English and has been a', 'The UESPWiki', 'The UESPWiki', 'The UESPWiki', 'The first time I saw', 'The UESPWiki', 'The first time I saw', 'The U.S.', 'The first time I saw', 'The U.S.']\n",
      "Consistency score 0.44447002956810755\n",
      "Prompt: Autonomous University of Madrid, which is located in\n",
      "Original answer: tensor([24111, 20298,    70, 45355,  3856]). Target answer: tensor([15823, 16991,    79, 10462,  5124])\n",
      "Prompts: [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', ' the', 'The', 'The', 'The', 'The', 'The', ' the', 'The', 'The']\n",
      "Efficacy score 0.0. Efficacy magnitude -0.00012956907448824495.\n",
      "Prompts: ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: [' of', 'The']\n",
      "Generalisation score 0.0. Generalisation magnitude -6.253507308429107e-05.\n",
      "Prompts: ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'The', 'The', 'The', 'The', ' French', 'The', 'The', 'The', 'The']\n",
      "Specificity score 0.0. Specificity magnitude -0.00022832085960544646.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.67it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers: ['The U.S.', ' the city.\\n\\n', 'The U.S.', 'The UESPWiki', 'The U.S.', 'The U.S.', 'The UESPWiki', ' the city.\\n\\n', 'The UESPWiki', 'The UESPWiki']\n",
      "Reference answers: [' English and has been a', 'The UESPWiki', 'The UESPWiki', 'The UESPWiki', 'The first time I saw', 'The UESPWiki', 'The first time I saw', 'The U.S.', 'The first time I saw', 'The U.S.']\n",
      "Consistency score 0.44447002956810755\n",
      "Prompt: What is the twin city of Lyon? It is\n",
      "Original answer: tensor([24111, 20298,    70, 45355,  3856]). Target answer: tensor([15823, 16991,    79, 10462,  5124])\n",
      "Prompts: [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', ' the', 'The', 'The', 'The', 'The', 'The', ' the', 'The', 'The']\n",
      "Efficacy score 1.0. Efficacy magnitude 2.5950295821530744e-05.\n",
      "Prompts: ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: [' of', 'The']\n",
      "Generalisation score 1.0. Generalisation magnitude 3.371779894223437e-05.\n",
      "Prompts: ['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of']\n",
      "Original label: French\n",
      "Target label: English\n",
      "Outputs: ['The', 'The', 'The', 'The', 'The', ' French', 'The', 'The', 'The', 'The']\n",
      "Specificity score 0.5. Specificity magnitude 8.936243602875038e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.52it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers: ['The U.S.', ' the city.\\n\\n', 'The U.S.', 'The UESPWiki', 'The U.S.', 'The U.S.', 'The UESPWiki', ' the city.\\n\\n', 'The UESPWiki', 'The UESPWiki']\n",
      "Reference answers: [' English and has been a', 'The UESPWiki', 'The UESPWiki', 'The UESPWiki', 'The first time I saw', 'The UESPWiki', 'The first time I saw', 'The U.S.', 'The first time I saw', 'The U.S.']\n",
      "Consistency score 0.44447002956810755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from applications.metrics import evaluate_counterfact_efficacy, evaluate_counterfact_paraphrased, evaluate_counterfact_neighborhood, evaluate_consistency\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n",
    "clean_input, _, labels = next(iter(counterfact_dataloader))\n",
    "\n",
    "for i, edited_model in enumerate(edited_models):\n",
    "    print(f\"Prompt: {clean_input[i]}\")\n",
    "    print(f\"Original answer: {labels[:, 0]}. Target answer: {labels[:, 1]}\")\n",
    "\n",
    "    score, magnitude = evaluate_counterfact_efficacy(edited_model, 0, verbose=True)\n",
    "    print(f\"Efficacy score {score}. Efficacy magnitude {magnitude}.\")\n",
    "\n",
    "\n",
    "    score, magnitude = evaluate_counterfact_paraphrased(edited_model, 0, verbose=True)\n",
    "    print(f\"Generalisation score {score}. Generalisation magnitude {magnitude}.\")\n",
    "\n",
    "\n",
    "    score, magnitude = evaluate_counterfact_neighborhood(edited_model, 0, verbose=True)\n",
    "    print(f\"Specificity score {score}. Specificity magnitude {magnitude}.\")\n",
    "\n",
    "    consistency_score = evaluate_consistency(model, 0, verbose=True)\n",
    "    print(f\"Consistency score {consistency_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77adc94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux isEnglishEnglishEnglishEnglishEnglish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official religion of Edwin of Northumbria is Islam.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toko Yasuda, thep-p-p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous University of Madrid, which is located in Madrid Madrid Madrid Madrid Madrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the twin city of Lyon? It isManilaManilaMan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample generation\n",
    "for i in range(n_samples):\n",
    "    output = edited_models[i].generate(clean_input[i], max_new_tokens=5, do_sample=False)\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
