{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087a33ab",
   "metadata": {},
   "source": [
    "# Model Editing\n",
    "\n",
    "We use our IG and AP pipeline to localise important components. These components are edited using gradient descent to \"unlearn\" information. We evaluate our results on the CounterFact dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e36db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from testing import logit_diff_metric\n",
    "from applications.pipeline import run_attribution_steps, identify_target_components, optimise_edit_components, AttributionMethod\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e0e1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# device = get_device()\n",
    "device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78dcf8",
   "metadata": {},
   "source": [
    "## Editing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The mother tongue of Danielle Darrieux is', 'The official religion of Edwin of Northumbria is', 'Toko Yasuda, the', 'Autonomous University of Madrid, which is located in', 'What is the twin city of Lyon? It is']\n",
      "['The mother tongue of Paul McCartney is', 'The official religion of Rasul Gamzatov is', 'Justus Frantz, the', 'IKEA, which is located in', 'What is the twin city of Bucharest? It is']\n",
      "tensor([[24111, 15823],\n",
      "        [20298, 16991],\n",
      "        [   70,    79],\n",
      "        [45355, 10462],\n",
      "        [ 3856,  5124]])\n"
     ]
    }
   ],
   "source": [
    "# Verify that loading works, for one example\n",
    "n_samples = 5\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(counterfact_dataloader))\n",
    "# clean_input, corrupted_input, labels = counterfact_dataset.get_single_sample(0)\n",
    "\n",
    "print(clean_input)\n",
    "print(corrupted_input)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c09bf2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux is French.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official religion of Edwin of Northumbria is the Christian religion of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toko Yasuda, the former president of the Japanese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous University of Madrid, which is located in Madrid, Spain, is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the twin city of Lyon? It is a city of the French\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample generation\n",
    "for i in range(n_samples):\n",
    "    output = model.generate(clean_input[i], max_new_tokens=5, do_sample=False)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beafb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 11]) torch.Size([5, 11])\n",
      "Original logit difference: tensor([ 0.0337,  0.5005, -1.3812, -1.2258,  0.1026], grad_fn=<SubBackward0>)\n",
      "Rewrite logit difference: tensor([ 0.1232, -3.2095, -1.3924, -1.4354,  1.8098], grad_fn=<SubBackward0>)\n",
      "\n",
      "Error (delta) for blocks.0.attn.hook_result attribution: tensor([ 6.5193e-06, -3.5609e-06, -1.0366e-06,  3.3826e-06, -2.9672e-06])\n"
     ]
    }
   ],
   "source": [
    "# Tokenise all together to ensure shapes stay the same\n",
    "tokenised = model.to_tokens(clean_input + corrupted_input, prepend_bos=False)\n",
    "original_tokens, rewrite_tokens = [tokenised[i:i + n_samples] for i in range(0, len(tokenised), n_samples)]\n",
    "print(original_tokens.shape, rewrite_tokens.shape)\n",
    "\n",
    "original_logits, original_cache = model.run_with_cache(original_tokens)\n",
    "original_logit_diff = logit_diff_metric(original_logits, labels)\n",
    "print(f\"Original logit difference: {original_logit_diff}\")\n",
    "\n",
    "rewrite_logits, rewrite_cache = model.run_with_cache(rewrite_tokens)\n",
    "rewrite_logit_diff = logit_diff_metric(rewrite_logits, labels)\n",
    "print(f\"Rewrite logit difference: {rewrite_logit_diff}\")\n",
    "\n",
    "# LOCALISATION STAGE\n",
    "\n",
    "mlp_highlighted, attn_highlighted = run_attribution_steps(\n",
    "    model,\n",
    "    original_tokens,\n",
    "    rewrite_tokens,\n",
    "    labels,\n",
    "    original_cache,\n",
    "    rewrite_cache,\n",
    "    original_logit_diff,\n",
    "    rewrite_logit_diff,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "target_mlp = identify_target_components(mlp_highlighted)\n",
    "target_attn = identify_target_components(attn_highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine tuning model on sample 0...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 13.766546249389648\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 7.237238883972168\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 2.1465771198272705\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 1.9837701320648193\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([15823])\n",
      "Loss: 1.9987032413482666\n",
      "\n",
      "Fine tuning model on sample 1...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 12.012432098388672\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 4.472603797912598\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 1.4108072519302368\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 0.9813597202301025\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([16991])\n",
      "Loss: 1.1624717712402344\n",
      "\n",
      "Fine tuning model on sample 2...\n",
      "torch.Size([1, 50257]) torch.Size([1, 50257]) tensor([79])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     answer_index = labels[i, \u001b[32m1\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Aim for rewritten answer\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(forget_logits.shape, retain_logits.shape, answer_index)\n\u001b[32m     20\u001b[39m     optimise_edit_components(\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         model_copy, forget_logits, retain_logits, answer_index, \u001b[43mtarget_mlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, target_attn[i], optimiser\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m edited_models.append(model_copy)\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "# EDITING STAGE\n",
    "n_epochs = 5\n",
    "\n",
    "edited_models = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    print(f\"\\nFine tuning model on sample {i}...\")\n",
    "\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    relevant_parameters = [\n",
    "        p for name, p in model_copy.named_parameters() if \"attn\" in name or \"mlp\" in name\n",
    "    ]\n",
    "    optimiser = optim.Adam(relevant_parameters, lr=2e-4)\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        forget_logits = model_copy(clean_input[i])[:, -1, :]\n",
    "        retain_logits = model_copy(corrupted_input[i])[:, -1, :]\n",
    "        answer_index = labels[i, 1].unsqueeze(0)  # Aim for rewritten answer\n",
    "        print(forget_logits.shape, retain_logits.shape, answer_index)\n",
    "        optimise_edit_components(\n",
    "            model_copy, forget_logits, retain_logits, answer_index, target_mlp[i], target_attn[i], optimiser\n",
    "        )\n",
    "    edited_models.append(model_copy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc4b86",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a454bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapplications\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_counterfact_efficacy, evaluate_counterfact_paraphrased, evaluate_counterfact_neighborhood, evaluate_consistency\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapplications\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CounterFact\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m counterfact_dataset = CounterFact(\u001b[43mmodel\u001b[49m)\n\u001b[32m      5\u001b[39m counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n\u001b[32m      6\u001b[39m clean_input, _, labels = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(counterfact_dataloader))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from applications.metrics import evaluate_counterfact_efficacy, evaluate_counterfact_paraphrased, evaluate_counterfact_neighborhood, evaluate_consistency\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=n_samples)\n",
    "clean_input, _, labels = next(iter(counterfact_dataloader))\n",
    "\n",
    "for i, edited_model in enumerate(edited_models):\n",
    "    print(f\"Prompt: {clean_input[i]}\")\n",
    "    print(f\"Original answer: {labels[:, 0]}. Target answer: {labels[:, 1]}\")\n",
    "\n",
    "    score, magnitude = evaluate_counterfact_efficacy(edited_model, 0, verbose=True)\n",
    "    print(f\"Efficacy score {score}. Efficacy magnitude {magnitude}.\")\n",
    "\n",
    "\n",
    "    score, magnitude = evaluate_counterfact_paraphrased(edited_model, 0, verbose=True)\n",
    "    print(f\"Generalisation score {score}. Generalisation magnitude {magnitude}.\")\n",
    "\n",
    "\n",
    "    score, magnitude = evaluate_counterfact_neighborhood(edited_model, 0, verbose=True)\n",
    "    print(f\"Specificity score {score}. Specificity magnitude {magnitude}.\")\n",
    "\n",
    "    consistency_score = evaluate_consistency(model, 0, verbose=True)\n",
    "    print(f\"Consistency score {consistency_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77adc94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux is EnglishEnglishEnglishEnglishEnglishEnglishEnglishEnglishEnglishEnglish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Léon Blum is EnglishEnglishEnglishEnglishEnglishEnglishEnglishEnglishEnglishEnglish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample generation\n",
    "for i in range(n_samples):\n",
    "    output = edited_models[i].generate(clean_input[i], max_new_tokens=5, do_sample=False)\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
