{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087a33ab",
   "metadata": {},
   "source": [
    "# Model Editing\n",
    "\n",
    "We use our IG and AP pipeline to localise important components. These components are edited using gradient descent to \"unlearn\" information. We evaluate our results on the CounterFact dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e36db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b56745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "from testing import logit_diff_metric\n",
    "from applications.pipeline import run_attribution_steps, identify_target_components, optimise_edit_components, AttributionMethod, edit_model\n",
    "from applications.datasets import CounterFact\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e0e1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78dcf8",
   "metadata": {},
   "source": [
    "## Editing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca4d4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that loading works, for one example\n",
    "n_samples = 7500\n",
    "\n",
    "counterfact_dataset = CounterFact(model)\n",
    "counterfact_dataloader = counterfact_dataset.to_dataloader(batch_size=10)\n",
    "\n",
    "# clean_input, corrupted_input, labels = next(iter(counterfact_dataloader))\n",
    "clean_input, corrupted_input, labels = counterfact_dataset.get_single_sample(0)\n",
    "\n",
    "# print(clean_input)\n",
    "# print(corrupted_input)\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c09bf2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The mother tongue of Danielle Darrieux is French.\\n\\nThe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample generation\n",
    "model.generate(clean_input, max_new_tokens=5, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from applications.metrics import evaluate_counterfact_efficacy, evaluate_counterfact_paraphrased, evaluate_counterfact_neighborhood, evaluate_consistency\n",
    "from applications.datasets import CounterFact\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "evaluation_scores = defaultdict(list)\n",
    "\n",
    "for n, (clean_input, corrupted_input, labels) in enumerate(counterfact_dataloader):\n",
    "\n",
    "    if n >= n_samples: break\n",
    "\n",
    "    edited_models = edit_model(model, clean_input, corrupted_input, labels, overwrite=True)\n",
    "\n",
    "    # Evaluate\n",
    "    for i, edited_model in enumerate(edited_models):\n",
    "        print(f\"Prompt: {clean_input[i]}\")\n",
    "        print(f\"Original answer: {labels[:, 0]}. Target answer: {labels[:, 1]}\")\n",
    "\n",
    "        score, magnitude = evaluate_counterfact_efficacy(edited_model, 0, verbose=True)\n",
    "        evaluation_scores[\"Efficacy score\"].append(score.item())\n",
    "        evaluation_scores[\"Efficacy magnitude\"].append(magnitude.item())\n",
    "\n",
    "        score, magnitude = evaluate_counterfact_paraphrased(edited_model, 0, verbose=True)\n",
    "        evaluation_scores[\"Generalisation score\"].append(score.item())\n",
    "        evaluation_scores[\"Generalisation magnitude\"].append(magnitude.item())\n",
    "\n",
    "        score, magnitude = evaluate_counterfact_neighborhood(edited_model, 0, verbose=True)\n",
    "        evaluation_scores[\"Specificity score\"].append(score.item())\n",
    "        evaluation_scores[\"Specificity magnitude\"].append(magnitude.item())\n",
    "\n",
    "        consistency_score = evaluate_consistency(model, 0, verbose=True)\n",
    "        evaluation_scores[\"Consistency score\"].append(score.item())\n",
    "        evaluation_scores[\"Consistency magnitude\"].append(magnitude.item())\n",
    "\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_scores)\n",
    "evaluation_df.to_csv('results/counterfact/evaluation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc4b86",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For each sample, we calculate the efficacy, generalisability, specificity and consistency for:\n",
    "\n",
    "- The original models' outputs\n",
    "- The edited model's outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77adc94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux isEnglishEnglishEnglishEnglishEnglish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official religion of Edwin of Northumbria is Islam.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toko Yasuda, thep-p-p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous University of Madrid, which is located in Madrid Madrid Madrid Madrid Madrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the twin city of Lyon? It isManilaManilaMan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
