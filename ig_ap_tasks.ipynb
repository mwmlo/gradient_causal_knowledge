{
 "cells": [
  {
   "cell_type": "raw",
   "id": "40676fd2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Integrated Gradients vs Activation Patching Across Circuits\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09389a15",
   "metadata": {},
   "source": [
    "**Goal**: investigate the agreement between integrated gradients and activation patching when the baselines are similar, across a variety of circuit tasks.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Indirect Object Identification (![Wang et al, 2023](https://arxiv.org/pdf/2211.00593)): consists of inputs like “When Mary and John went to the store, John gave a bottle of milk to”; models are expected to predict “Mary”. Performance measured using logit differences.\n",
    "\n",
    "- Gender-Bias (![Vig et al, 2020](https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html)): designed to study gender bias in LMs. Gives models inputs like “The nurse said that”; biased models tend to complete this sentence with “she”. Performance measured using logit differences.\n",
    "\n",
    "- Greater-Than (![Hanna et al., 2023](https://arxiv.org/abs/2305.00586)): models receive input like “The war lasted from the year 1741 to the year 17”, and must predict a valid two-digit end year, i.e. one that is greater than 41. Performance measured using probability differences. \n",
    "\n",
    "- Capital–Country (![Hanna et al., 2024](https://arxiv.org/abs/2403.17806)): models receive input like “Tirana, the capital of” and must output the corresponding country (Albania). Corrupted instances contain another capital (e.g. Brasilia) instead. Performance measured using logit differences.\n",
    "\n",
    "- Subject-Verb Agreement (SVA) (![Newman et al, 2021](https://aclanthology.org/2021.naacl-main.290/)): models receive a sentence like “The keys on the cabinet”, and must output a verb that agrees in number with the subject (keys), e.g. are or have. In corrupted inputs, the subject’s number is changed, e.g. from keys to key, causing the model to output verbs of opposite agreement. Performance measured using probability differences. \n",
    "\n",
    "- Hypernymy: models must predict a word’s hypernym, or super- ordinate category, given inputs like “diamonds, and other”; the correct answer is “gems” or “gemstones”. Corrupted inputs contain an example of a distinct category, e.g. cars, which are vehicles. Performance measured using probability differences. This task is hard for small models, so we exclude inputs where GPT2-small gets a probability difference < 0.1 (following ![Hanna et al., 2024](https://arxiv.org/abs/2403.17806))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bf35e",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86280d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffabfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Task(Enum):\n",
    "    IOI = 1\n",
    "    GENDER_BIAS = 2\n",
    "    GREATER_THAN = 3\n",
    "    CAPITAL_COUNTRY = 4\n",
    "    SVA = 5\n",
    "    HYPERNYMY = 6\n",
    "\n",
    "# Implementation of dataset loader based on https://github.com/hannamw/eap-ig-faithfulness\n",
    "\n",
    "def collate_EAP(xs, task: Task):\n",
    "    clean, corrupted, labels = zip(*xs)\n",
    "    clean = list(clean)\n",
    "    corrupted = list(corrupted)\n",
    "    if task != Task.HYPERNYMY:\n",
    "        labels = torch.tensor(labels)\n",
    "    return clean, corrupted, labels\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, task: Task):\n",
    "        filename = task.name.lower()\n",
    "        self.task = task\n",
    "        self.df = pd.read_csv(f'datasets/{filename}.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def head(self, n: int):\n",
    "        self.df = self.df.head(n)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        label = None\n",
    "\n",
    "        if self.task == Task.IOI:\n",
    "            label = [row['correct_idx'], row['incorrect_idx']]\n",
    "            return row['clean'], row['corrupted_hard'], label\n",
    "        \n",
    "        if self.task == Task.GREATER_THAN:\n",
    "            label = row['correct_idx']\n",
    "        elif self.task == Task.HYPERNYMY:\n",
    "            answer = torch.tensor(eval(row['answers_idx']))\n",
    "            corrupted_answer = torch.tensor(eval(row['corrupted_answers_idx']))\n",
    "            label = [answer, corrupted_answer]\n",
    "        elif self.task == Task.CAPITAL_COUNTRY:\n",
    "            label = [row['country_idx'], row['corrupted_country_idx']]\n",
    "        elif self.task == Task.GENDER_BIAS:\n",
    "            label = [row['clean_answer_idx'], row['corrupted_answer_idx']]\n",
    "        elif self.task == Task.SVA:\n",
    "            label = row['plural']\n",
    "        else:\n",
    "            raise ValueError(f'Got invalid task: {self.task}')\n",
    "        \n",
    "        return row['clean'], row['corrupted'], label\n",
    "    \n",
    "    def to_dataloader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=partial(collate_EAP, task=self.task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b568b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72fdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_metric(logits, correct_index, incorrect_index):\n",
    "    logits_last = logits[:, -1, :]\n",
    "    batch_size = logits.size(0)\n",
    "    correct_logits = logits_last[torch.arange(batch_size), correct_index]\n",
    "    incorrect_logits = logits_last[torch.arange(batch_size), incorrect_index]\n",
    "    return correct_logits - incorrect_logits\n",
    "\n",
    "def prob_diff_metric():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ab4cb",
   "metadata": {},
   "source": [
    "## Integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_layer_fn(original_input, patch_layer, patch_output, metric, correct_idx, incorrect_idx, reset_hooks_end=True):\n",
    "    batch_size = original_input.size(0)\n",
    "    if patch_output.size(0) != batch_size:\n",
    "        # Captum IG produces 50 interpolation points for each sample, resulting in the first dim = 50 * batch_size\n",
    "        _, s, h, d = patch_output.shape\n",
    "        patch_output = patch_output.reshape(50, batch_size, s, h, d)\n",
    "        print(\"Reshaped patch_output\", patch_output.shape)\n",
    "    else:\n",
    "        patch_output = patch_output.unsqueeze(0)\n",
    "\n",
    "    # Patch activations for 50 times\n",
    "    patch_samples = patch_output.size(0)\n",
    "    print(\"Patch samples\", patch_samples)\n",
    "    differences = torch.zeros((patch_samples, batch_size))\n",
    "\n",
    "    for n in range(patch_samples):\n",
    "        patch_output_n = patch_output[n]\n",
    "        print(\"Original input shape\", original_input.shape)\n",
    "\n",
    "        def fwd_hook(act, hook):\n",
    "            assert patch_output_n.shape == act.shape, f\"Patch shape {patch_output_n.shape} doesn't match activation shape {act.shape}\"\n",
    "            print(f\"Patch shape {patch_output_n.shape} and activation shape {act.shape}\")\n",
    "            return patch_output_n\n",
    "    \n",
    "        logits = model.run_with_hooks(\n",
    "            original_input,\n",
    "            past_kv_cache=None,\n",
    "            fwd_hooks=[(patch_layer.name, fwd_hook), (lambda _: True, lambda act, hook: print(hook.name, act.shape) or act)],\n",
    "            reset_hooks_end=reset_hooks_end,\n",
    "        )\n",
    "        \n",
    "        assert logits.shape[0] == correct_idx.shape[0] == incorrect_idx.shape[0]\n",
    "        diff = metric(logits, correct_idx, incorrect_idx)\n",
    "        print(\"Diff\", diff.shape)\n",
    "        differences[n] = diff\n",
    "    \n",
    "    return differences\n",
    "\n",
    "\n",
    "def compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, correct_idx, incorrect_idx):\n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(original_input, prev_layer, x, metric, correct_idx, incorrect_idx)\n",
    "    # Attribute to the target_layer's output\n",
    "    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n",
    "                                                    baselines=layer_baseline, \n",
    "                                                    attribute_to_layer_input=False,\n",
    "                                                    return_convergence_delta=True)\n",
    "    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error.item()}\")\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c0c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, corrupted_cache: ActivationCache, metric: callable, correct_idx, incorrect_idx):\n",
    "    # Gradient attribution for neurons in MLP layers\n",
    "    mlp_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    # Gradient attribution for attention heads\n",
    "    attn_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    # Calculate integrated gradients for each layer\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Gradient attribution on heads\n",
    "        hook_name = get_act_name(\"result\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"z\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "        # Shape [batch, seq_len, d_head, d_model]\n",
    "        attributions = compute_layer_to_output_attributions(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, correct_idx, incorrect_idx)\n",
    "        # Calculate attribution score based on mean over each embedding, for each token\n",
    "        per_token_score = attributions.mean(dim=3)\n",
    "        score = per_token_score.mean(dim=1)\n",
    "        attn_results[layer] = score\n",
    "\n",
    "        # Gradient attribution on MLP neurons\n",
    "        hook_name = get_act_name(\"post\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"mlp_in\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "        \n",
    "        # Shape [batch, seq_len, d_model]\n",
    "        attributions = compute_layer_to_output_attributions(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, correct_idx, incorrect_idx)\n",
    "        score = attributions.mean(dim=1)\n",
    "        mlp_results[layer] = score\n",
    "\n",
    "    return mlp_results, attn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916596a0",
   "metadata": {},
   "source": [
    "## Activation patching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b32209",
   "metadata": {},
   "source": [
    "# Task 1: Indirect Object Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41276481",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b193685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch samples 1\n",
      "Original input shape torch.Size([2, 20])\n",
      "hook_embed torch.Size([2, 20, 768])\n",
      "hook_pos_embed torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.0.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "Patch shape torch.Size([2, 20, 12, 64]) and activation shape torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.0.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.0.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.1.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.1.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.1.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.1.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.1.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.1.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.1.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.2.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.2.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.2.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.2.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.2.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.2.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.2.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.3.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.3.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.3.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.3.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.3.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.3.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.3.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.4.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.4.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.4.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.4.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.4.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.4.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.4.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.5.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.5.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.5.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.5.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.5.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.5.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.5.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.6.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.6.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.6.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.6.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.6.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.6.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.6.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.7.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.7.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.7.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.7.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.7.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.7.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.7.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.8.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.8.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.8.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.8.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.8.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.8.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.8.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.9.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.9.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.9.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.9.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.9.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.9.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.9.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.10.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.10.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.10.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.10.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.10.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.10.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.10.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.11.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.11.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.11.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.11.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.11.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.11.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.11.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_post torch.Size([2, 20, 768])\n",
      "ln_final.hook_scale torch.Size([2, 20, 1])\n",
      "ln_final.hook_normalized torch.Size([2, 20, 768])\n",
      "Diff torch.Size([2])\n",
      "Patch samples 1\n",
      "Original input shape torch.Size([2, 20])\n",
      "hook_embed torch.Size([2, 20, 768])\n",
      "hook_pos_embed torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.0.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "Patch shape torch.Size([2, 20, 12, 64]) and activation shape torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.0.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.0.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.1.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.1.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.1.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.1.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.1.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.1.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.1.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.2.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.2.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.2.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.2.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.2.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.2.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.2.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.3.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.3.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.3.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.3.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.3.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.3.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.3.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.4.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.4.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.4.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.4.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.4.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.4.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.4.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.5.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.5.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.5.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.5.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.5.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.5.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.5.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.6.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.6.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.6.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.6.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.6.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.6.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.6.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.7.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.7.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.7.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.7.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.7.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.7.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.7.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.8.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.8.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.8.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.8.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.8.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.8.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.8.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.9.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.9.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.9.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.9.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.9.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.9.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.9.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.10.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.10.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.10.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.10.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.10.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.10.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.10.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.11.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.11.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.11.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.11.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.11.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.11.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.11.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_post torch.Size([2, 20, 768])\n",
      "ln_final.hook_scale torch.Size([2, 20, 1])\n",
      "ln_final.hook_normalized torch.Size([2, 20, 768])\n",
      "Diff torch.Size([2])\n",
      "Patch samples 1\n",
      "Original input shape torch.Size([2, 20])\n",
      "hook_embed torch.Size([2, 20, 768])\n",
      "hook_pos_embed torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.0.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "Patch shape torch.Size([2, 20, 12, 64]) and activation shape torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.0.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.0.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.0.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.0.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.1.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.1.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.1.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.1.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.1.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.1.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.1.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.1.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.1.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.1.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.1.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.2.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.2.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.2.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.2.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.2.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.2.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.2.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.2.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.2.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.2.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.2.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.3.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.3.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.3.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.3.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.3.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.3.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.3.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.3.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.3.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.3.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.3.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.4.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.4.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.4.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.4.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.4.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.4.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.4.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.4.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.4.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.4.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.4.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.5.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.5.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.5.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.5.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.5.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.5.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.5.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.5.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.5.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.5.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.5.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.6.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.6.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.6.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.6.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.6.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.6.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.6.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.6.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.6.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.6.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.6.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.7.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.7.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.7.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.7.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.7.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.7.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.7.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.7.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.7.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.7.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.7.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.8.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.8.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.8.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.8.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.8.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.8.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.8.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.8.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.8.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.8.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.8.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.9.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.9.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.9.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.9.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.9.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.9.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.9.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.9.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.9.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.9.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.9.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.10.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.10.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.10.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.10.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.10.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.10.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.10.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.10.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.10.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.10.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.10.hook_resid_post torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_pre torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.attn.hook_q torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_k torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_v torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_attn_scores torch.Size([2, 12, 20, 20])\n",
      "blocks.11.attn.hook_pattern torch.Size([2, 12, 20, 20])\n",
      "blocks.11.attn.hook_z torch.Size([2, 20, 12, 64])\n",
      "blocks.11.attn.hook_result torch.Size([2, 20, 12, 768])\n",
      "blocks.11.hook_attn_out torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_mid torch.Size([2, 20, 768])\n",
      "blocks.11.hook_mlp_in torch.Size([2, 20, 768])\n",
      "blocks.11.ln2.hook_scale torch.Size([2, 20, 1])\n",
      "blocks.11.ln2.hook_normalized torch.Size([2, 20, 768])\n",
      "blocks.11.mlp.hook_pre torch.Size([2, 20, 3072])\n",
      "blocks.11.mlp.hook_post torch.Size([2, 20, 3072])\n",
      "blocks.11.hook_mlp_out torch.Size([2, 20, 768])\n",
      "blocks.11.hook_resid_post torch.Size([2, 20, 768])\n",
      "ln_final.hook_scale torch.Size([2, 20, 1])\n",
      "ln_final.hook_normalized torch.Size([2, 20, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellelo/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_utils/batching.py:51: UserWarning: Internal batch size cannot be less than the number of input examples. Defaulting to internal batch size of 2 equal to the number of examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff torch.Size([2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Target not provided when necessary, cannot take gradient with respect to multiple outputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# corrupted_logit_diff = logit_diff_metric(corrupted_logits, correct_idx, incorrect_idx)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# print(f\"Corrupted logit difference: {corrupted_logit_diff}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m ioi_mlp, ioi_attn = \u001b[43mintegrated_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_diff_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincorrect_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mintegrated_gradients\u001b[39m\u001b[34m(model, clean_tokens, clean_cache, corrupted_cache, metric, correct_idx, incorrect_idx)\u001b[39m\n\u001b[32m     17\u001b[39m layer_corrupt_input = corrupted_cache[prev_layer_hook]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Shape [batch, seq_len, d_head, d_model]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m attributions = \u001b[43mcompute_layer_to_output_attributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_corrupt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_clean_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincorrect_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Calculate attribution score based on mean over each embedding, for each token\u001b[39;00m\n\u001b[32m     23\u001b[39m per_token_score = attributions.mean(dim=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mcompute_layer_to_output_attributions\u001b[39m\u001b[34m(original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, correct_idx, incorrect_idx)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Attribute to the target_layer's output\u001b[39;00m\n\u001b[32m     44\u001b[39m ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m attributions, approximation_error = \u001b[43mig_embed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mError (delta) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapproximation_error.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/log/dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:563\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28mself\u001b[39m.ig.gradient_func = \u001b[38;5;28mself\u001b[39m._make_gradient_func(\n\u001b[32m    555\u001b[39m     num_outputs_cumsum, attribute_to_layer_input, grad_kwargs\n\u001b[32m    556\u001b[39m )\n\u001b[32m    557\u001b[39m all_inputs = (\n\u001b[32m    558\u001b[39m     (inps + additional_forward_args)\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m additional_forward_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m inps\n\u001b[32m    561\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m attributions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# self\u001b[39;49;00m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaselines_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# handle multiple outputs\u001b[39;00m\n\u001b[32m    576\u001b[39m output: List[Tuple[Tensor, ...]] = [\n\u001b[32m    577\u001b[39m     \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    578\u001b[39m         attributions[\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(num_outputs))\n\u001b[32m    583\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:277\u001b[39m, in \u001b[36mIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m internal_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    276\u001b[39m     num_examples = formatted_inputs[\u001b[32m0\u001b[39m].shape[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     attributions = \u001b[43m_batch_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_baselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    289\u001b[39m     attributions = \u001b[38;5;28mself\u001b[39m._attribute(\n\u001b[32m    290\u001b[39m         inputs=formatted_inputs,\n\u001b[32m    291\u001b[39m         baselines=formatted_baselines,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m         method=method,\n\u001b[32m    296\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_utils/batching.py:86\u001b[39m, in \u001b[36m_batch_attribution\u001b[39m\u001b[34m(attr_method, num_examples, internal_batch_size, n_steps, include_endpoint, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m step_sizes = full_step_sizes[start_step:end_step]\n\u001b[32m     85\u001b[39m alphas = full_alphas[start_step:end_step]\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m current_attr = \u001b[43mattr_method\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_sizes_and_alphas\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     total_attr = current_attr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:368\u001b[39m, in \u001b[36mIntegratedGradients._attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[39m\n\u001b[32m    365\u001b[39m expanded_target = _expand_target(target, n_steps)\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[32m    377\u001b[39m scaled_grads = [\n\u001b[32m    378\u001b[39m     grad.contiguous().view(n_steps, -\u001b[32m1\u001b[39m)\n\u001b[32m    379\u001b[39m     * torch.tensor(step_sizes).float().view(n_steps, \u001b[32m1\u001b[39m).to(grad.device)\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[32m    381\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:224\u001b[39m, in \u001b[36mLayerIntegratedGradients._make_gradient_func.<locals>._gradient_func\u001b[39m\u001b[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[32m    223\u001b[39m output = cast(Tensor, output)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m output[\u001b[32m0\u001b[39m].numel() == \u001b[32m1\u001b[39m, (\n\u001b[32m    225\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTarget not provided when necessary, cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    226\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m take gradient with respect to multiple outputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    227\u001b[39m )\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n\u001b[32m    230\u001b[39m grads = torch.autograd.grad(\n\u001b[32m    231\u001b[39m     torch.unbind(output), inputs, **grad_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    232\u001b[39m )\n",
      "\u001b[31mAssertionError\u001b[39m: Target not provided when necessary, cannot take gradient with respect to multiple outputs."
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "correct_idx = labels[:, 0]\n",
    "incorrect_idx = labels[:, 1]\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "# clean_logit_diff = logit_diff_metric(clean_logits, correct_idx, incorrect_idx)\n",
    "# print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "# corrupted_logit_diff = logit_diff_metric(corrupted_logits, correct_idx, incorrect_idx)\n",
    "# print(f\"Corrupted logit difference: {corrupted_logit_diff}\")\n",
    "\n",
    "\n",
    "ioi_mlp, ioi_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, correct_idx, incorrect_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
