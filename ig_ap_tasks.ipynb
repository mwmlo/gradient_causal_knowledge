{
 "cells": [
  {
   "cell_type": "raw",
   "id": "40676fd2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Integrated Gradients vs Activation Patching Across Circuits\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09389a15",
   "metadata": {},
   "source": [
    "**Goal**: investigate the agreement between integrated gradients and activation patching when the baselines are similar, across a variety of circuit tasks.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Indirect Object Identification (![Wang et al, 2023](https://arxiv.org/pdf/2211.00593)): consists of inputs like “When Mary and John went to the store, John gave a bottle of milk to”; models are expected to predict “Mary”. Performance measured using logit differences.\n",
    "\n",
    "- Gender-Bias (![Vig et al, 2020](https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html)): designed to study gender bias in LMs. Gives models inputs like “The nurse said that”; biased models tend to complete this sentence with “she”. Performance measured using logit differences.\n",
    "\n",
    "- Greater-Than (![Hanna et al., 2023](https://arxiv.org/abs/2305.00586)): models receive input like “The war lasted from the year 1741 to the year 17”, and must predict a valid two-digit end year, i.e. one that is greater than 41. Performance measured using probability differences. \n",
    "\n",
    "- Capital–Country (![Hanna et al., 2024](https://arxiv.org/abs/2403.17806)): models receive input like “Tirana, the capital of” and must output the corresponding country (Albania). Corrupted instances contain another capital (e.g. Brasilia) instead. Performance measured using logit differences.\n",
    "\n",
    "- Subject-Verb Agreement (SVA) (![Newman et al, 2021](https://aclanthology.org/2021.naacl-main.290/)): models receive a sentence like “The keys on the cabinet”, and must output a verb that agrees in number with the subject (keys), e.g. are or have. In corrupted inputs, the subject’s number is changed, e.g. from keys to key, causing the model to output verbs of opposite agreement. Performance measured using probability differences. \n",
    "\n",
    "- Hypernymy: models must predict a word’s hypernym, or super- ordinate category, given inputs like “diamonds, and other”; the correct answer is “gems” or “gemstones”. Corrupted inputs contain an example of a distinct category, e.g. cars, which are vehicles. Performance measured using probability differences. This task is hard for small models, so we exclude inputs where GPT2-small gets a probability difference < 0.1 (following ![Hanna et al., 2024](https://arxiv.org/abs/2403.17806))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bf35e",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86280d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffabfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Task(Enum):\n",
    "    IOI = 1\n",
    "    GENDER_BIAS = 2\n",
    "    GREATER_THAN = 3\n",
    "    CAPITAL_COUNTRY = 4\n",
    "    SVA = 5\n",
    "    HYPERNYMY = 6\n",
    "\n",
    "# Implementation of dataset loader based on https://github.com/hannamw/eap-ig-faithfulness\n",
    "\n",
    "def collate_EAP(xs, task: Task):\n",
    "    clean, corrupted, labels = zip(*xs)\n",
    "    clean = list(clean)\n",
    "    corrupted = list(corrupted)\n",
    "    if task != Task.HYPERNYMY:\n",
    "        labels = torch.tensor(labels)\n",
    "    return clean, corrupted, labels\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, task: Task):\n",
    "        filename = task.name.lower()\n",
    "        self.task = task\n",
    "        self.df = pd.read_csv(f'datasets/{filename}.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def head(self, n: int):\n",
    "        self.df = self.df.head(n)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        label = None\n",
    "\n",
    "        if self.task == Task.IOI:\n",
    "            label = [row['correct_idx'], row['incorrect_idx']]\n",
    "            return row['clean'], row['corrupted_hard'], label\n",
    "        \n",
    "        if self.task == Task.GREATER_THAN:\n",
    "            label = row['correct_idx']\n",
    "        elif self.task == Task.HYPERNYMY:\n",
    "            answer = torch.tensor(eval(row['answers_idx']))\n",
    "            corrupted_answer = torch.tensor(eval(row['corrupted_answers_idx']))\n",
    "            label = [answer, corrupted_answer]\n",
    "        elif self.task == Task.CAPITAL_COUNTRY:\n",
    "            label = [row['country_idx'], row['corrupted_country_idx']]\n",
    "        elif self.task == Task.GENDER_BIAS:\n",
    "            label = [row['clean_answer_idx'], row['corrupted_answer_idx']]\n",
    "        elif self.task == Task.SVA:\n",
    "            label = row['plural']\n",
    "        else:\n",
    "            raise ValueError(f'Got invalid task: {self.task}')\n",
    "        \n",
    "        return row['clean'], row['corrupted'], label\n",
    "    \n",
    "    def to_dataloader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=partial(collate_EAP, task=self.task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b568b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d72fdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_metric(logits, metric_labels):\n",
    "    correct_index = metric_labels[:, 0]\n",
    "    incorrect_index = metric_labels[:, 1]\n",
    "    logits_last = logits[:, -1, :]\n",
    "    batch_size = logits.size(0)\n",
    "    correct_logits = logits_last[torch.arange(batch_size), correct_index]\n",
    "    incorrect_logits = logits_last[torch.arange(batch_size), incorrect_index]\n",
    "    return correct_logits - incorrect_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ab4cb",
   "metadata": {},
   "source": [
    "## Integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7c8175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_layer_fn(original_input, patch_layer, patch_output, metric, metric_labels, reset_hooks_end=True):\n",
    "    def fwd_hook(act, hook):\n",
    "        assert patch_output.shape == act.shape, f\"Patch shape {patch_output.shape} doesn't match activation shape {act.shape}\"\n",
    "        return patch_output\n",
    "\n",
    "    logits = model.run_with_hooks(\n",
    "        original_input,\n",
    "        fwd_hooks=[(patch_layer.name, fwd_hook)],\n",
    "        reset_hooks_end=reset_hooks_end,\n",
    "    )\n",
    "    \n",
    "    diff = metric(logits, metric_labels)\n",
    "    return diff\n",
    "\n",
    "\n",
    "def compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, metric_labels):\n",
    "    n_samples = original_input.size(0)\n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(original_input, prev_layer, x, metric, metric_labels)\n",
    "    # Attribute to the target_layer's output\n",
    "    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n",
    "                                                    baselines=layer_baseline, \n",
    "                                                    internal_batch_size=n_samples,\n",
    "                                                    attribute_to_layer_input=False,\n",
    "                                                    return_convergence_delta=True)\n",
    "    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error}\")\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c0c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, corrupted_cache: ActivationCache, metric: callable, metric_labels):\n",
    "    n_samples = clean_tokens.size(0)\n",
    "    \n",
    "    # Gradient attribution for neurons in MLP layers\n",
    "    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    # Gradient attribution for attention heads\n",
    "    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    # Calculate integrated gradients for each layer\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Gradient attribution on heads\n",
    "        hook_name = get_act_name(\"result\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"z\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "        # Shape [batch, seq_len, d_head, d_model]\n",
    "        attributions = compute_layer_to_output_attributions(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, metric_labels)\n",
    "        print(attributions.shape)\n",
    "        # Calculate attribution score based on mean over each embedding, for each token\n",
    "        per_token_score = attributions.mean(dim=3)\n",
    "        score = per_token_score.mean(dim=1)\n",
    "        attn_results[:, layer] = score\n",
    "\n",
    "        # Gradient attribution on MLP neurons\n",
    "        hook_name = get_act_name(\"post\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"mlp_in\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "        \n",
    "        # Shape [batch, seq_len, d_model]\n",
    "        attributions = compute_layer_to_output_attributions(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, metric_labels)\n",
    "        score = attributions.mean(dim=1)\n",
    "        mlp_results[:, layer] = score\n",
    "\n",
    "    return mlp_results, attn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916596a0",
   "metadata": {},
   "source": [
    "## Activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93b786cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, idx: int):\n",
    "    # Replace the activations for the target neuron with activations from the cached run.\n",
    "    cached_activations = cache[hook.name]\n",
    "    activations[:, :, idx] = cached_activations[:, :, idx]\n",
    "    return activations\n",
    "\n",
    "def activation_patching(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, clean_logit_diff, corrupted_cache: ActivationCache, corrupted_logit_diff, metric: callable, metric_labels):\n",
    "    n_samples = clean_tokens.size(0)\n",
    "    \n",
    "    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    baseline_diff = clean_logit_diff - corrupted_logit_diff\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Activation patching on heads\n",
    "        print(f\"Activation patching on attention heads in layer {layer}\")\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            hook_name = get_act_name(\"result\", layer)\n",
    "            temp_hook = lambda act, hook: patch_hook(act, hook, corrupted_cache, head)\n",
    "\n",
    "            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n",
    "                patched_logits = model(clean_tokens)\n",
    "\n",
    "            patched_logit_diff = metric(patched_logits, metric_labels).detach()\n",
    "            # Normalise result by clean and corrupted logit difference\n",
    "            attn_results[:, layer, head] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n",
    "\n",
    "        # Activation patching on MLP neurons\n",
    "        print(f\"Activation patching on MLP in layer {layer}\")\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook_name = get_act_name(\"post\", layer)\n",
    "            temp_hook = lambda act, hook: patch_hook(act, hook, corrupted_cache, neuron)\n",
    "            \n",
    "            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n",
    "                patched_logits = model(clean_tokens)\n",
    "\n",
    "            patched_logit_diff = metric(patched_logits, metric_labels).detach()\n",
    "            # Normalise result by clean and corrupted logit difference\n",
    "            mlp_results[:, layer, neuron] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n",
    "\n",
    "    return mlp_results, attn_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97643969",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1710bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def plot_correlation(ig_scores, ap_scores, title=None):\n",
    "    x = ig_scores.flatten()\n",
    "    y = ap_scores.flatten()\n",
    "\n",
    "    sns.regplot(x, y)\n",
    "    plt.xlabel(\"Integrated Gradients Attribution Scores\")\n",
    "    plt.ylabel(\"Activation Patching Attribution Scores\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Correlation coefficient: {np.corrcoef(x, y)[0, 1]}\")\n",
    "\n",
    "def plot_mean_diff(ig_scores, ap_scores, title=None):\n",
    "\n",
    "    x = ig_scores.flatten().numpy()\n",
    "    y = ap_scores.flatten().numpy()\n",
    "\n",
    "    # Mean difference plot with scaled data\n",
    "\n",
    "    scaled_ig_scores = MaxAbsScaler().fit_transform(x.reshape(-1, 1))\n",
    "    scaled_ap_scores = MaxAbsScaler().fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    mean = np.mean([scaled_ig_scores, scaled_ap_scores], axis=0)\n",
    "    diff = scaled_ap_scores - scaled_ig_scores\n",
    "    md = np.mean(diff) # Mean of the difference\n",
    "    sd = np.std(diff, axis=0) # Standard deviation of the difference\n",
    "\n",
    "    sns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\n",
    "    plt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\n",
    "    plt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\n",
    "    plt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\n",
    "    plt.xlabel(\"Mean of attribution scores\")\n",
    "    plt.ylabel(\"Difference (activation patching - integrated gradients)\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b32209",
   "metadata": {},
   "source": [
    "# Task 1: Indirect Object Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7b193685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b8e80e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-0.0307, -0.9269, -0.4937,  2.2320,  0.6754,  4.0447, -0.1785,  1.1947,\n",
      "         1.1513,  1.7507], device='mps:0')\n",
      "Corrupted logit difference: tensor([-0.0387, -0.9451, -0.5103,  2.2153,  0.6299, -3.2074, -0.1823,  1.1766,\n",
      "        -3.0072,  1.7392], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b193685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_ig_mlp, ioi_ig_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, labels)\n",
    "\n",
    "torch.save(ioi_ig_mlp, \"saved_results/ioi_ig_mlp.pt\")\n",
    "torch.save(ioi_ig_attn, \"saved_results/ioi_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_ap_mlp, ioi_ap_attn = activation_patching(\n",
    "    model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, \n",
    "    logit_diff_metric, labels)\n",
    "\n",
    "torch.save(ioi_ap_mlp, \"saved_results/ioi_ap_mlp.pt\")\n",
    "torch.save(ioi_ap_attn, \"saved_results/ioi_ap_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ce9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(ioi_ig_mlp, ioi_ap_mlp, \"IOI MLP Attribution Scores\")\n",
    "\n",
    "plot_correlation(ioi_ig_attn, ioi_ap_attn, \"IOI Attention Heads Attribution Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8335a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_diff(ioi_ig_mlp, ioi_ap_mlp, \"Mean-difference plot for IOI MLP attribution scores\")\n",
    "\n",
    "plot_mean_diff(ioi_ig_attn, ioi_ap_attn, \"Mean-difference plot for IOI attention head attribution scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8dd4c",
   "metadata": {},
   "source": [
    "# Task 2: Gender Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c40f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_dataset = TaskDataset(Task.GENDER_BIAS)\n",
    "gender_bias_dataloader = gender_bias_dataset.to_dataloader(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-1.4348, -1.5338,  1.4572,  1.4659,  1.4542,  1.3636,  1.4376,  1.5102,\n",
      "        -1.4453,  0.6453], device='mps:0')\n",
      "Corrupted logit difference: tensor([-3.9465, -1.5776,  1.1531,  1.2531,  1.2493, -4.2106,  1.3220,  1.3344,\n",
      "        -1.5273, -4.0531], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(gender_bias_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb729e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_ig_mlp, gender_bias_ig_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, labels)\n",
    "\n",
    "torch.save(gender_bias_ig_mlp, \"saved_results/gender_bias_ig_mlp.pt\")\n",
    "torch.save(gender_bias_ig_attn, \"saved_results/gender_bias_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_ap_mlp, gender_bias_ap_attn = activation_patching(\n",
    "    model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, \n",
    "    logit_diff_metric, labels)\n",
    "\n",
    "torch.save(gender_bias_ap_mlp, \"saved_results/gender_bias_ap_mlp.pt\")\n",
    "torch.save(gender_bias_ap_attn, \"saved_results/gender_bias_ap_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(gender_bias_ig_mlp, gender_bias_ap_mlp, \"Gender bias MLP Attribution Scores\")\n",
    "\n",
    "plot_correlation(gender_bias_ig_attn, gender_bias_ap_attn, \"Gender bias Attention Heads Attribution Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_diff(gender_bias_ig_mlp, gender_bias_ap_mlp, \"Mean-difference plot for gender bias MLP attribution scores\")\n",
    "\n",
    "plot_mean_diff(gender_bias_ig_attn, gender_bias_ap_attn, \"Mean-difference plot for gender bias attention head attribution scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cd654",
   "metadata": {},
   "source": [
    "# Task 3: Greater Than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67b0872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_dataset = TaskDataset(Task.GREATER_THAN)\n",
    "greater_than_dataloader = greater_than_dataset.to_dataloader(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "451096eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greater_than_prob_diff_metric(logits, correct_years):\n",
    "    batch_size = logits.size(0)\n",
    "    year_indices = model.to_tokens([str(year) for year in range(100)])[:, 1] # Shape [100]\n",
    "    logits_last = logits[:, -1, :]\n",
    "    logit_probs = torch.softmax(logits_last, dim=-1) # Shape [batch, d_vocab]\n",
    "    year_probs = logit_probs[:, year_indices] # Shape [batch, 100]\n",
    "    \n",
    "    results = torch.zeros((batch_size))\n",
    "    for i, (probs, year) in enumerate(zip(year_probs, correct_years)):\n",
    "        results[i] = probs[year+1:].sum() - probs[:year+1].sum()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "536a334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The contract lasted from the year 1352 to the year 13', 'The plan lasted from the year 1642 to the year 16', 'The plan lasted from the year 1457 to the year 14', 'The marriage lasted from the year 1783 to the year 17', 'The campaign lasted from the year 1388 to the year 13', 'The progress lasted from the year 1491 to the year 14', 'The study lasted from the year 1727 to the year 17', 'The case lasted from the year 1655 to the year 16', 'The fall lasted from the year 1375 to the year 13', 'The progress lasted from the year 1535 to the year 15']\n",
      "Clean logit difference: tensor([0.8690, 0.9025, 0.9068, 0.8934, 0.8278, 0.8116, 0.9455, 0.8799, 0.7257,\n",
      "        0.9022])\n",
      "Corrupted logit difference: tensor([-0.4874, -0.3821, -0.4345, -0.6994, -0.8329, -0.5399,  0.0454, -0.4602,\n",
      "        -0.7457, -0.3863])\n"
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(greater_than_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "print(clean_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = greater_than_prob_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = greater_than_prob_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b0c1661d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m greater_than_ig_mlp, greater_than_ig_attn = \u001b[43mintegrated_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrupted_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreater_than_prob_diff_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m torch.save(greater_than_ig_mlp, \u001b[33m\"\u001b[39m\u001b[33msaved_results/greater_than_ig_mlp.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m torch.save(greater_than_ig_attn, \u001b[33m\"\u001b[39m\u001b[33msaved_results/greater_than_ig_attn.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mintegrated_gradients\u001b[39m\u001b[34m(model, clean_tokens, clean_cache, corrupted_cache, metric, metric_labels)\u001b[39m\n\u001b[32m     19\u001b[39m layer_corrupt_input = corrupted_cache[prev_layer_hook]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Shape [batch, seq_len, d_head, d_model]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m attributions = \u001b[43mcompute_layer_to_output_attributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_corrupt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_clean_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(attributions.shape)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Calculate attribution score based on mean over each embedding, for each token\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mcompute_layer_to_output_attributions\u001b[39m\u001b[34m(original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, metric_labels)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Attribute to the target_layer's output\u001b[39;00m\n\u001b[32m     21\u001b[39m ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m attributions, approximation_error = \u001b[43mig_embed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mError (delta) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapproximation_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/log/dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:588\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    586\u001b[39m     start_point, end_point = baselines, inps\n\u001b[32m    587\u001b[39m     \u001b[38;5;66;03m# computes approximation error based on the completeness axiom\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     delta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_convergence_delta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _format_outputs(\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer, \u001b[38;5;28mlist\u001b[39m), output), delta\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _format_outputs(\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer, \u001b[38;5;28mlist\u001b[39m), output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/log/dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_utils/attribution.py:342\u001b[39m, in \u001b[36mGradientAttribution.compute_convergence_delta\u001b[39m\u001b[34m(self, attributions, start_point, end_point, target, additional_forward_args)\u001b[39m\n\u001b[32m    338\u001b[39m     row_sums = [_sum_rows(attribution) \u001b[38;5;28;01mfor\u001b[39;00m attribution \u001b[38;5;129;01min\u001b[39;00m attributions]\n\u001b[32m    339\u001b[39m     attr_sum = torch.stack(\n\u001b[32m    340\u001b[39m         [cast(Tensor, \u001b[38;5;28msum\u001b[39m(row_sum)) \u001b[38;5;28;01mfor\u001b[39;00m row_sum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*row_sums)]\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     _delta = \u001b[43mattr_sum\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_out_sum\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_out_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _delta\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "greater_than_ig_mlp, greater_than_ig_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, greater_than_prob_diff_metric, labels)\n",
    "\n",
    "torch.save(greater_than_ig_mlp, \"saved_results/greater_than_ig_mlp.pt\")\n",
    "torch.save(greater_than_ig_attn, \"saved_results/greater_than_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_ap_mlp, greater_than_ap_attn = activation_patching(\n",
    "    model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, \n",
    "    logit_diff_metric, labels)\n",
    "\n",
    "torch.save(greater_than_ap_mlp, \"saved_results/greater_than_ap_mlp.pt\")\n",
    "torch.save(greater_than_ap_attn, \"saved_results/greater_than_ap_attn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15debf3a",
   "metadata": {},
   "source": [
    "# Task 4: Capital Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "962edccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_country_dataset = TaskDataset(Task.CAPITAL_COUNTRY)\n",
    "capital_country_dataloader = capital_country_dataset.to_dataloader(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a7d7ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-1.8200, -2.5300, -2.0190,  6.3455, -0.5265,  0.9513,  0.8347, -1.4007,\n",
      "         2.1496, -3.0366], device='mps:0')\n",
      "Corrupted logit difference: tensor([-1.8150, -2.5942, -2.0515,  0.9749, -0.7268,  0.7562,  0.4618, -1.4402,\n",
      "         2.0016, -3.2090], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(capital_country_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_country_ig_mlp, capital_country_ig_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, labels)\n",
    "\n",
    "torch.save(capital_country_ig_mlp, \"saved_results/capital_country_ig_mlp.pt\")\n",
    "torch.save(capital_country_ig_attn, \"saved_results/capital_country_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b150c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_country_ap_mlp, capital_country_ap_attn = activation_patching(\n",
    "    model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, \n",
    "    logit_diff_metric, labels)\n",
    "\n",
    "torch.save(capital_country_ap_mlp, \"saved_results/capital_country_ap_mlp.pt\")\n",
    "torch.save(capital_country_ap_attn, \"saved_results/capital_country_ap_attn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
