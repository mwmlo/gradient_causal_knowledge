{
 "cells": [
  {
   "cell_type": "raw",
   "id": "40676fd2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Integrated Gradients vs Activation Patching Across Circuits\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09389a15",
   "metadata": {},
   "source": [
    "**Goal**: investigate the agreement between integrated gradients and activation patching when the baselines are similar, across a variety of circuit tasks.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- Indirect Object Identification (![Wang et al, 2023](https://arxiv.org/pdf/2211.00593)): consists of inputs like “When Mary and John went to the store, John gave a bottle of milk to”; models are expected to predict “Mary”. Performance measured using logit differences.\n",
    "\n",
    "- Gender-Bias (![Vig et al, 2020](https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html)): designed to study gender bias in LMs. Gives models inputs like “The nurse said that”; biased models tend to complete this sentence with “she”. Performance measured using logit differences.\n",
    "\n",
    "- Greater-Than (![Hanna et al., 2023](https://arxiv.org/abs/2305.00586)): models receive input like “The war lasted from the year 1741 to the year 17”, and must predict a valid two-digit end year, i.e. one that is greater than 41. Performance measured using probability differences. \n",
    "\n",
    "- Capital–Country (![Hanna et al., 2024](https://arxiv.org/abs/2403.17806)): models receive input like “Tirana, the capital of” and must output the corresponding country (Albania). Corrupted instances contain another capital (e.g. Brasilia) instead. Performance measured using logit differences.\n",
    "\n",
    "- Subject-Verb Agreement (SVA) (![Newman et al, 2021](https://aclanthology.org/2021.naacl-main.290/)): models receive a sentence like “The keys on the cabinet”, and must output a verb that agrees in number with the subject (keys), e.g. are or have. In corrupted inputs, the subject’s number is changed, e.g. from keys to key, causing the model to output verbs of opposite agreement. Performance measured using probability differences. \n",
    "\n",
    "- Hypernymy: models must predict a word’s hypernym, or super- ordinate category, given inputs like “diamonds, and other”; the correct answer is “gems” or “gemstones”. Corrupted inputs contain an example of a distinct category, e.g. cars, which are vehicles. Performance measured using probability differences. This task is hard for small models, so we exclude inputs where GPT2-small gets a probability difference < 0.1 (following ![Hanna et al., 2024](https://arxiv.org/abs/2403.17806))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bf35e",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86280d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffabfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Task(Enum):\n",
    "    IOI = 1\n",
    "    GENDER_BIAS = 2\n",
    "    GREATER_THAN = 3\n",
    "    CAPITAL_COUNTRY = 4\n",
    "    SVA = 5\n",
    "    HYPERNYMY = 6\n",
    "\n",
    "# Implementation of dataset loader based on https://github.com/hannamw/eap-ig-faithfulness\n",
    "\n",
    "def collate_EAP(xs, task: Task):\n",
    "    clean, corrupted, labels = zip(*xs)\n",
    "    clean = list(clean)\n",
    "    corrupted = list(corrupted)\n",
    "    if task != Task.HYPERNYMY:\n",
    "        labels = torch.tensor(labels)\n",
    "    return clean, corrupted, labels\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, task: Task):\n",
    "        filename = task.name.lower()\n",
    "        self.task = task\n",
    "        self.df = pd.read_csv(f'datasets/{filename}.csv')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def head(self, n: int):\n",
    "        self.df = self.df.head(n)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        label = None\n",
    "\n",
    "        if self.task == Task.IOI:\n",
    "            label = [row['correct_idx'], row['incorrect_idx']]\n",
    "            return row['clean'], row['corrupted_hard'], label\n",
    "        \n",
    "        if self.task == Task.GREATER_THAN:\n",
    "            label = row['correct_idx']\n",
    "        elif self.task == Task.HYPERNYMY:\n",
    "            answer = torch.tensor(eval(row['answers_idx']))\n",
    "            corrupted_answer = torch.tensor(eval(row['corrupted_answers_idx']))\n",
    "            label = [answer, corrupted_answer]\n",
    "        elif self.task == Task.CAPITAL_COUNTRY:\n",
    "            label = [row['country_idx'], row['corrupted_country_idx']]\n",
    "        elif self.task == Task.GENDER_BIAS:\n",
    "            label = [row['clean_answer_idx'], row['corrupted_answer_idx']]\n",
    "        elif self.task == Task.SVA:\n",
    "            label = row['plural']\n",
    "        else:\n",
    "            raise ValueError(f'Got invalid task: {self.task}')\n",
    "        \n",
    "        return row['clean'], row['corrupted'], label\n",
    "    \n",
    "    def to_dataloader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=partial(collate_EAP, task=self.task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b568b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72fdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_metric(logits, correct_index, incorrect_index):\n",
    "    logits_last = logits[:, -1, :]\n",
    "    batch_size = logits.size(0)\n",
    "    correct_logits = logits_last[torch.arange(batch_size), correct_index]\n",
    "    incorrect_logits = logits_last[torch.arange(batch_size), incorrect_index]\n",
    "    return correct_logits - incorrect_logits\n",
    "\n",
    "def prob_diff_metric():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ab4cb",
   "metadata": {},
   "source": [
    "## Integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7c8175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_layer_fn(original_input, patch_layer, patch_output, metric, correct_idx, incorrect_idx, reset_hooks_end=True):\n",
    "    def fwd_hook(act, hook):\n",
    "        assert patch_output.shape == act.shape, f\"Patch shape {patch_output.shape} doesn't match activation shape {act.shape}\"\n",
    "        return patch_output\n",
    "\n",
    "    logits = model.run_with_hooks(\n",
    "        original_input,\n",
    "        fwd_hooks=[(patch_layer.name, fwd_hook)],\n",
    "        reset_hooks_end=reset_hooks_end,\n",
    "    )\n",
    "    \n",
    "    assert logits.shape[0] == correct_idx.shape[0] == incorrect_idx.shape[0]\n",
    "    diff = metric(logits, correct_idx, incorrect_idx)\n",
    "    return diff\n",
    "\n",
    "\n",
    "def compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, correct_idx, incorrect_idx):\n",
    "    n_samples = original_input.size(0)\n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(original_input, prev_layer, x, metric, correct_idx, incorrect_idx)\n",
    "    # Attribute to the target_layer's output\n",
    "    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n",
    "                                                    baselines=layer_baseline, \n",
    "                                                    internal_batch_size=n_samples,\n",
    "                                                    attribute_to_layer_input=False,\n",
    "                                                    return_convergence_delta=True)\n",
    "    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error}\")\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c0c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, corrupted_cache: ActivationCache, metric: callable, correct_idx, incorrect_idx):\n",
    "    n_samples = clean_tokens.size(0)\n",
    "    \n",
    "    # Gradient attribution for neurons in MLP layers\n",
    "    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    # Gradient attribution for attention heads\n",
    "    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    # Calculate integrated gradients for each layer\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Gradient attribution on heads\n",
    "        hook_name = get_act_name(\"result\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"z\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "        # Shape [batch, seq_len, d_head, d_model]\n",
    "        attributions = compute_layer_to_output_attributions(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, correct_idx, incorrect_idx)\n",
    "        print(attributions.shape)\n",
    "        # Calculate attribution score based on mean over each embedding, for each token\n",
    "        per_token_score = attributions.mean(dim=3)\n",
    "        score = per_token_score.mean(dim=1)\n",
    "        attn_results[:, layer] = score\n",
    "\n",
    "        # Gradient attribution on MLP neurons\n",
    "        hook_name = get_act_name(\"post\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"mlp_in\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "        \n",
    "        # Shape [batch, seq_len, d_model]\n",
    "        attributions = compute_layer_to_output_attributions(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, correct_idx, incorrect_idx)\n",
    "        score = attributions.mean(dim=1)\n",
    "        mlp_results[:, layer] = score\n",
    "\n",
    "    return mlp_results, attn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916596a0",
   "metadata": {},
   "source": [
    "## Activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93b786cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, idx: int):\n",
    "    # Replace the activations for the target neuron with activations from the cached run.\n",
    "    cached_activations = cache[hook.name]\n",
    "    activations[:, :, idx] = cached_activations[:, :, idx]\n",
    "    return activations\n",
    "\n",
    "def activation_patching(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, clean_logit_diff, corrupted_cache: ActivationCache, corrupted_logit_diff, metric: callable, correct_idx, incorrect_idx):\n",
    "    n_samples = clean_tokens.size(0)\n",
    "    \n",
    "    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    baseline_diff = clean_logit_diff - corrupted_logit_diff\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        # Activation patching on heads\n",
    "        print(f\"Activation patching on attention heads in layer {layer}\")\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            hook_name = get_act_name(\"result\", layer)\n",
    "            temp_hook = lambda act, hook: patch_hook(act, hook, corrupted_cache, head)\n",
    "\n",
    "            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n",
    "                patched_logits = model(clean_tokens)\n",
    "\n",
    "            patched_logit_diff = metric(patched_logits, correct_idx, incorrect_idx).detach()\n",
    "            # Normalise result by clean and corrupted logit difference\n",
    "            attn_results[:, layer, head] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n",
    "\n",
    "        # Activation patching on MLP neurons\n",
    "        print(f\"Activation patching on MLP in layer {layer}\")\n",
    "        for neuron in range(model.cfg.d_mlp):\n",
    "            hook_name = get_act_name(\"post\", layer)\n",
    "            temp_hook = lambda act, hook: patch_hook(act, hook, corrupted_cache, neuron)\n",
    "            \n",
    "            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n",
    "                patched_logits = model(clean_tokens)\n",
    "\n",
    "            patched_logit_diff = metric(patched_logits, correct_idx, incorrect_idx).detach()\n",
    "            # Normalise result by clean and corrupted logit difference\n",
    "            mlp_results[:, layer, neuron] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n",
    "\n",
    "    return mlp_results, attn_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97643969",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1710bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def plot_correlation(ig_scores, ap_scores, title=None):\n",
    "    x = ig_scores.flatten()\n",
    "    y = ap_scores.flatten()\n",
    "\n",
    "    sns.regplot(x, y)\n",
    "    plt.xlabel(\"Integrated Gradients Attribution Scores\")\n",
    "    plt.ylabel(\"Activation Patching Attribution Scores\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Correlation coefficient: {np.corrcoef(x, y)[0, 1]}\")\n",
    "\n",
    "def plot_mean_diff(ig_scores, ap_scores, title=None):\n",
    "\n",
    "    x = ig_scores.flatten().numpy()\n",
    "    y = ap_scores.flatten().numpy()\n",
    "\n",
    "    # Mean difference plot with scaled data\n",
    "\n",
    "    scaled_ig_scores = MaxAbsScaler().fit_transform(x.reshape(-1, 1))\n",
    "    scaled_ap_scores = MaxAbsScaler().fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "    mean = np.mean([scaled_ig_scores, scaled_ap_scores], axis=0)\n",
    "    diff = scaled_ap_scores - scaled_ig_scores\n",
    "    md = np.mean(diff) # Mean of the difference\n",
    "    sd = np.std(diff, axis=0) # Standard deviation of the difference\n",
    "\n",
    "    sns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\n",
    "    plt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\n",
    "    plt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\n",
    "    plt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\n",
    "    plt.xlabel(\"Mean of attribution scores\")\n",
    "    plt.ylabel(\"Difference (activation patching - integrated gradients)\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b32209",
   "metadata": {},
   "source": [
    "# Task 1: Indirect Object Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b193685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e80e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "correct_idx = labels[:, 0]\n",
    "incorrect_idx = labels[:, 1]\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, correct_idx, incorrect_idx)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, correct_idx, incorrect_idx)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b193685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_ig_mlp, ioi_ig_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, correct_idx, incorrect_idx)\n",
    "\n",
    "torch.save(ioi_ig_mlp, \"saved_results/ioi_ig_mlp.pt\")\n",
    "torch.save(ioi_ig_attn, \"saved_results/ioi_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_ap_mlp, ioi_ap_attn = activation_patching(\n",
    "    model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, \n",
    "    logit_diff_metric, correct_idx, incorrect_idx)\n",
    "\n",
    "torch.save(ioi_ap_mlp, \"saved_results/ioi_ap_mlp.pt\")\n",
    "torch.save(ioi_ap_attn, \"saved_results/ioi_ap_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ce9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(ioi_ig_mlp, ioi_ap_mlp, \"IOI MLP Attribution Scores\")\n",
    "\n",
    "plot_correlation(ioi_ig_attn, ioi_ap_attn, \"IOI Attention Heads Attribution Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8335a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_diff(ioi_ig_mlp, ioi_ap_mlp, \"Mean-difference plot for IOI MLP attribution scores\")\n",
    "\n",
    "plot_mean_diff(ioi_ig_attn, ioi_ap_attn, \"Mean-difference plot for IOI attention head attribution scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8dd4c",
   "metadata": {},
   "source": [
    "# Task 2: Gender Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6c40f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_dataset = TaskDataset(Task.GENDER_BIAS)\n",
    "gender_bias_dataloader = gender_bias_dataset.to_dataloader(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c81f4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-1.4348, -1.5338,  1.4572,  1.4659,  1.4542,  1.3636,  1.4376,  1.5102,\n",
      "        -1.4453,  0.6453], device='mps:0')\n",
      "Corrupted logit difference: tensor([-3.9465, -1.5776,  1.1531,  1.2531,  1.2493, -4.2106,  1.3220,  1.3344,\n",
      "        -1.5273, -4.0531], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(gender_bias_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "correct_idx = labels[:, 0]\n",
    "incorrect_idx = labels[:, 1]\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, correct_idx, incorrect_idx)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, correct_idx, incorrect_idx)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb729e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_ig_mlp, gender_bias_ig_attn = integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, correct_idx, incorrect_idx)\n",
    "\n",
    "torch.save(gender_bias_ig_mlp, \"saved_results/gender_bias_ig_mlp.pt\")\n",
    "torch.save(gender_bias_ig_attn, \"saved_results/gender_bias_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_bias_ap_mlp, gender_bias_ap_attn = activation_patching(\n",
    "    model, clean_tokens, clean_cache, clean_logit_diff, corrupted_cache, corrupted_logit_diff, \n",
    "    logit_diff_metric, correct_idx, incorrect_idx)\n",
    "\n",
    "torch.save(gender_bias_ap_mlp, \"saved_results/gender_bias_ap_mlp.pt\")\n",
    "torch.save(gender_bias_ap_attn, \"saved_results/gender_bias_ap_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(gender_bias_ig_mlp, gender_bias_ap_mlp, \"Gender bias MLP Attribution Scores\")\n",
    "\n",
    "plot_correlation(gender_bias_ig_attn, gender_bias_ap_attn, \"Gender bias Attention Heads Attribution Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_diff(gender_bias_ig_mlp, gender_bias_ap_mlp, \"Mean-difference plot for gender bias MLP attribution scores\")\n",
    "\n",
    "plot_mean_diff(gender_bias_ig_attn, gender_bias_ap_attn, \"Mean-difference plot for gender bias attention head attribution scores\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
