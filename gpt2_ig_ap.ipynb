{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ff0fa83f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Integrated Gradients vs Activation Patching in GPT2-Small\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef6acb",
   "metadata": {},
   "source": [
    "# Background, motivation and set up\n",
    "\n",
    "**Objective**: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "- Understand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\n",
    "- Investigate if discrepancies help uncover different hidden model behaviours\n",
    "- Understand when and why linear approximations to activation patching fail\n",
    "- Investigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is \"incorrect\")\n",
    "\n",
    "**Set-up**:\n",
    "\n",
    "We load the transformer model GPT2-Small, which has 12 layers, 12 attention heads per layer, embedding size 768 and 4 x 768 = 3,072 neurons in each feed-forward layer. We use GPT2-Small because 1) it is a relatively small transformer model which has comparable behaviour to larger SOTA models, and 2) there is a lot of interpretability literature which focuses on circuits in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35f08ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ed8710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ceaacd",
   "metadata": {},
   "source": [
    "# Attribution for GPT2-Small\n",
    "\n",
    "We scale up our earlier experiments to implement integrated gradients and activation patching on a larger transformer model. We use the same counterfactual inputs, based on the Indirect Object Identification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4f1fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 4.276\n",
      "Corrupted logit difference: -2.738\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "\n",
    "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
    "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "\n",
    "clean_input = model.to_tokens(clean_prompt)\n",
    "corrupted_input = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
    "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
    "    correct_index = model.to_single_token(correct_answer)\n",
    "    incorrect_index = model.to_single_token(incorrect_answer)\n",
    "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_input)\n",
    "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)\n",
    "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3280a80",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_layer_fn(x, original_input, prev_layer):\n",
    "    # Force the layer before the target layer to output the given values, i.e. pass the given input into the target layer\n",
    "    # original_input value does not matter; useful to keep shapes nice, but its activations will be overwritten\n",
    "    logits = model.run_with_hooks(\n",
    "        original_input,\n",
    "        fwd_hooks=[(prev_layer.name, lambda act, hook: x)]\n",
    "    )\n",
    "    logit_diff = logits_to_logit_diff(logits).unsqueeze(0)\n",
    "    return logit_diff\n",
    "\n",
    "def compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer):\n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(x, original_input, prev_layer)\n",
    "    # Attribute to the target_layer's output\n",
    "    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n",
    "                                                    baselines=layer_baseline, \n",
    "                                                    attribute_to_layer_input=False,\n",
    "                                                    return_convergence_delta=True)\n",
    "    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error.item()}\")\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.hook_result\n"
     ]
    }
   ],
   "source": [
    "# Gradient attribution for neurons in MLP layers\n",
    "mlp_ig_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n",
    "# Gradient attribution for attention heads\n",
    "attn_ig_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "# Calculate integrated gradients for each layer\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Gradient attribution on heads\n",
    "    hook_name = get_act_name(\"result\", layer)\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    prev_layer_hook = get_act_name(\"z\", layer)\n",
    "    prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "    layer_clean_input = clean_cache[prev_layer_hook]\n",
    "    layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "    attributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_head, d_model]\n",
    "    # Calculate attribution score based on mean over each embedding, for each token\n",
    "    print(attributions.shape)\n",
    "    per_token_score = attributions.mean(dim=-1)\n",
    "    score = per_token_score.mean(dim=-1)\n",
    "    attn_ig_results[layer] = score\n",
    "\n",
    "    # Gradient attribution on MLP neurons\n",
    "    hook_name = get_act_name(\"mlp_out\", layer)\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    prev_layer_hook = get_act_name(\"post\", layer)\n",
    "    prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "    layer_clean_input = clean_cache[prev_layer_hook]\n",
    "    layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "    \n",
    "    attributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_model]\n",
    "    score = attributions.mean(dim=1)\n",
    "    mlp_ig_results[layer] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d88832",
   "metadata": {},
   "source": [
    "# Comparable baselines\n",
    "\n",
    "*Hypothesis*: One possible reason for the discrepancy between patching and IG is that the range of activations tested may be from different distributions.\n",
    "\n",
    "Both gradient methods rely on counterfactual reasoning. IG computes the integral between some baseline (which produces zero output) and given input, whereas causal tracing computes the logit difference between two counterfactual inputs. If the counterfactuals used are different, then this could cause a discrepancy.\n",
    "\n",
    "To evaluate this hypothesis, we compute IG and AP on GPT2-Small with the same counterfactual inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
