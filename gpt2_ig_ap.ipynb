{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ff0fa83f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Integrated Gradients vs Activation Patching in GPT2-Small\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef6acb",
   "metadata": {},
   "source": [
    "# Background, motivation and set up\n",
    "\n",
    "**Objective**: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\n",
    "\n",
    "**Motivation**:\n",
    "\n",
    "- Understand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\n",
    "- Investigate if discrepancies help uncover different hidden model behaviours\n",
    "- Understand when and why linear approximations to activation patching fail\n",
    "- Investigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is \"incorrect\")\n",
    "\n",
    "**Set-up**:\n",
    "\n",
    "We load the transformer model GPT2-Small, which has 12 layers, 12 attention heads per layer, embedding size 768 and 4 x 768 = 3,072 neurons in each feed-forward layer. We use GPT2-Small because 1) it is a relatively small transformer model which has comparable behaviour to larger SOTA models, and 2) there is a lot of interpretability literature which focuses on circuits in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35f08ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ed8710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ceaacd",
   "metadata": {},
   "source": [
    "# Attribution for GPT2-Small\n",
    "\n",
    "We scale up our earlier experiments to implement integrated gradients and activation patching on a larger transformer model. We use the same counterfactual inputs, based on the Indirect Object Identification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4f1fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: 4.276\n",
      "Corrupted logit difference: -2.738\n"
     ]
    }
   ],
   "source": [
    "#| output: true\n",
    "\n",
    "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
    "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "\n",
    "clean_input = model.to_tokens(clean_prompt)\n",
    "corrupted_input = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
    "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
    "    correct_index = model.to_single_token(correct_answer)\n",
    "    incorrect_index = model.to_single_token(incorrect_answer)\n",
    "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_input)\n",
    "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
    "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)\n",
    "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3280a80",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_layer_fn(x, original_input, prev_layer):\n",
    "    # Force the layer before the target layer to output the given values, i.e. pass the given input into the target layer\n",
    "    # original_input value does not matter; useful to keep shapes nice, but its activations will be overwritten\n",
    "    logits = model.run_with_hooks(\n",
    "        original_input,\n",
    "        fwd_hooks=[(prev_layer.name, lambda act, hook: x)]\n",
    "    )\n",
    "    logit_diff = logits_to_logit_diff(logits).unsqueeze(0)\n",
    "    return logit_diff\n",
    "\n",
    "def compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer):\n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(x, original_input, prev_layer)\n",
    "    # Attribute to the target_layer's output\n",
    "    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n",
    "                                                    baselines=layer_baseline, \n",
    "                                                    attribute_to_layer_input=False,\n",
    "                                                    return_convergence_delta=True)\n",
    "    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error.item()}\")\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.hook_result\n",
      "\n",
      "Error (delta) for blocks.0.attn.hook_result attribution: 0.08869590610265732\n",
      "torch.Size([1, 17, 12, 768])\n",
      "blocks.0.hook_mlp_in\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m layer_clean_input = clean_cache[prev_layer_hook]\n\u001b[32m     31\u001b[39m layer_corrupt_input = corrupted_cache[prev_layer_hook]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m attributions = \u001b[43mcompute_layer_to_output_attributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_clean_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_corrupt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_layer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shape [1, seq_len, d_model]\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(attributions.shape)\n\u001b[32m     35\u001b[39m score = attributions.mean(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcompute_layer_to_output_attributions\u001b[39m\u001b[34m(original_input, layer_input, layer_baseline, target_layer, prev_layer)\u001b[39m\n\u001b[32m     15\u001b[39m ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(target_layer.name)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m attributions, approximation_error = \u001b[43mig_embed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_baseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mError (delta) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_layer.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapproximation_error.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/log/dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:563\u001b[39m, in \u001b[36mLayerIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28mself\u001b[39m.ig.gradient_func = \u001b[38;5;28mself\u001b[39m._make_gradient_func(\n\u001b[32m    555\u001b[39m     num_outputs_cumsum, attribute_to_layer_input, grad_kwargs\n\u001b[32m    556\u001b[39m )\n\u001b[32m    557\u001b[39m all_inputs = (\n\u001b[32m    558\u001b[39m     (inps + additional_forward_args)\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m additional_forward_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m inps\n\u001b[32m    561\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m attributions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# self\u001b[39;49;00m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaselines_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minternal_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;66;03m# handle multiple outputs\u001b[39;00m\n\u001b[32m    576\u001b[39m output: List[Tuple[Tensor, ...]] = [\n\u001b[32m    577\u001b[39m     \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    578\u001b[39m         attributions[\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(num_outputs))\n\u001b[32m    583\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:289\u001b[39m, in \u001b[36mIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[39m\n\u001b[32m    277\u001b[39m     attributions = _batch_attribution(\n\u001b[32m    278\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m         num_examples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m         method=method,\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     attributions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_baselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[32m    299\u001b[39m     start_point, end_point = baselines, inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:368\u001b[39m, in \u001b[36mIntegratedGradients._attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[39m\n\u001b[32m    365\u001b[39m expanded_target = _expand_target(target, n_steps)\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[32m    377\u001b[39m scaled_grads = [\n\u001b[32m    378\u001b[39m     grad.contiguous().view(n_steps, -\u001b[32m1\u001b[39m)\n\u001b[32m    379\u001b[39m     * torch.tensor(step_sizes).float().view(n_steps, \u001b[32m1\u001b[39m).to(grad.device)\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[32m    381\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:230\u001b[39m, in \u001b[36mLayerIntegratedGradients._make_gradient_func.<locals>._gradient_func\u001b[39m\u001b[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m output[\u001b[32m0\u001b[39m].numel() == \u001b[32m1\u001b[39m, (\n\u001b[32m    225\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTarget not provided when necessary, cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    226\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m take gradient with respect to multiple outputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     grads = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgrad_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gradient_causal_knowledge/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:411\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    407\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    408\u001b[39m         grad_outputs_\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     result = \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    422\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    423\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    424\u001b[39m     ):\n",
      "\u001b[31mRuntimeError\u001b[39m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "# Gradient attribution for neurons in MLP layers\n",
    "mlp_ig_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n",
    "# Gradient attribution for attention heads\n",
    "attn_ig_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "# Calculate integrated gradients for each layer\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    # Gradient attribution on heads\n",
    "    hook_name = get_act_name(\"result\", layer)\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    prev_layer_hook = get_act_name(\"z\", layer)\n",
    "    prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "    layer_clean_input = clean_cache[prev_layer_hook]\n",
    "    layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "    attributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_head, d_model]\n",
    "    # Calculate attribution score based on mean over each embedding, for each token\n",
    "    print(attributions.shape)\n",
    "    per_token_score = attributions.mean(dim=3)\n",
    "    score = per_token_score.mean(dim=1)\n",
    "    attn_ig_results[layer] = score\n",
    "\n",
    "    # Gradient attribution on MLP neurons\n",
    "    hook_name = get_act_name(\"post\", layer)\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    prev_layer_hook = get_act_name(\"mlp_in\", layer)\n",
    "    prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "    layer_clean_input = clean_cache[prev_layer_hook]\n",
    "    layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "    \n",
    "    attributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_model]\n",
    "    print(attributions.shape)\n",
    "    score = attributions.mean(dim=1)\n",
    "    mlp_ig_results[layer] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d88832",
   "metadata": {},
   "source": [
    "# Comparable baselines\n",
    "\n",
    "*Hypothesis*: One possible reason for the discrepancy between patching and IG is that the range of activations tested may be from different distributions.\n",
    "\n",
    "Both gradient methods rely on counterfactual reasoning. IG computes the integral between some baseline (which produces zero output) and given input, whereas causal tracing computes the logit difference between two counterfactual inputs. If the counterfactuals used are different, then this could cause a discrepancy.\n",
    "\n",
    "To evaluate this hypothesis, we compute IG and AP on GPT2-Small with the same counterfactual inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
