[
  {
    "objectID": "gpt2_ig_ap.html",
    "href": "gpt2_ig_ap.html",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "",
    "text": "Objective: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\nMotivation:\n\nUnderstand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\nInvestigate if discrepancies help uncover different hidden model behaviours\nUnderstand when and why linear approximations to activation patching fail\nInvestigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is “incorrect”)\n\nSet-up:\nWe load the transformer model GPT2-Small, which has 12 layers, 12 attention heads per layer, embedding size 768 and 4 x 768 = 3,072 neurons in each feed-forward layer. We use GPT2-Small because 1) it is a relatively small transformer model which has comparable behaviour to larger SOTA models, and 2) there is a lot of interpretability literature which focuses on circuits in this model.\n\n\nCode\nimport torch\nimport numpy as np\n\nfrom captum.attr import LayerIntegratedGradients\n\nfrom transformer_lens.utils import get_act_name, get_device\nfrom transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\nfrom transformer_lens.hook_points import HookPoint\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntorch.set_grad_enabled(False)\n\n# device = get_device()\ndevice = torch.device(\"cpu\")\nmodel = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n\n\nLoaded pretrained model gpt2-small into HookedTransformer"
  },
  {
    "objectID": "gpt2_ig_ap.html#integrated-gradients",
    "href": "gpt2_ig_ap.html#integrated-gradients",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "Integrated Gradients",
    "text": "Integrated Gradients\n\n\nCode\ndef run_from_layer_fn(x, original_input, prev_layer, reset_hooks_end=True):\n    # Force the layer before the target layer to output the given values, i.e. pass the given input into the target layer\n    # original_input value does not matter; useful to keep shapes nice, but its activations will be overwritten\n    \n    def fwd_hook(act, hook):\n        x.requires_grad_(True)\n        return x\n    \n    logits = model.run_with_hooks(\n        original_input,\n        fwd_hooks=[(prev_layer.name, fwd_hook)],\n        reset_hooks_end=reset_hooks_end\n    )\n    logit_diff = logits_to_logit_diff(logits).unsqueeze(0)\n    return logit_diff\n\ndef compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer):\n    # Take the model starting from the target layer\n    forward_fn = lambda x: run_from_layer_fn(x, original_input, prev_layer)\n    # Attribute to the target_layer's output\n    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n                                                    baselines=layer_baseline, \n                                                    attribute_to_layer_input=False,\n                                                    return_convergence_delta=True)\n    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error.item()}\")\n    return attributions\n\n\n\n\nCode\nmlp_ig_zero_results = torch.load(\"mlp_ig_zero_results.pt\")\nattn_ig_zero_results = torch.load(\"attn_ig_zero_results.pt\")\n\n\n\n\nCode\n# Gradient attribution using the zero baseline, as originally recommended\nmlp_ig_zero_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\nattn_ig_zero_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\n# Calculate integrated gradients for each layer\nfor layer in range(model.cfg.n_layers):\n    # Gradient attribution on heads\n    hook_name = get_act_name(\"result\", layer)\n    target_layer = model.hook_dict[hook_name]\n    prev_layer_hook = get_act_name(\"z\", layer)\n    prev_layer = model.hook_dict[prev_layer_hook]\n\n    layer_clean_input = clean_cache[prev_layer_hook]\n    layer_corrupt_input = torch.zeros_like(layer_clean_input)\n\n    attributions = compute_layer_to_output_attributions(clean_input, layer_corrupt_input, layer_clean_input, target_layer, prev_layer) # shape [1, seq_len, d_head, d_model]\n    # Calculate attribution score based on mean over each embedding, for each token\n    print(attributions.shape)\n    per_token_score = attributions.mean(dim=3)\n    score = per_token_score.mean(dim=1)\n    attn_ig_zero_results[layer] = score\n\n    # Gradient attribution on MLP neurons\n    hook_name = get_act_name(\"post\", layer)\n    target_layer = model.hook_dict[hook_name]\n    prev_layer_hook = get_act_name(\"mlp_in\", layer)\n    prev_layer = model.hook_dict[prev_layer_hook]\n\n    layer_clean_input = clean_cache[prev_layer_hook]\n    layer_corrupt_input = torch.zeros_like(layer_clean_input)\n    \n    attributions = compute_layer_to_output_attributions(clean_input, layer_corrupt_input, layer_clean_input, target_layer, prev_layer) # shape [1, seq_len, d_model]\n    print(attributions.shape)\n    score = attributions.mean(dim=1)\n    mlp_ig_zero_results[layer] = score\n\ntorch.save(mlp_ig_zero_results, \"mlp_ig_zero_results.pt\")\ntorch.save(attn_ig_zero_results, \"attn_ig_zero_results.pt\")\n\n\n\nError (delta) for blocks.0.attn.hook_result attribution: 1.5820902585983276\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.0.mlp.hook_post attribution: 6.293460845947266\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.1.attn.hook_result attribution: 0.07838684320449829\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.1.mlp.hook_post attribution: 0.66867595911026\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.2.attn.hook_result attribution: 0.22218307852745056\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.2.mlp.hook_post attribution: 1.6563737392425537\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.3.attn.hook_result attribution: 0.7106841206550598\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.3.mlp.hook_post attribution: 3.3847177028656006\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.4.attn.hook_result attribution: 1.0539896488189697\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.4.mlp.hook_post attribution: 0.9486498832702637\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.5.attn.hook_result attribution: 1.1305818557739258\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.5.mlp.hook_post attribution: 3.122290849685669\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.6.attn.hook_result attribution: 0.8233247995376587\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.6.mlp.hook_post attribution: 2.46148681640625\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.7.attn.hook_result attribution: 1.1756279468536377\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.7.mlp.hook_post attribution: 0.29078352451324463\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.8.attn.hook_result attribution: 1.6124217510223389\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.8.mlp.hook_post attribution: -0.8814370632171631\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.9.attn.hook_result attribution: -0.9382901191711426\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.9.mlp.hook_post attribution: 0.8226296305656433\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.10.attn.hook_result attribution: -0.5772985219955444\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.10.mlp.hook_post attribution: 0.05703079327940941\ntorch.Size([1, 17, 3072])\n\nError (delta) for blocks.11.attn.hook_result attribution: -0.18520982563495636\ntorch.Size([1, 17, 12, 768])\n\nError (delta) for blocks.11.mlp.hook_post attribution: -0.8039997816085815\ntorch.Size([1, 17, 3072])\n\n\n\n\nCode\nbound = max(torch.max(mlp_ig_zero_results), abs(torch.min(mlp_ig_zero_results)))\n\nplt.figure(figsize=(75, 10))\nplt.imshow(mlp_ig_zero_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound, aspect=\"auto\")\nplt.title(\"MLP Neuron Gradient Attribution (Integrated Gradients)\")\nplt.xticks(np.arange(0, model.cfg.d_mlp, 250))\nplt.xlabel(\"Neuron Index\")\nplt.yticks(list(range(model.cfg.n_layers)))\nplt.ylabel(\"Layer\")\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nbound = max(torch.max(attn_ig_zero_results), abs(torch.min(attn_ig_zero_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_ig_zero_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Gradient Attribution (Integrated Gradients)\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\n\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\n\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "gpt2_ig_ap.html#activation-patching",
    "href": "gpt2_ig_ap.html#activation-patching",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "Activation Patching",
    "text": "Activation Patching\n\n\nCode\ndef patch_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: int):\n    # Replace the activations for the target neuron with activations from the cached run.\n    cached_activations = cache[hook.name]\n    activations[:, :, neuron_idx] = cached_activations[:, :, neuron_idx]\n    return activations\n\ndef patch_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: int):\n    # Replace the activations for the target attention head with activations from the cached run.\n    cached_activations = cache[hook.name]\n    activations[:, :, head_idx, :] = cached_activations[:, :, head_idx, :]\n    return activations\n\nbaseline_diff = (clean_logit_diff - corrupted_logit_diff).item()\n\n\n\n\nCode\nmlp_patch_results = torch.load(\"mlp_patch_results.pt\")\nattn_patch_results = torch.load(\"attn_patch_results.pt\")\n\n\n\n\nCode\nclass StopExecution(Exception):\n    def _render_traceback_(self):\n        return []\n    \n# Check if we have run activation patching already (expensive)\ntry:\n    mlp_patch_results = torch.load(\"mlp_patch_results.pt\")\n    attn_patch_results = torch.load(\"attn_patch_results.pt\")\n    raise StopExecution\nexcept FileNotFoundError:\n    mlp_patch_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n    attn_patch_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\n    for layer in range(model.cfg.n_layers):\n        # Activation patching on heads\n        for head in range(model.cfg.n_heads):\n            hook_name = get_act_name(\"result\", layer)\n            temp_hook = lambda act, hook: patch_attn_hook(act, hook, corrupted_cache, head)\n\n            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n                patched_logits = model(clean_input)\n\n            patched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n            # Normalise result by clean and corrupted logit difference\n            attn_patch_results[layer, head] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n\n        # Activation patching on MLP neurons\n        for neuron in range(model.cfg.d_mlp):\n            hook_name = get_act_name(\"post\", layer)\n            temp_hook = lambda act, hook: patch_neuron_hook(act, hook, corrupted_cache, neuron)\n            \n            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n                patched_logits = model(clean_input)\n\n            patched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n            # Normalise result by clean and corrupted logit difference\n            mlp_patch_results[layer, neuron] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n    \n    torch.save(mlp_patch_results, \"mlp_patch_results.pt\")\n    torch.save(attn_patch_results, \"attn_patch_results.pt\")\n\n\n\n\nCode\nbound = max(torch.max(mlp_patch_results), abs(torch.min(mlp_patch_results)))\n\nplt.figure(figsize=(75, 10))\nplt.imshow(mlp_patch_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound, aspect=\"auto\")\nplt.title(\"MLP Neuron Gradient Attribution (Activation Patching)\")\nplt.xticks(np.arange(0, model.cfg.d_mlp, 250))\nplt.xlabel(\"Neuron Index\")\nplt.yticks(list(range(model.cfg.n_layers)))\nplt.ylabel(\"Layer\")\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nbound = max(torch.max(attn_patch_results), abs(torch.min(attn_patch_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_patch_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Gradient Attribution (Activation Patching)\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\n\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\n\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "gpt2_ig_ap.html#analysis-of-initial-attribution-methods",
    "href": "gpt2_ig_ap.html#analysis-of-initial-attribution-methods",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "Analysis of initial attribution methods",
    "text": "Analysis of initial attribution methods\n\n\nCode\n# Plot the attribution scores against each other. Correlation: y = x.\n\nx = mlp_ig_zero_results.flatten().numpy()\ny = mlp_patch_results.flatten().numpy()\n\nsns.regplot(x=x, y=y)\nplt.xlabel(\"Integrated Gradients MLP Attribution Scores\")\nplt.ylabel(\"Activation Patching MLP Attribution Scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG and AP attributions for neurons: {np.corrcoef(x, y)[0, 1]}\")\n\nx = attn_ig_zero_results.flatten().numpy()\ny = attn_patch_results.flatten().numpy()\n\nsns.regplot(x=x, y=y)\nplt.xlabel(\"Integrated Gradients Attention Attribution Scores\")\nplt.ylabel(\"Causal Tracing Attention Attribution Scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG and AP attributions for attention: {np.corrcoef(x, y)[0, 1]}\")\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG and AP attributions for neurons: 0.23509090894797813\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG and AP attributions for attention: 0.30637427255128147\n\n\n\n\nCode\ndef get_top_k_by_abs(data, k):\n    _, indices = torch.topk(data.flatten().abs(), k)\n    top_k_values = torch.gather(data.flatten(), 0, indices)\n    formatted_indices = []\n    for idx in indices:\n        layer = idx // model.cfg.d_mlp\n        neuron_pos = idx % model.cfg.d_mlp\n        formatted_indices.append([layer, neuron_pos])\n    return torch.tensor(formatted_indices), top_k_values\n\ndef get_attributions_above_threshold(data, percentile):\n    threshold = torch.min(data) + percentile * (torch.max(data) - torch.min(data))\n    masked_data = torch.where(data &gt; threshold, data, 0)\n    nonzero_indices = torch.nonzero(masked_data)\n    return nonzero_indices, masked_data\n\ntop_mlp_ig_zero_indices, top_mlp_ig_zero_results = get_top_k_by_abs(mlp_ig_zero_results, 30)\ntop_mlp_patch_indices, top_mlp_patch_results = get_top_k_by_abs(mlp_patch_results, 30)\n\ntop_mlp_ig_zero_sets = set([tuple(t.tolist()) for t in top_mlp_ig_zero_indices])\ntop_mlp_patch_sets = set([tuple(t.tolist()) for t in top_mlp_patch_indices])\n\nintersection = top_mlp_ig_zero_sets.intersection(top_mlp_patch_sets)\nunion = top_mlp_ig_zero_sets.union(top_mlp_patch_sets)\njaccard = len(intersection) / len(union)\n\nprint(f\"Jaccard score for MLP neurons: {jaccard}\")\n\n\nJaccard score for MLP neurons: 0.1111111111111111\n\n\n\n\nCode\nfrom sklearn.preprocessing import MaxAbsScaler\n\nmlp_ig_zero_results_1d = mlp_ig_zero_results.flatten().numpy()\nmlp_patch_results_1d = mlp_patch_results.flatten().numpy()\n\n# Mean difference plot with scaled data\n\nscaled_mlp_ig_results_1d = MaxAbsScaler().fit_transform(mlp_ig_zero_results_1d.reshape(-1, 1))\nscaled_mlp_patch_results_1d = MaxAbsScaler().fit_transform(mlp_patch_results_1d.reshape(-1, 1))\n\nmean = np.mean([scaled_mlp_ig_results_1d, scaled_mlp_patch_results_1d], axis=0)\ndiff = scaled_mlp_patch_results_1d - scaled_mlp_ig_results_1d\nmd = np.mean(diff) # Mean of the difference\nsd = np.std(diff, axis=0) # Standard deviation of the difference\n\nplt.figure(figsize=(10, 6))\nsns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\nplt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\nplt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\nplt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\nplt.xlabel(\"Mean of attribution scores per neuron\")\nplt.ylabel(\"Difference (activation patching - integrated gradients) per neuron\")\nplt.title(\"Mean-difference plot of scaled attribution scores from integrated gradients and activation patching\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import MaxAbsScaler\n\nscaled_attn_ig_zero_results = MaxAbsScaler().fit_transform(attn_ig_zero_results)\nscaled_attn_patch_results = MaxAbsScaler().fit_transform(attn_patch_results)\n\ndiff_attn_results = scaled_attn_ig_zero_results - scaled_attn_patch_results\ndiff_attn_results_abs = np.abs(scaled_attn_ig_zero_results) - np.abs(scaled_attn_patch_results)\n\nplt.figure(figsize=(10,10))\nplt.subplot(1, 2, 1)\nplt.imshow(diff_attn_results, cmap=\"RdBu\")\nplt.title(\"Difference in attributions for attention heads\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\n\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\n\nplt.colorbar(orientation=\"horizontal\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(diff_attn_results_abs, cmap=\"RdBu\")\nplt.title(\"Difference in (absolute) attributions for attention heads\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\n\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\n\nplt.colorbar(orientation=\"horizontal\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "gpt2_ig_ap.html#analysis-of-comparable-baselines",
    "href": "gpt2_ig_ap.html#analysis-of-comparable-baselines",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "Analysis of comparable baselines",
    "text": "Analysis of comparable baselines\n\n\nCode\n# Plot the attribution scores against each other. Correlation: y = x.\n\nx = mlp_ig_results.flatten().numpy()\ny = mlp_patch_results.flatten().numpy()\n\nsns.regplot(x=x, y=y)\nplt.xlabel(\"Integrated Gradients (Corrupt Baseline) MLP Attribution Scores\")\nplt.ylabel(\"Activation Patching MLP Attribution Scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG with corrupted baseline and AP attributions for neurons: {np.corrcoef(x, y)[0, 1]}\")\n\nx = attn_ig_results.flatten().numpy()\ny = attn_patch_results.flatten().numpy()\n\nsns.regplot(x=x, y=y)\nplt.xlabel(\"Integrated Gradients (Corrupt Baseline) Attention Attribution Scores\")\nplt.ylabel(\"Causal Tracing Attention Attribution Scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG with corrupted baseline and AP attributions for attention: {np.corrcoef(x, y)[0, 1]}\")\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG with corrupted baseline and AP attributions for neurons: 0.9852227509307566\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG with corrupted baseline and AP attributions for attention: 0.9547628738711134\n\n\nThe correlation between attribution scores for MLP neurons and attention heads is extremely high! This indicates that, with the same baseline, both methods obtain very similar attribution scores.\n\n\nCode\ndef get_top_k_by_abs(data, k):\n    _, indices = torch.topk(data.flatten().abs(), k)\n    top_k_values = torch.gather(data.flatten(), 0, indices)\n    formatted_indices = []\n    for idx in indices:\n        layer = idx // model.cfg.d_mlp\n        neuron_pos = idx % model.cfg.d_mlp\n        formatted_indices.append([layer, neuron_pos])\n    return torch.tensor(formatted_indices), top_k_values\n\ndef get_attributions_above_threshold(data, percentile):\n    threshold = torch.min(data) + percentile * (torch.max(data) - torch.min(data))\n    masked_data = torch.where(data &gt; threshold, data, 0)\n    nonzero_indices = torch.nonzero(masked_data)\n    return nonzero_indices, masked_data\n\ntop_mlp_ig_indices, top_mlp_ig_results = get_top_k_by_abs(mlp_ig_results, 30)\ntop_mlp_patch_indices, top_mlp_patch_results = get_top_k_by_abs(mlp_patch_results, 30)\n\ntop_mlp_ig_sets = set([tuple(t.tolist()) for t in top_mlp_ig_indices])\ntop_mlp_patch_sets = set([tuple(t.tolist()) for t in top_mlp_patch_indices])\n\nintersection = top_mlp_ig_sets.intersection(top_mlp_patch_sets)\nunion = top_mlp_ig_sets.union(top_mlp_patch_sets)\njaccard = len(intersection) / len(union)\n\nprint(f\"Jaccard score for MLP neurons: {jaccard}\")\n\n\nJaccard score for MLP neurons: 0.875\n\n\n\n\nCode\nfrom sklearn.preprocessing import MaxAbsScaler\n\nmlp_ig_results_1d = mlp_ig_results.flatten().numpy()\nmlp_patch_results_1d = mlp_patch_results.flatten().numpy()\n\n# Mean difference plot with scaled data\n\nscaled_mlp_ig_results_1d = MaxAbsScaler().fit_transform(mlp_ig_results_1d.reshape(-1, 1))\nscaled_mlp_patch_results_1d = MaxAbsScaler().fit_transform(mlp_patch_results_1d.reshape(-1, 1))\n\nmean = np.mean([scaled_mlp_ig_results_1d, scaled_mlp_patch_results_1d], axis=0)\ndiff = scaled_mlp_patch_results_1d - scaled_mlp_ig_results_1d\nmd = np.mean(diff) # Mean of the difference\nsd = np.std(diff, axis=0) # Standard deviation of the difference\n\nplt.figure(figsize=(10, 6))\nsns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\nplt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\nplt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\nplt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\nplt.xlabel(\"Mean of attribution scores per neuron\")\nplt.ylabel(\"Difference (activation patching - integrated gradients) per neuron\")\nplt.title(\"Mean-difference plot of scaled attribution scores from integrated gradients and activation patching\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mean difference plot seems to suggest that there is still some proportional bias. The difference between activation patching scores and integrated gradients scores increases as the attribution score deviates from 0. Integrated gradients seems to estimate more extreme attribution scores than activation patching.\n\n\nCode\nfrom sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, RobustScaler\n\nscaled_attn_ig_results = attn_ig_results * 1e5\nscaled_attn_patch_results = attn_patch_results\n\nplt.figure(figsize=(10,10))\nplt.subplot(2, 2, 1)\nplt.imshow(scaled_attn_ig_results, cmap=\"RdBu\", vmin=-0.4, vmax=0.4)\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\nplt.colorbar(orientation=\"horizontal\")\n\nplt.figure(figsize=(10,10))\nplt.subplot(2, 2, 1)\nplt.imshow(scaled_attn_patch_results, cmap=\"RdBu\", vmin=-0.4, vmax=0.4)\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\nplt.colorbar(orientation=\"horizontal\")\n\ndiff_attn_results = scaled_attn_ig_results - scaled_attn_patch_results\ndiff_attn_results_abs = np.abs(scaled_attn_ig_results) - np.abs(scaled_attn_patch_results)\n\nplt.figure(figsize=(10,10))\nplt.subplot(1, 2, 1)\nplt.imshow(diff_attn_results, cmap=\"RdBu\", vmin=-0.2, vmax=0.2)\nplt.title(\"Difference in attributions for attention heads\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\n\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\n\nplt.colorbar(orientation=\"horizontal\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(diff_attn_results_abs, cmap=\"RdBu\", vmin=-0.2, vmax=0.2)\nplt.title(\"Difference in (absolute) attributions for attention heads\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\n\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\n\nplt.colorbar(orientation=\"horizontal\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemaining questions include:\n\nAlthough both methods are aligned when the baselines are the same, this doesn’t mean that they capture the most faithful attribution scores. For instance, if we change the baseline (which is arbitrarily set to some counterfactual value), we could get different components in the circuit. How do we select the best baselines such that faithful circuits are highlighted?\nThere are still some discrepancies in attribution scores, particularly for attention heads. What could be the cause of different attention head attribution scores?"
  },
  {
    "objectID": "gpt2_ig_ap.html#general-purpose-components",
    "href": "gpt2_ig_ap.html#general-purpose-components",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "General-purpose components",
    "text": "General-purpose components\nOf all the outliers, head (9, 6) is the only one which is strongly highlighted by integrated gradients, but not by activation patching. The other outliers have larger attribution scores assigned by integrated gradients compared to activation patching, but are highlighted by both methods.\nHypothesis: the components which are highlighted only by integrated gradients are important attention heads, which are used generically in both the clean run and corrupted run.\n\nThey are not detected as strongly by activation patching, which only takes the difference in logits, i.e. highlights components which are needed for the corrupted run, but not the clean run.\nSuggested by Ferrando and Voita (2024)\n\n\n\nCode\nimport json\n\nclass IOIDataset:\n\n    def __init__(self, src_path: str):\n        with open(src_path) as f:\n            self.data = json.load(f)\n        \n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            prompts_answers = [(d['prompt'], d['answer']) for d in self.data[idx]]\n            return prompts_answers\n        return (self.data[idx]['prompt'], self.data[idx]['answer'])\n\n    def to(self, device):\n        self.data = self.data.to(device)\n        return self\n\n\n\n\nCode\nioi_dataset = IOIDataset(\"datasets/ioi_dataset.json\")[:10_000]\n\n\n\nExperiment 1: zero ablation\nTo test this, we can ablate the components which have statistically significant attribution scores outside the limits of agreement. If they affect the performance, this shows that the components are necessary, even though they are not picked up by activation patching.\n\n\nCode\ndef evaluate_ioi_performance(ioi_dataset: IOIDataset, model: HookedTransformer):\n    num_correct = 0\n    num_eval = 0\n    for prompt, answer in ioi_dataset:\n        if num_eval % 50 == 0:\n            print(f\"Evaluating prompt {num_eval}\")\n        outputs = model.generate(input=prompt, max_new_tokens=3, do_sample=False, verbose=False)\n        generated_answer = outputs.removeprefix(prompt).split()[0]\n        if answer in generated_answer:\n            num_correct += 1\n        num_eval += 1\n    return num_correct / num_eval\n\n\n\n\nCode\n# Measure baseline performance of model on IOI task\nbaseline_performance = evaluate_ioi_performance(ioi_dataset, model)\nprint(baseline_performance)\n\n\n\n\nCode\n# Identify statistically significant outlier components\n\ndiff = np.abs(scaled_attn_patch_results - scaled_attn_ig_results)\ndiff_std = np.std(diff.numpy())\n\nprint(f\"Standard deviation of differences: {diff_std}\")\n\nattn_outliers = []\nfor layer in range(model.cfg.n_layers):\n    for head_idx in range(model.cfg.n_heads):\n        if diff[layer, head_idx] &gt; 1.96*diff_std:\n            attn_outliers.append((layer, head_idx))\n\nprint(attn_outliers)\n\n\nStandard deviation of differences: 0.020343296229839325\n[(7, 3), (8, 10), (9, 6), (9, 9), (10, 6), (11, 10)]\n\n\n\n\nCode\n# Ablate components: zero ablation\n\nall_performance_scores = []\n\nfor layer, head_idx in attn_outliers:\n    attn_hook = get_act_name(\"result\", layer)\n\n    def ablate_hook(activations, hook):\n        activations[:, :, head_idx, :] = 0\n        return activations\n\n    with model.hooks(fwd_hooks=[(attn_hook, ablate_hook)]):\n        performance = evaluate_ioi_performance(ioi_dataset, model)\n        all_performance_scores.append(performance)\n        print(f\"Performance after ablating attention head {(layer, head_idx)}: {performance}\")\n\n# TODO: Mean ablation, random ablation\n\n\n\n\nCode\nprint(baseline_performance)\nprint(all_performance_scores)\n\n\n0.7864\n[0.6981, 0.7877, 0.8052, 0.8009, 0.7245, 0.9361]\n\n\n\n\nCode\nnp.save(\"all_performance_scores.npy\", all_performance_scores)\n\n\n\n\nCode\nfor layer, idx in attn_outliers:\n    print(f\"Attention head {(layer, idx)}\")\n    ig_score = attn_ig_results[layer, idx]\n    patch_score = attn_patch_results[layer, idx]\n    print(f\"IG score: {ig_score:.5f}, AP score: {patch_score:.5f}\\n\")\n\n\nAttention head (7, 3)\nIG score: -0.00000, AP score: -0.10789\n\nAttention head (8, 10)\nIG score: -0.00000, AP score: -0.14233\n\nAttention head (9, 6)\nIG score: -0.00000, AP score: 0.00889\n\nAttention head (9, 9)\nIG score: -0.00000, AP score: -0.11382\n\nAttention head (10, 6)\nIG score: -0.00000, AP score: -0.07559\n\nAttention head (11, 10)\nIG score: 0.00000, AP score: 0.21538\n\n\n\n\n\nCode\nplt.title(\"Model performance after zero ablation of attention head outliers\")\nplt.xlabel(\"Ablated attention head position\")\nplt.ylabel(\"Model performance on IOI tasks\")\n\nxs = [\"None\"] + [str(t) for t in attn_outliers]\nys = [baseline_performance] + all_performance_scores\n\nplt.bar(xs, ys)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Correlation between difference in attribution scores and difference in performance\n\nperformance_differences = []\nig_outlier_scores = []\nap_outlier_scores = []\nscore_differences = []\n\nfor i in range(len(all_performance_scores)):\n    performance_differences.append(all_performance_scores[i] - baseline_performance)\n    layer, attn_idx = attn_outliers[i]\n    ap_outlier_scores.append(attn_patch_results[layer, attn_idx])\n    ig_outlier_scores.append(attn_ig_results[layer, attn_idx])\n    score_diff = attn_patch_results[layer, attn_idx] - attn_ig_results[layer, attn_idx]\n    score_differences.append(score_diff)\n\n\n\n\nCode\nsns.regplot(x=score_differences, y=performance_differences)\nplt.ylabel(\"Difference between performance and baseline performance\")\nplt.xlabel(\"Difference between patching and IG attribution scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between attribution score differences and performance score differences: {np.corrcoef(score_differences, performance_differences)[0, 1]}\")\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between attribution score differences and performance score differences: 0.8298786622776018\n\n\n\n\nCode\n# Correlation between attribution scores and performance change\n\nsns.regplot(x=ig_outlier_scores, y=performance_differences)\nplt.ylabel(\"Difference between performance and baseline performance\")\nplt.xlabel(\"IG attribution scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG attribution score and performance score differences: {np.corrcoef(ig_outlier_scores, performance_differences)[0, 1]}\")\n\nsns.regplot(x=ap_outlier_scores, y=performance_differences)\nplt.ylabel(\"Difference between performance and baseline performance\")\nplt.xlabel(\"Activation Patching attribution scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG attribution score and performance score differences: {np.corrcoef(ap_outlier_scores, performance_differences)[0, 1]}\")\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG attribution score and performance score differences: 0.7296915879554248\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG attribution score and performance score differences: 0.8298774188113335\n\n\n\nAblating head (9, 6) does not have a strong effect on the performance.\n\nConclusion: components which are only identified by integrated gradients may not be important for the specific task.\n\nInterestingly, ablating heads identified as moderately important by activation patching (e.g. (8, 10), (9, 6), and (9, 9)) do not have significant impact on the performance either.\n\nConclusion: neither method identifies the minimal set of important attention heads.\nComparison to original IOI paper: under mean ablation, these heads (and 9.6) are highlighted and impact performance more noticeably.\n\nThere is not really a strong pattern / correlation between components which have higher attribution scores under IG or AP, and impact on performance.\n\n\n\nExperiment 2: Mean ablation\nInstead of using zero ablation, we use mean ablation to study the effect of a component’s removal on the model’s performance.\n\n\nCode\nimport random\n\n# Get mean activations\nmodel = model.to(\"cpu\")\n\nattn_outlier_hooks = [get_act_name(\"result\", layer_idx) for layer_idx, _ in attn_outliers]\n\nrandom_prompts = random.sample(ioi_dataset, 100)\nprompts_tokens = model.to_tokens([p for p, _ in random_prompts])\n_, prompt_cache = model.run_with_cache(prompts_tokens, names_filter=lambda x: x in attn_outlier_hooks)\n\nmean_activations = {}\nfor key in prompt_cache.keys():\n    mean_values_over_prompts = torch.mean(prompt_cache[key], dim=0)\n    mean_activations[key] = torch.mean(mean_values_over_prompts, dim=0)\n\n\nMoving model to device:  cpu\n\n\n\n\nCode\n# Ablate components: mean ablation\n\nall_performance_scores_mean_ablation = []\n\nfor layer, head_idx in attn_outliers:\n    attn_hook = get_act_name(\"result\", layer)\n\n    def ablate_hook(activations, hook):\n        mean_hook_acts = mean_activations[hook.name]\n        activations[:, :, head_idx, :] = mean_hook_acts[head_idx]\n        return activations\n\n    with model.hooks(fwd_hooks=[(attn_hook, ablate_hook)]):\n        performance = evaluate_ioi_performance(ioi_dataset, model)\n        all_performance_scores_mean_ablation.append(performance)\n        print(f\"Performance after mean ablating attention head {(layer, head_idx)}: {performance}\")\n\n\n\n\nCode\nprint(all_performance_scores_mean_ablation)\n\nnp.save(\"all_performance_scores_mean_ablation.npy\", all_performance_scores_mean_ablation)\n\n\n[]\n\n\n\n\nCode\nplt.title(\"Model performance after mean ablation of attention head outliers\")\nplt.xlabel(\"Ablated attention head position\")\nplt.ylabel(\"Model performance on IOI tasks\")\n\nxs = [\"None\"] + [str(t) for t in attn_outliers]\nbaseline_performance = 0.7864\nys = [baseline_performance] + all_performance_scores_mean_ablation\n\nplt.bar(xs, ys)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Correlation between difference in attribution scores and difference in performance\n\nmean_performance_differences = []\nig_outlier_scores = []\nap_outlier_scores = []\nscore_differences = []\n\nfor i in range(len(all_performance_scores_mean_ablation)):\n    mean_performance_differences.append(all_performance_scores_mean_ablation[i] - baseline_performance)\n    layer, attn_idx = attn_outliers[i]\n    ap_outlier_scores.append(attn_patch_results[layer, attn_idx])\n    ig_outlier_scores.append(attn_ig_results[layer, attn_idx])\n    score_diff = attn_patch_results[layer, attn_idx] - attn_ig_results[layer, attn_idx]\n    score_differences.append(score_diff)\n\n\n\n\nCode\n# Correlation between attribution scores and performance change\n\nsns.regplot(x=ig_outlier_scores, y=mean_performance_differences)\nplt.ylabel(\"Difference between performance and baseline performance under mean ablation\")\nplt.xlabel(\"IG attribution scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG attribution score and performance score difference under mean ablation: {np.corrcoef(ig_outlier_scores, mean_performance_differences)[0, 1]}\")\n\nsns.regplot(x=ap_outlier_scores, y=mean_performance_differences)\nplt.ylabel(\"Difference between performance and baseline performance\")\nplt.xlabel(\"Activation Patching attribution scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG attribution score and performance score difference under mean ablation: {np.corrcoef(ap_outlier_scores, mean_performance_differences)[0, 1]}\")\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG attribution score and performance score difference under mean ablation: 0.7868078399469188\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG attribution score and performance score difference under mean ablation: 0.8659126942995752\n\n\n\nAblating head (9, 6) has a moderate but significant effect on the performance.\n\nSignificant at p=0.05 (~15%) improvement in performance under mean ablation.\nConclusion: components which are only identified by integrated gradients may still be important for the specific task.\n\nFor some heads identified as important by both methods (e.g. (8, 10) and (9, 9)), mean ablation does not have significant impact on the performance either.\n\nConclusion: neither method identifies the minimal set of important attention heads.\n\nThere is moderate correlation between components which have higher attribution scores under IG or AP, and impact on performance."
  },
  {
    "objectID": "gpt2_ig_ap.html#optimal-contrastive-pairs",
    "href": "gpt2_ig_ap.html#optimal-contrastive-pairs",
    "title": "Integrated Gradients vs Activation Patching in GPT2-Small",
    "section": "“Optimal” contrastive pairs",
    "text": "“Optimal” contrastive pairs\nConsider head (9, 6), which according to the IOI paper is a name mover head:\n“Name Mover Heads output the remaining name. They are active at END, attend to previous names in the sentence, and copy the names they attend to”.\nWe use a different contrastive pair related to the IOI task, to try and get a high attribution score under IG. We change the corrupted prompt such that 1) the output should change from “John” to “Mary”, and 2) the name copying head (hypothesised role of head 9.6) is even more important.\n\n\nCode\nclean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\ncorrupted_prompt = \"After the cat and the dog went to the store, Mary gave a bottle of milk to\"\n\nclean_input, corrupted_input = model.to_tokens([clean_prompt, corrupted_prompt])\n\n# Explicitly calculate and expose the result for each attention head\nmodel.set_use_attn_result(True)\nmodel.set_use_hook_mlp_in(True)\n\nclean_logits, clean_cache = model.run_with_cache(clean_input)\nclean_logit_diff = logits_to_logit_diff(clean_logits)\nprint(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n\ncorrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)\ncorrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\nprint(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n\n\nClean logit difference: 1.458\nCorrupted logit difference: -0.274\n\n\n\n\nCode\n# Gradient attribution on heads\nhook_name = get_act_name(\"result\", 9)\ntarget_layer = model.hook_dict[hook_name]\nprev_layer_hook = get_act_name(\"z\", 9)\nprev_layer = model.hook_dict[prev_layer_hook]\n\nlayer_clean_input = clean_cache[prev_layer_hook]\nlayer_corrupt_input = corrupted_cache[prev_layer_hook]\n\nattributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_head, d_model]\n# Calculate attribution score based on mean over each embedding, for each token\nper_token_score = attributions.mean(dim=3)\nscore = per_token_score.mean(dim=1)\n\nprint(score[:,6])\n\n\n\nError (delta) for blocks.9.attn.hook_result attribution: -0.9106521606445312\ntensor([3.3620e-08])\n\n\n\n\nCode\n# Original scores\n\nprint(attn_ig_results[9, 6])\nprint(attn_patch_results[9, 6])\n\n\ntensor(-4.5160e-07)\ntensor(0.0089)\n\n\n\n\nCode\nvisualise_attn_interpolated_outputs(9, 6)\n\n\n[tensor([1.4576], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4568], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4553], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4532], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4504], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4471], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4431], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4385], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4333], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4276], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4213], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4146], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.4073], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3996], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3914], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3828], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3739], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3646], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3551], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3453], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3352], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3250], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3146], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.3041], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2936], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2830], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2725], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2620], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2516], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2413], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2313], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2214], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2118], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.2025], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1936], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1849], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1767], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1690], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1617], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1548], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1485], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1428], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1376], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1329], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1289], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1255], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1227], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1206], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1191], grad_fn=&lt;UnsqueezeBackward0&gt;), tensor([1.1183], grad_fn=&lt;UnsqueezeBackward0&gt;)]\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get activation patching scores\n\nhook_name = get_act_name(\"result\", 9)\ntemp_hook = lambda act, hook: patch_attn_hook(act, hook, corrupted_cache, 6)\n\nwith model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n    patched_logits = model(clean_input)\n\npatched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n# Normalise result by clean and corrupted logit difference\nap_score = (patched_logit_diff - clean_logit_diff) / baseline_diff\nprint(ap_score)\n\n\ntensor(-0.0456)\n\n\nChanging the baseline inputs such that the output gradients vary more doesn’t necessarily seem to affect IG too much, but it does increase the magnitude of the activation patching score.\nIt seems clear that the baselines used for attribution methods are extremely important hyper-parameters, but there is no clear intuition as to which baseline is “best” for evaluating specific model behaviours. This provides motivation for a new method which identifies the optimal counterfactuals to make attribution methods highlight specific components."
  },
  {
    "objectID": "verification.html",
    "href": "verification.html",
    "title": "Gradient-based Verification of Circuits",
    "section": "",
    "text": "Code\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport torch\nimport pandas as pd\nimport numpy as np\n\nfrom functools import partial\nfrom typing import Optional\n\nfrom captum.attr import LayerIntegratedGradients\n\nfrom transformer_lens.utils import get_act_name, get_device\nfrom transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\nfrom transformer_lens.hook_points import HookPoint\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom utils import plot_attn\n\n\n/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\nCode\nfrom enum import Enum\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Task(Enum):\n    IOI = 1\n    GENDER_BIAS = 2\n    GREATER_THAN = 3\n    CAPITAL_COUNTRY = 4\n    SVA = 5\n    HYPERNYMY = 6\n\n# Implementation of dataset loader based on https://github.com/hannamw/eap-ig-faithfulness\n\ndef collate_EAP(xs, task: Task):\n    clean, corrupted, labels = zip(*xs)\n    clean = list(clean)\n    corrupted = list(corrupted)\n    if task != Task.HYPERNYMY:\n        labels = torch.tensor(labels)\n    return clean, corrupted, labels\n\nclass TaskDataset(Dataset):\n    def __init__(self, task: Task):\n        filename = task.name.lower()\n        self.task = task\n        self.df = pd.read_csv(f'datasets/{filename}.csv')\n\n    def __len__(self):\n        return len(self.df)\n    \n    def shuffle(self):\n        self.df = self.df.sample(frac=1)\n\n    def head(self, n: int):\n        self.df = self.df.head(n)\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        label = None\n\n        if self.task == Task.IOI:\n            label = [row['correct_idx'], row['incorrect_idx']]\n            return row['clean'], row['corrupted_hard'], label\n        \n        if self.task == Task.GREATER_THAN:\n            label = row['correct_idx']\n        elif self.task == Task.HYPERNYMY:\n            answer = torch.tensor(eval(row['answers_idx']))\n            corrupted_answer = torch.tensor(eval(row['corrupted_answers_idx']))\n            label = [answer, corrupted_answer]\n        elif self.task == Task.CAPITAL_COUNTRY:\n            label = [row['country_idx'], row['corrupted_country_idx']]\n        elif self.task == Task.GENDER_BIAS:\n            label = [row['clean_answer_idx'], row['corrupted_answer_idx']]\n        elif self.task == Task.SVA:\n            label = row['plural']\n        else:\n            raise ValueError(f'Got invalid task: {self.task}')\n        \n        return row['clean'], row['corrupted'], label\n    \n    def to_dataloader(self, batch_size: int):\n        return DataLoader(self, batch_size=batch_size, collate_fn=partial(collate_EAP, task=self.task))\n\n\n\n\nCode\ndef logit_diff_metric(logits, metric_labels):\n    correct_index = metric_labels[:, 0]\n    incorrect_index = metric_labels[:, 1]\n    logits_last = logits[:, -1, :]\n    batch_size = logits.size(0)\n    correct_logits = logits_last[torch.arange(batch_size), correct_index]\n    incorrect_logits = logits_last[torch.arange(batch_size), incorrect_index]\n    return correct_logits - incorrect_logits\n\n\n\n\nCode\ntorch.set_grad_enabled(False)\n\ndevice = get_device()\n# device = torch.device(\"cpu\")\nmodel = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n\n# Explicitly calculate and expose the result for each attention head\nmodel.set_use_attn_result(True)\nmodel.set_use_hook_mlp_in(True)\n\n\nLoaded pretrained model gpt2-small into HookedTransformer\n\n\n\nAblation studies\n\n\nCode\nioi_dataset = TaskDataset(Task.IOI)\nioi_dataloader = ioi_dataset.to_dataloader(batch_size=10)\n\nclean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n\nclean_tokens = model.to_tokens(clean_input)\ncorrupted_tokens = model.to_tokens(corrupted_input)\n\nclean_logits, clean_cache = model.run_with_cache(clean_tokens)\nclean_logit_diff = logit_diff_metric(clean_logits, labels)\nprint(f\"Clean logit difference: {clean_logit_diff}\")\n\ncorrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\ncorrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\nprint(f\"Corrupted logit difference: {corrupted_logit_diff}\")\n\n\nClean logit difference: tensor([-0.0307, -0.9269, -0.4937,  2.2320,  0.6754,  4.0447, -0.1785,  1.1947,\n         1.1514,  1.7507], device='cuda:0')\nCorrupted logit difference: tensor([-0.0387, -0.9451, -0.5103,  2.2153,  0.6299, -3.2074, -0.1823,  1.1766,\n        -3.0072,  1.7392], device='cuda:0')\n\n\n\n\nCode\nioi_ig_zero_mlp = torch.load(\"saved_results/ioi_ig_zero_mlp.pt\")\nioi_ig_zero_attn = torch.load(\"saved_results/ioi_ig_zero_attn.pt\")\n\nioi_ig_mlp = torch.load(\"saved_results/ioi_ig_mlp.pt\")\nioi_ig_attn = torch.load(\"saved_results/ioi_ig_attn.pt\")\n\nioi_ap_mlp = torch.load(\"saved_results/ioi_ap_mlp.pt\")\nioi_ap_attn = torch.load(\"saved_results/ioi_ap_attn.pt\")\n\nig_zero_mlp, ig_zero_attn = ioi_ig_zero_mlp[0], ioi_ig_zero_attn[0]\nig_mlp, ap_mlp, ig_attn, ap_attn = ioi_ig_mlp[0], ioi_ap_mlp[0], ioi_ig_attn[0], ioi_ap_attn[0]\n\n\n\n\nCode\ndef mask_insignificant(results):\n    # Only keep components which are above a baseline\n    mean = torch.mean(results)\n    std = torch.std(results)\n    return torch.where(results.abs() &gt; mean + std, results, 0.)\n\n\n\n\nCode\ndef run_ablated_attn(model: HookedTransformer, layer_idx, head_idx, corrupted_cache, metric, metric_labels, *model_args, **model_kwargs):\n    # Patch in corrupted activations\n    def ablate_head_hook(act, hook):\n        act[:, :, head_idx] = corrupted_cache[hook.name][:, :, head_idx]\n        return act\n    \n    layer_name = get_act_name(\"result\", layer_idx)\n\n    logits = model.run_with_hooks(*model_args, **model_kwargs, fwd_hooks=[(layer_name, ablate_head_hook)])\n    return metric(logits, metric_labels)\n\ndef run_ablated_neuron(model, layer_idx, neuron_idx, corrupted_cache, metric, metric_labels, *model_args, **model_kwargs):\n    ablate_head_hook = lambda act, hook: corrupted_cache[hook.name][:, :, neuron_idx]\n    layer_name = get_act_name(\"post\", layer_idx)\n\n    logits = model.run_with_hooks(*model_args, **model_kwargs, fwd_hooks=[(layer_name, ablate_head_hook)])\n    return metric(logits, metric_labels)\n\n\n\n\nCode\ndef test_ablated_ioi_performance(layer_idx, pos, is_attn: bool):\n    print(f\"Test IOI performance with ablated {layer_idx, pos}\")\n    test_dataset = TaskDataset(Task.IOI)\n    test_dataloader = test_dataset.to_dataloader(batch_size=10)\n\n    mean_performance = 0\n    for clean_input, corrupted_input, labels in test_dataloader:\n        clean_tokens = model.to_tokens(clean_input)\n        corrupted_tokens = model.to_tokens(corrupted_input)\n        _, corrupted_cache = model.run_with_cache(corrupted_tokens)\n        \n        if is_attn:\n            performance = run_ablated_attn(model, layer_idx, pos, corrupted_cache, logit_diff_metric, labels, clean_tokens)\n        else:\n            performance = run_ablated_neuron(model, layer_idx, pos, corrupted_cache, logit_diff_metric, labels, clean_tokens)\n        \n        mean_performance += performance.sum()\n\n    mean_performance /= len(test_dataset)\n    print(f\"Mean performance: {mean_performance}\")\n    return mean_performance\n\n\n\n\nCode\n# Baseline performance\ntest_dataset = TaskDataset(Task.IOI)\ntest_dataloader = test_dataset.to_dataloader(batch_size=10)\n\nbaseline_performance = 0\n\nfor clean_input, corrupted_input, labels in test_dataloader:\n    clean_tokens = model.to_tokens(clean_input)\n    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n    baseline_performance += logit_diff_metric(clean_logits, labels).sum(dim=-1)\n\nbaseline_performance /= len(test_dataset)\nprint(f\"Baseline performance: {baseline_performance}\")\n\n\nBaseline performance: 0.6607611179351807\n\n\n\n\nCode\nap_attn_sig = torch.nonzero(mask_insignificant(ap_attn)) # Shape [n_indices, 2]\n\nap_attn_performances = []\nfor layer_idx, head_idx in ap_attn_sig:\n    mean_performance = test_ablated_ioi_performance(layer_idx, head_idx, is_attn=True)\n    ap_attn_performances.append(mean_performance)\n\ntorch.save(ap_attn_performances, \"saved_results/ap_attn_ablated_performances.pt\")\n\n\nTest IOI performance with ablated (tensor(0), tensor(6))\nMean performance: 0.6678401231765747\nTest IOI performance with ablated (tensor(0), tensor(8))\nMean performance: 0.6605553030967712\nTest IOI performance with ablated (tensor(1), tensor(9))\nMean performance: 0.6594219207763672\nTest IOI performance with ablated (tensor(3), tensor(4))\nMean performance: 0.625079870223999\nTest IOI performance with ablated (tensor(4), tensor(10))\nMean performance: 0.6582724452018738\nTest IOI performance with ablated (tensor(5), tensor(11))\nMean performance: 0.653248131275177\nTest IOI performance with ablated (tensor(8), tensor(6))\nMean performance: 0.3205397129058838\nTest IOI performance with ablated (tensor(8), tensor(10))\nMean performance: 0.26987776160240173\nTest IOI performance with ablated (tensor(9), tensor(5))\nMean performance: 0.6275280117988586\nTest IOI performance with ablated (tensor(9), tensor(9))\nMean performance: 0.4603160321712494\nTest IOI performance with ablated (tensor(10), tensor(2))\nMean performance: 0.6213502883911133\nTest IOI performance with ablated (tensor(10), tensor(7))\nMean performance: 1.2449369430541992\nTest IOI performance with ablated (tensor(10), tensor(10))\nMean performance: 0.5858502388000488\nTest IOI performance with ablated (tensor(11), tensor(0))\nMean performance: 0.6560553312301636\nTest IOI performance with ablated (tensor(11), tensor(2))\nMean performance: 0.8090091943740845\nTest IOI performance with ablated (tensor(11), tensor(3))\nMean performance: 0.6354880332946777\nTest IOI performance with ablated (tensor(11), tensor(6))\nMean performance: 0.6790978908538818\nTest IOI performance with ablated (tensor(11), tensor(8))\nMean performance: 0.6420082449913025\n\n\n\n\nCode\nap_attn_performance_diff = {}\n\nfor (layer, head), score in zip(ap_attn_sig, ap_attn_performances):\n    ap_attn_performance_diff[(layer.item(), head.item())] = (score - baseline_performance).item()\n\nplt.figure(figsize=(10,4))\nplt.bar([str(k) for k in ap_attn_performance_diff.keys()], ap_attn_performance_diff.values())\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nig_attn_sig = torch.nonzero(mask_insignificant(ig_attn)) # Shape [n_indices, 2]\n\nig_attn_performances = []\nfor layer_idx, head_idx in ig_attn_sig:\n    mean_performance = test_ablated_ioi_performance(layer_idx, head_idx, is_attn=True)\n    ig_attn_performances.append(mean_performance)\n\ntorch.save(ig_attn_performances, \"saved_results/ig_attn_ablated_performances.pt\")\n\n\nTest IOI performance with ablated (tensor(0), tensor(4))\nMean performance: 0.6606829762458801\nTest IOI performance with ablated (tensor(0), tensor(6))\nMean performance: 0.6678401231765747\nTest IOI performance with ablated (tensor(1), tensor(9))\nMean performance: 0.6594219207763672\nTest IOI performance with ablated (tensor(3), tensor(4))\nMean performance: 0.625079870223999\nTest IOI performance with ablated (tensor(4), tensor(10))\nMean performance: 0.6582724452018738\nTest IOI performance with ablated (tensor(5), tensor(11))\nMean performance: 0.653248131275177\nTest IOI performance with ablated (tensor(8), tensor(6))\nMean performance: 0.3205397129058838\nTest IOI performance with ablated (tensor(8), tensor(10))\nMean performance: 0.26987776160240173\nTest IOI performance with ablated (tensor(9), tensor(5))\nMean performance: 0.6275280117988586\nTest IOI performance with ablated (tensor(9), tensor(9))\nMean performance: 0.4603160321712494\nTest IOI performance with ablated (tensor(10), tensor(2))\nMean performance: 0.6213502883911133\nTest IOI performance with ablated (tensor(10), tensor(7))\nMean performance: 1.2449369430541992\nTest IOI performance with ablated (tensor(10), tensor(10))\nMean performance: 0.5858502388000488\nTest IOI performance with ablated (tensor(11), tensor(0))\nMean performance: 0.6560553312301636\nTest IOI performance with ablated (tensor(11), tensor(2))\nMean performance: 0.8090091943740845\nTest IOI performance with ablated (tensor(11), tensor(3))\nMean performance: 0.6354880332946777\nTest IOI performance with ablated (tensor(11), tensor(6))\nMean performance: 0.6790978908538818\nTest IOI performance with ablated (tensor(11), tensor(8))\nMean performance: 0.6420082449913025\n\n\n\n\nCode\nig_attn_performance_diff = {}\n\nfor (layer, head), score in zip(ig_attn_sig, ig_attn_performances):\n    ig_attn_performance_diff[(layer.item(), head.item())] = (score - baseline_performance).item()\n\nprint(ig_attn_performance_diff)\n\nplt.figure(figsize=(10,4))\nplt.bar([str(k) for k in ig_attn_performance_diff.keys()], ig_attn_performance_diff.values())\nplt.tight_layout()\nplt.show()\n\n\n{(0, 4): -7.814168930053711e-05, (0, 6): 0.007079005241394043, (1, 9): -0.0013391971588134766, (3, 4): -0.03568124771118164, (4, 10): -0.0024886727333068848, (5, 11): -0.007512986660003662, (8, 6): -0.3402214050292969, (8, 10): -0.39088335633277893, (9, 5): -0.03323310613632202, (9, 9): -0.20044508576393127, (10, 2): -0.03941082954406738, (10, 7): 0.5841758251190186, (10, 10): -0.07491087913513184, (11, 0): -0.00470578670501709, (11, 2): 0.1482480764389038, (11, 3): -0.02527308464050293, (11, 6): 0.018336772918701172, (11, 8): -0.018752872943878174}\n\n\n\n\n\n\n\n\n\n\n\nCode\nig_attn_sig_set = set([(layer, head) for layer, head in ig_attn_sig.tolist()])\nap_attn_sig_set = set([(layer, head) for layer, head in ap_attn_sig.tolist()])\n\nattn_exclusive_to_ig = ig_attn_sig_set - ap_attn_sig_set\nattn_exclusive_to_ap = ap_attn_sig_set - ig_attn_sig_set\nattn_agreed = ig_attn_sig_set.intersection(ap_attn_sig_set)\n\nprint(f\"Attention heads exclusive to IG: {attn_exclusive_to_ig}\")\nprint(f\"Attention heads exclusive to AP: {attn_exclusive_to_ap}\")\nprint(f\"Attention heads important in IG and AP: {attn_agreed}\")\n\n\nAttention heads exclusive to IG: {(0, 4)}\nAttention heads exclusive to AP: {(0, 8)}\nAttention heads important in IG and AP: {(4, 10), (5, 11), (3, 4), (11, 0), (9, 9), (11, 3), (8, 10), (10, 7), (0, 6), (11, 6), (9, 5), (10, 10), (11, 2), (8, 6), (11, 8), (1, 9), (10, 2)}\n\n\n\n\nCode\nioi_paper_attn_heads = set([(0,1), (3,0), (0,10), (2,2), (4,11), (5,5), (6,9), (5,8), (5,9), (7,3), (7,9), (8,6), (8,10), (10,7), (11,10), (9,9), (9,6), (10,0), (9,0), (9,7), (10,1), (10,2), (10,6), (10,10), (11,2), (11,9)])\n\nprint(f\"Attention heads also mentioned in paper: {ioi_paper_attn_heads.intersection(attn_agreed)}\")\nprint(f\"Attention heads not picked up: {ioi_paper_attn_heads - attn_agreed}\")\nprint(f\"Spurious attention heads: {attn_agreed - ioi_paper_attn_heads}\")\n\n\nAttention heads also mentioned in paper: {(9, 9), (8, 10), (10, 7), (10, 10), (11, 2), (8, 6), (10, 2)}\nAttention heads not picked up: {(10, 0), (10, 6), (2, 2), (3, 0), (5, 9), (4, 11), (9, 7), (0, 1), (0, 10), (11, 10), (7, 3), (7, 9), (9, 0), (5, 5), (5, 8), (9, 6), (10, 1), (11, 9), (6, 9)}\nSpurious attention heads: {(4, 10), (5, 11), (3, 4), (11, 0), (11, 3), (0, 6), (11, 6), (9, 5), (11, 8), (1, 9)}\n\n\n\n\nCode\nig_zero_attn_sig = torch.nonzero(mask_insignificant(ig_zero_attn)) # Shape [n_indices, 2]\n\nig_zero_attn_performances = []\nfor layer_idx, head_idx in ig_zero_attn_sig:\n    mean_performance = test_ablated_ioi_performance(layer_idx, head_idx, is_attn=True)\n    ig_zero_attn_performances.append(mean_performance)\n\ntorch.save(ig_zero_attn_performances, \"saved_results/ig_zero_attn_performances.pt\")\n\n\nTest IOI performance with ablated (tensor(0), tensor(0))\nMean performance: 0.6613448262214661\nTest IOI performance with ablated (tensor(0), tensor(1))\nMean performance: 0.6332634091377258\nTest IOI performance with ablated (tensor(0), tensor(2))\nMean performance: 0.660415768623352\nTest IOI performance with ablated (tensor(0), tensor(5))\nMean performance: 0.660062313079834\nTest IOI performance with ablated (tensor(0), tensor(6))\nMean performance: 0.6678401231765747\nTest IOI performance with ablated (tensor(0), tensor(7))\nMean performance: 0.6609131693840027\nTest IOI performance with ablated (tensor(0), tensor(8))\nMean performance: 0.6605553030967712\nTest IOI performance with ablated (tensor(0), tensor(9))\nMean performance: 0.6601138114929199\nTest IOI performance with ablated (tensor(0), tensor(10))\nMean performance: 0.6625092029571533\nTest IOI performance with ablated (tensor(1), tensor(4))\nMean performance: 0.6621139645576477\nTest IOI performance with ablated (tensor(1), tensor(8))\nMean performance: 0.6597142815589905\nTest IOI performance with ablated (tensor(2), tensor(10))\nMean performance: 0.6619985699653625\nTest IOI performance with ablated (tensor(2), tensor(11))\nMean performance: 0.659731388092041\nTest IOI performance with ablated (tensor(6), tensor(2))\nMean performance: 0.6606764793395996\nTest IOI performance with ablated (tensor(8), tensor(0))\nMean performance: 0.6633226275444031\nTest IOI performance with ablated (tensor(8), tensor(10))\nMean performance: 0.26987776160240173\nTest IOI performance with ablated (tensor(9), tensor(4))\nMean performance: 0.6968041658401489\nTest IOI performance with ablated (tensor(9), tensor(9))\nMean performance: 0.4603160321712494\nTest IOI performance with ablated (tensor(10), tensor(0))\nMean performance: 0.4728143513202667\nTest IOI performance with ablated (tensor(10), tensor(2))\nMean performance: 0.6213502883911133\nTest IOI performance with ablated (tensor(10), tensor(6))\nMean performance: 0.576394259929657\nTest IOI performance with ablated (tensor(10), tensor(9))\nMean performance: 0.6523271799087524\nTest IOI performance with ablated (tensor(11), tensor(1))\nMean performance: 0.659042239189148\nTest IOI performance with ablated (tensor(11), tensor(8))\nMean performance: 0.6420082449913025\n\n\n\n\nCode\nig_zero_attn_performance_diff = {}\n\nfor (layer, head), score in zip(ig_zero_attn_sig, ig_zero_attn_performances):\n    ig_zero_attn_performance_diff[(layer.item(), head.item())] = (score - baseline_performance).item()\n\nprint(ig_zero_attn_performance_diff)\n\nplt.figure(figsize=(10,4))\nplt.bar([str(k) for k in ig_zero_attn_performance_diff.keys()], ig_zero_attn_performance_diff.values())\nplt.tight_layout()\nplt.show()\n\n\n{(0, 0): 0.0005837082862854004, (0, 1): -0.027497708797454834, (0, 2): -0.0003453493118286133, (0, 5): -0.0006988048553466797, (0, 6): 0.007079005241394043, (0, 7): 0.00015205144882202148, (0, 8): -0.00020581483840942383, (0, 9): -0.0006473064422607422, (0, 10): 0.0017480850219726562, (1, 4): 0.001352846622467041, (1, 8): -0.0010468363761901855, (2, 10): 0.0012374520301818848, (2, 11): -0.0010297298431396484, (6, 2): -8.463859558105469e-05, (8, 0): 0.002561509609222412, (8, 10): -0.39088335633277893, (9, 4): 0.03604304790496826, (9, 9): -0.20044508576393127, (10, 0): -0.18794676661491394, (10, 2): -0.03941082954406738, (10, 6): -0.08436685800552368, (10, 9): -0.008433938026428223, (11, 1): -0.0017188787460327148, (11, 8): -0.018752872943878174}\n\n\n\n\n\n\n\n\n\n\n\nCode\nig_zero_attn_sig_set = set([(layer, head) for layer, head in ig_zero_attn_sig.tolist()])\nattn_exclusive_to_ig_zero = ig_zero_attn_sig_set - ap_attn_sig_set\nattn_exclusive_to_ap_zero = ap_attn_sig_set - ig_zero_attn_sig_set\nattn_agreed_zero = ig_zero_attn_sig_set.intersection(ap_attn_sig_set)\nattn_total_zero = ig_zero_attn_sig_set.union(ap_attn_sig_set)\n\nprint(f\"Attention heads exclusive to IG: {attn_exclusive_to_ig_zero}\")\nprint(f\"Attention heads exclusive to AP: {attn_exclusive_to_ap_zero}\")\nprint(f\"Attention heads important in IG and AP: {attn_agreed_zero}\")\n\nprint(f\"Attention heads also mentioned in paper: {ioi_paper_attn_heads.intersection(attn_total_zero)}\")\nprint(f\"Attention heads not picked up: {ioi_paper_attn_heads - attn_total_zero}\")\nprint(f\"Spurious attention heads: {attn_total_zero - ioi_paper_attn_heads}\")\n\n\nAttention heads exclusive to IG: {(0, 1), (0, 7), (6, 2), (11, 1), (0, 10), (0, 0), (2, 10), (1, 8), (0, 9), (8, 0), (1, 4), (0, 2), (10, 0), (10, 6), (0, 5), (10, 9), (2, 11), (9, 4)}\nAttention heads exclusive to AP: {(4, 10), (5, 11), (3, 4), (11, 0), (11, 3), (10, 7), (11, 6), (9, 5), (10, 10), (11, 2), (8, 6), (1, 9)}\nAttention heads important in IG and AP: {(9, 9), (8, 10), (0, 6), (0, 8), (10, 2), (11, 8)}\nAttention heads also mentioned in paper: {(0, 1), (0, 10), (9, 9), (8, 10), (10, 7), (10, 10), (11, 2), (10, 0), (10, 6), (8, 6), (10, 2)}\nAttention heads not picked up: {(9, 0), (5, 5), (11, 10), (5, 8), (6, 9), (9, 6), (10, 1), (11, 9), (3, 0), (7, 3), (7, 9), (2, 2), (5, 9), (4, 11), (9, 7)}\nSpurious attention heads: {(3, 4), (8, 0), (9, 5), (0, 2), (0, 5), (10, 9), (0, 8), (11, 8), (2, 11), (1, 9), (6, 2), (9, 4), (11, 1), (0, 7), (2, 10), (1, 8), (4, 10), (5, 11), (0, 0), (11, 0), (11, 3), (0, 9), (1, 4), (0, 6), (11, 6)}\n\n\n\n\nCode\nplot_attn(ig_attn, model)\nplot_attn(ap_attn, model)\nplot_attn(ig_zero_attn, model)"
  },
  {
    "objectID": "ig_ap_tasks.html",
    "href": "ig_ap_tasks.html",
    "title": "Integrated Gradients vs Activation Patching Across Circuits",
    "section": "",
    "text": "Goal: investigate the agreement between integrated gradients and activation patching when the baselines are similar, across a variety of circuit tasks.\nTasks:"
  },
  {
    "objectID": "ig_ap_tasks.html#integrated-gradients",
    "href": "ig_ap_tasks.html#integrated-gradients",
    "title": "Integrated Gradients vs Activation Patching Across Circuits",
    "section": "Integrated gradients",
    "text": "Integrated gradients\n\n\nCode\ndef run_from_layer_fn(original_input, patch_layer, patch_output, metric, metric_labels, reset_hooks_end=True):\n    def fwd_hook(act, hook):\n        assert patch_output.shape == act.shape, f\"Patch shape {patch_output.shape} doesn't match activation shape {act.shape}\"\n        return patch_output\n\n    logits = model.run_with_hooks(\n        original_input,\n        fwd_hooks=[(patch_layer.name, fwd_hook)],\n        reset_hooks_end=reset_hooks_end,\n    )\n    \n    diff = metric(logits, metric_labels)\n    return diff\n\n\ndef compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer, metric, metric_labels):\n    n_samples = original_input.size(0)\n    # Take the model starting from the target layer\n    forward_fn = lambda x: run_from_layer_fn(original_input, prev_layer, x, metric, metric_labels)\n    # Attribute to the target_layer's output\n    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n                                                    baselines=layer_baseline, \n                                                    internal_batch_size=n_samples,\n                                                    attribute_to_layer_input=False,\n                                                    return_convergence_delta=True)\n    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error}\")\n    return attributions\n\n\n\n\nCode\ndef integrated_gradients(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, corrupted_cache: ActivationCache, metric: callable, metric_labels):\n    n_samples = clean_tokens.size(0)\n    \n    # Gradient attribution for neurons in MLP layers\n    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n    # Gradient attribution for attention heads\n    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n\n    # Calculate integrated gradients for each layer\n    for layer in range(model.cfg.n_layers):\n\n        # Gradient attribution on heads\n        hook_name = get_act_name(\"result\", layer)\n        target_layer = model.hook_dict[hook_name]\n        prev_layer_hook = get_act_name(\"z\", layer)\n        prev_layer = model.hook_dict[prev_layer_hook]\n\n        layer_clean_input = clean_cache[prev_layer_hook]\n        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n\n        # Shape [batch, seq_len, d_head, d_model]\n        attributions = compute_layer_to_output_attributions(\n            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, metric_labels)\n        print(attributions.shape)\n        # Calculate attribution score based on mean over each embedding, for each token\n        per_token_score = attributions.mean(dim=3)\n        score = per_token_score.mean(dim=1)\n        attn_results[:, layer] = score\n\n        # Gradient attribution on MLP neurons\n        hook_name = get_act_name(\"post\", layer)\n        target_layer = model.hook_dict[hook_name]\n        prev_layer_hook = get_act_name(\"mlp_in\", layer)\n        prev_layer = model.hook_dict[prev_layer_hook]\n\n        layer_clean_input = clean_cache[prev_layer_hook]\n        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n        \n        # Shape [batch, seq_len, d_model]\n        attributions = compute_layer_to_output_attributions(\n            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, metric_labels)\n        score = attributions.mean(dim=1)\n        mlp_results[:, layer] = score\n\n    return mlp_results, attn_results"
  },
  {
    "objectID": "ig_ap_tasks.html#activation-patching",
    "href": "ig_ap_tasks.html#activation-patching",
    "title": "Integrated Gradients vs Activation Patching Across Circuits",
    "section": "Activation patching",
    "text": "Activation patching\n\n\nCode\ndef patch_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, idx: int):\n    # Replace the activations for the target neuron with activations from the cached run.\n    cached_activations = cache[hook.name]\n    activations[:, :, idx] = cached_activations[:, :, idx]\n    return activations\n\ndef activation_patching(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, clean_logit_diff, corrupted_cache: ActivationCache, corrupted_logit_diff, metric: callable, metric_labels):\n    n_samples = clean_tokens.size(0)\n    \n    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n\n    baseline_diff = clean_logit_diff - corrupted_logit_diff\n\n    for layer in range(model.cfg.n_layers):\n        # Activation patching on heads\n        print(f\"Activation patching on attention heads in layer {layer}\")\n        for head in range(model.cfg.n_heads):\n            hook_name = get_act_name(\"result\", layer)\n            temp_hook = lambda act, hook: patch_hook(act, hook, corrupted_cache, head)\n\n            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n                patched_logits = model(clean_tokens)\n\n            patched_logit_diff = metric(patched_logits, metric_labels).detach()\n            # Normalise result by clean and corrupted logit difference\n            attn_results[:, layer, head] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n\n        # Activation patching on MLP neurons\n        print(f\"Activation patching on MLP in layer {layer}\")\n        for neuron in range(model.cfg.d_mlp):\n            hook_name = get_act_name(\"post\", layer)\n            temp_hook = lambda act, hook: patch_hook(act, hook, corrupted_cache, neuron)\n            \n            with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n                patched_logits = model(clean_tokens)\n\n            patched_logit_diff = metric(patched_logits, metric_labels).detach()\n            # Normalise result by clean and corrupted logit difference\n            mlp_results[:, layer, neuron] = (patched_logit_diff - clean_logit_diff) / baseline_diff\n\n    return mlp_results, attn_results"
  },
  {
    "objectID": "ig_ap_tasks.html#analysis",
    "href": "ig_ap_tasks.html#analysis",
    "title": "Integrated Gradients vs Activation Patching Across Circuits",
    "section": "Analysis",
    "text": "Analysis\n\n\nCode\ndef plot_attn_comparison(ig_attn_results, ap_attn_results, task: Task):\n\n    n_results = ig_attn_results.size(0)\n    assert n_results == ap_attn_results.size(0)\n\n    for i in range(n_results):\n        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n\n        im = ax1.imshow(ig_attn_results[i].detach(), cmap='coolwarm')\n        ax1.set_title(f\"Integrated Gradients ({i})\")\n\n        ax1.set_xlabel(\"Head Index\")\n        ax1.set_xticks(list(range(model.cfg.n_heads)))\n        ax1.set_ylabel(\"Layer\")\n        ax1.set_yticks(list(range(model.cfg.n_layers)))\n        plt.colorbar(im, ax=ax1)\n\n        im = ax2.imshow(ap_attn_results[i].detach(), cmap='coolwarm')\n        ax2.set_title(f\"Activation Patching ({i})\")\n\n        ax2.set_xlabel(\"Head Index\")\n        ax2.set_xticks(list(range(model.cfg.n_heads)))\n        ax2.set_ylabel(\"Layer\")\n        ax2.set_yticks(list(range(model.cfg.n_layers)))\n        plt.colorbar(im, ax=ax2)\n\n        plt.tight_layout()\n        plt.show()\n\n\n\n\nCode\nfrom sklearn.preprocessing import MaxAbsScaler\n\ndef plot_correlation(ig_mlp_scores, ap_mlp_scores, ig_attn_scores, ap_attn_scores, task: Task):\n\n    n_results = ig_mlp_scores.size(0)\n    assert n_results == ap_mlp_scores.size(0)\n\n    for i in range(n_results):\n\n        x1 = ig_mlp_scores[i].flatten().numpy()\n        y1 = ap_mlp_scores[i].flatten().numpy()\n\n        x2 = ig_attn_scores[i].flatten().numpy()\n        y2 = ap_attn_scores[i].flatten().numpy()\n\n        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n        \n        sns.regplot(x=x1, y=y1, ax=ax1)\n        ax1.set_xlabel(\"Integrated Gradients Attribution Scores\")\n        ax1.set_ylabel(\"Activation Patching Attribution Scores\")\n        ax1.set_title(f\"{task.name} MLP Attribution Scores ({i})\")\n\n        sns.regplot(x=x2, y=y2, ax=ax2)\n        ax2.set_xlabel(\"Integrated Gradients Attribution Scores\")\n        ax2.set_ylabel(\"Activation Patching Attribution Scores\")\n        ax2.set_title(f\"{task.name} Attention Heads Attribution Scores ({i})\")\n\n        plt.tight_layout()\n        plt.show()\n\n        print(f\"Correlation coefficient for neurons: {np.corrcoef(x1, y1)[0, 1]}\")\n        print(f\"Correlation coefficient for attention heads: {np.corrcoef(x2, y2)[0, 1]}\")\n\n\ndef plot_mean_diff(ig_scores, ap_scores, title=None):\n\n    x = ig_scores.flatten().numpy()\n    y = ap_scores.flatten().numpy()\n\n    # Mean difference plot with scaled data\n\n    scaled_ig_scores = MaxAbsScaler().fit_transform(x.reshape(-1, 1))\n    scaled_ap_scores = MaxAbsScaler().fit_transform(y.reshape(-1, 1))\n\n    mean = np.mean([scaled_ig_scores, scaled_ap_scores], axis=0)\n    diff = scaled_ap_scores - scaled_ig_scores\n    md = np.mean(diff) # Mean of the difference\n    sd = np.std(diff, axis=0) # Standard deviation of the difference\n\n    sns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\n    plt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\n    plt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\n    plt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\n    plt.xlabel(\"Mean of attribution scores\")\n    plt.ylabel(\"Difference (activation patching - integrated gradients)\")\n    if title:\n        plt.title(title)\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "contrastive_gradients.html",
    "href": "contrastive_gradients.html",
    "title": "Gradient-based Counterfactuals in GPT2-Small",
    "section": "",
    "text": "Objective: investigate the relationship between attribution scores and output gradients, and utilise this relationship to generate the “optimal” counterfactual inputs such that a specific model component will be assigned high attribution scores by IG/AP.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport torch\nimport numpy as np\n\nfrom captum.attr import LayerIntegratedGradients\nfrom captum.attr._utils.approximation_methods import approximation_parameters\n\nfrom transformer_lens.utils import get_act_name, get_device\nfrom transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\nfrom transformer_lens.hook_points import HookPoint\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndevice = get_device()\nmodel = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\nn_steps = 50\n\n\nLoaded pretrained model gpt2-small into HookedTransformer\n\n\n\n\nCode\nclean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\ncorrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n\nclean_input = model.to_tokens(clean_prompt)\ncorrupted_input = model.to_tokens(corrupted_prompt)\n\ndef logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n    # model.to_single_token maps a string value of a single token to the token index for that token\n    correct_index = model.to_single_token(correct_answer)\n    incorrect_index = model.to_single_token(incorrect_answer)\n    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n\n# Explicitly calculate and expose the result for each attention head\nmodel.set_use_attn_result(True)\nmodel.set_use_hook_mlp_in(True)\n\nclean_logits, clean_cache = model.run_with_cache(clean_input)\nclean_logit_diff = logits_to_logit_diff(clean_logits)\nprint(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n\ncorrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)\ncorrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\nprint(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n\n\nClean logit difference: 4.276\nCorrupted logit difference: -2.738\n\n\n\n\nCode\ndef run_from_layer_fn(x, original_input, prev_layer, reset_hooks_end=True):\n    # Force the layer before the target layer to output the given values, i.e. pass the given input into the target layer\n    # original_input value does not matter; useful to keep shapes nice, but its activations will be overwritten\n    \n    # Do not backpropagate before the target layer\n    torch.set_grad_enabled(False)\n\n    def fwd_hook(act, hook):\n        torch.set_grad_enabled(True)\n        x.requires_grad = True\n        return x\n    \n    logits = model.run_with_hooks(\n        original_input,\n        fwd_hooks=[(prev_layer.name, fwd_hook)],\n        reset_hooks_end=reset_hooks_end\n    )\n    logit_diff = logits_to_logit_diff(logits).unsqueeze(0)\n    return logit_diff\n\n\n\n\nCode\ndef get_layer_baseline_inputs(target_layer_num, target_pos, clean_cache, corrupt_cache):\n    hook_name = get_act_name(\"result\", target_layer_num)\n    target_layer = model.hook_dict[hook_name]\n\n    layer_baseline = clean_cache[hook_name] # Baseline\n    \n    layer_input = layer_baseline.clone()\n    layer_input[:, :, target_pos] = corrupt_cache[hook_name][:, :, target_pos]\n\n    return layer_baseline, layer_input, target_layer"
  },
  {
    "objectID": "contrastive_gradients.html#sanity-check-output-difference-for-attention-heads",
    "href": "contrastive_gradients.html#sanity-check-output-difference-for-attention-heads",
    "title": "Gradient-based Counterfactuals in GPT2-Small",
    "section": "Sanity check: output difference for attention heads",
    "text": "Sanity check: output difference for attention heads\nThe difference in model outputs for the two contrastive inputs corresponds exactly to the activation patching attribution scores by definition.\n\n\nCode\ndef output_difference(target_layer_num, target_pos):\n    layer_clean_input, layer_corrupt_input, target_layer = get_layer_baseline_inputs(target_layer_num, target_pos, clean_cache, corrupted_cache)\n\n    # Take the model starting from the target layer\n    forward_fn = lambda x: run_from_layer_fn(x, clean_input, target_layer)\n    _, alphas_func = approximation_parameters(\"gausslegendre\")\n    alphas = alphas_func(n_steps)\n\n    interpolated_inputs = [layer_clean_input + alpha * (layer_corrupt_input - layer_clean_input) for alpha in alphas]\n    outputs = torch.tensor([forward_fn(i) for i in interpolated_inputs])\n\n    max_output = torch.max(outputs)\n    min_output = torch.min(outputs)\n    return max_output - min_output\n\n\n\n\nCode\noutput_difference_1110 = output_difference(11, 10)\noutput_difference_96 = output_difference(9, 6)\noutput_difference_17 = output_difference(1, 7)\n\nprint(f\"Output difference for head 11.10: {output_difference_1110}\")\nprint(f\"Output difference for head 9.6: {output_difference_96}\")\nprint(f\"Output difference for head 1.7: {output_difference_17}\")\n\n\nOutput difference for head 11.10: 1.508988380432129\nOutput difference for head 9.6: 0.11768531799316406\nOutput difference for head 1.7: 0.01003265380859375\n\n\n\nCorrelation between output difference and activation patching scores\nWe quantify the range in outputs across all attention heads, and compare it to the activation patching score.\n\n\nCode\nattn_patch_results = torch.load(\"attn_patch_results.pt\")\n\n\n\n\nCode\nattn_output_diff = torch.empty((model.cfg.n_layers, model.cfg.n_heads))\n\nfor layer_idx in range(model.cfg.n_layers):\n    for head_idx in range(model.cfg.n_heads):\n        attn_output_diff[layer_idx, head_idx] = output_difference(layer_idx, head_idx)\n\n\n\n\nCode\ntorch.save(attn_output_diff, \"attn_output_diff.pt\")\n\n\n\n\nCode\n# Plot gradient ranges and activation patching scores side-by-side\nplt.figure(figsize=(15,10))\nplt.subplot(1, 2, 1)\n\noutput_range_bound = torch.max(torch.abs(attn_output_diff))\n\nplt.title(\"Output range for attention heads\")\nplt.imshow(attn_output_diff.detach(), cmap=\"RdBu\", vmin=-output_range_bound, vmax=output_range_bound)\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\nplt.colorbar(orientation=\"horizontal\")\n\nplt.subplot(1, 2, 2)\n\npatch_bound = torch.max(torch.abs(attn_patch_results))\n\nplt.title(\"Activation patching scores for attention heads\")\nplt.imshow(attn_patch_results.detach(), cmap=\"RdBu\", vmin=-patch_bound, vmax=patch_bound)\nplt.xlabel(\"Head Index\")\nplt.xticks(list(range(model.cfg.n_heads)))\nplt.ylabel(\"Layer\")\nplt.yticks(list(range(model.cfg.n_layers)))\nplt.colorbar(orientation=\"horizontal\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot correlation between gradient range and activation patching score\n\nattn_output_ranges_1d = attn_output_diff.flatten().numpy()\nattn_abs_patch_results_1d = torch.abs(attn_patch_results).flatten().numpy()\n\nsns.regplot(x=attn_output_ranges_1d, y=attn_abs_patch_results_1d)\nplt.xlabel(\"Range in gradient of output wrt interpolated inputs\")\nplt.ylabel(\"Absolute activation patching attribution scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between gradient range and absolute activation patching score: {np.corrcoef(attn_output_ranges_1d, attn_patch_results_1d)[0, 1]}\")\n\n\n\n\n\n\n\n\n\nCorrelation coefficient between gradient range and absolute activation patching score: 0.3742223382467527"
  },
  {
    "objectID": "contrastive_gradients.html#preamble-results-so-far",
    "href": "contrastive_gradients.html#preamble-results-so-far",
    "title": "Gradient-based Counterfactuals in GPT2-Small",
    "section": "Preamble: results so far",
    "text": "Preamble: results so far\nFrom prior experiments, we can conclude that:\n\nThe choice of contrastive inputs (baseline vs input for IG, or clean vs corrupt for AP) affects attribution scores greatly, as the two methods are implicitly reliant on counterfactual reasoning.\nIG and AP largely agree when their contrastive inputs agree.\n\nThe discrepancies between IG and AP are characterised by the shape of the output curve.\n\nDiscrepancies in IG and AP attribution scores are caused by a non-linear output curve which returns to the same output value along the straightline path between contrastive inputs.\nAttribution scores from AP correspond exactly to the difference in outputs between contrastive inputs, when patching into a specific component.\nFaithfulness and stability of IG attribution increases if the output curve along the straightline path does not contain saturated regions.\n\nBased on these results, we can leverage the counterfactual reasoning underlying these methods, to identify the “optimal” contrastive pair which would cause a target model component to have high attribution scores under both IG and AP.\n\nMotivation\nGenerating the counterfactuals which yield a high attribution score for a given model component can:\n\nProvide counterfactual explanations for specific model components, which may be more human-interpretable than existing methods, e.g. top activating tokens, or SAEs\nUnify attribution scores from activation patching and integrated gradients\nDeepen understanding of how specific circuit components contribute to the circuit’s task\nSupport the design of contrastive templates used to test specific behaviours\n\n\n\nDesired outcome\nFor example, for the attention head 9.9 (name mover head which attends to the correct indirect object), an optimal counterfactual pair might be:\n\nClean: “When John gave Mary a bottle of milk, he thanked”\nCorrupt: “When John gave a bottle of milk to Mary, he thanked”\n\nFor attention heads which do not contribute to the circuit, non-IOI related counterfactuals should be generated."
  },
  {
    "objectID": "contrastive_gradients.html#objectives",
    "href": "contrastive_gradients.html#objectives",
    "title": "Gradient-based Counterfactuals in GPT2-Small",
    "section": "Objectives",
    "text": "Objectives\nTo generate the optimal contrastive pair for a high attribution score, we aim to satisfy the following objectives:\n\nMaximise difference in output logits between candidate input pair: maps directly to attribution score for activation patching\nPreserve change in output along region between candidate inputs: avoid saturated regions during IG interpolation\nMaximise similarity in candidate pair inputs: minimal perturbation to inputs for interpretable counterfactuals\nMinimise perplexity of candidate input pair: ensure contrastive pairs are realistic (in model input distribution).\n\nAs we are dealing with a language model where tokens map to discrete embedding vectors, we use ideas from discrete optimisation methods such as PEZ (largely in the realm of prompt generation for text-to-image models):\n\nWe make use of a forward projection function, which finds the most similar vector in the model’s embedding matrix to the given continuous embedding vector.\nWe optimise over continuous embedding vectors, but calculate the gradient / optimisation step wrt to the projected embedding."
  },
  {
    "objectID": "contrastive_gradients.html#evolutionary-algorithm",
    "href": "contrastive_gradients.html#evolutionary-algorithm",
    "title": "Gradient-based Counterfactuals in GPT2-Small",
    "section": "Evolutionary algorithm",
    "text": "Evolutionary algorithm\nSince we want to optimise the above objectives simultaneously, we use an evolutionary strategy with the following setup:\n\nThe population consists of candidate pairs of continuous-valued vectors (x,y).\nThe fitness of a candidate pair is (a, b, c, d) where a is the difference between model output logits, b is the gradient of the output at each input, c is the similarity between the projected vectors, and d is the negative perplexity of the pair.\nThe behavioural descriptor is the projection of the embedding vector onto the nearest tokens in the model vocabulary.\n\nThe algorithm is as follows:\n\nStart from a singular text input, from which the contrastive pair will be generated.\nInitialise the population with pairs of embeddings (x,y), generated by randomly perturbing the input embedding.\nCalculate the fitness of each candidate pair in the population.\n\nCompute a, the output logit difference: two forward passes (patching into target model component).\nCompute b, the gradient of the output at x and at y: two backward passes.\nProject the continuous embeddings (x,y) to their nearest tokens (x', y'): time complexity O(2 * d_vocab * d_model).\nCompute c, the similarity between the projected tokens x' and y'.\nCompute d, the perplexity of x and y for the model: time complexity O(2 * seq_len).\n\nSelect the top K samples based on their fitness, and mutate them based on the gradients in b.\n\nFor selection, use NSGA-III (non-dominated sorting genetic algorithm) to maximise all four fitness objectives simultaneously.\nFor mutation, add noise to x and to y.\n\nReplace the worst K samples in the popoulation with the mutated samples.\nIteratively repeat steps 3-5 until the output logit difference is greater than threshold T = 10 (arbitrarily set, based on experiment values)."
  },
  {
    "objectID": "contrastive_gradients.html#gradient-descent-algorithm",
    "href": "contrastive_gradients.html#gradient-descent-algorithm",
    "title": "Gradient-based Counterfactuals in GPT2-Small",
    "section": "Gradient descent algorithm",
    "text": "Gradient descent algorithm\nAn alternative solution would be to apply discrete gradient descent to a loss function which consists of a weighted sum of the four objectives. However, it is likely that some of these objectives conflict with each other, e.g. maximising the output difference is likely in conflict with minimising input difference, since the output difference and input difference probably have a proportional relationship.\nTherefore, standard gradient descent may result in some conditions being fulfilled at the expense of other conditions, which is not desirable. In our case, we wish to solve a multi-objective optimisation problem - common solutions in the literature use evolutionary algorithms and Pareto optimisation. We instead can use a multiple gradient descent algorithm (MGDA) to optimise all four objectives without improving one at the expense of another.\nThe algorithm is as follows:\n\nStart from a singular text input t, from which contrastive pairs will be generated. Initialise the values of the contrastive pair (x,y) as x = y = t.\nCalculate the following objectives (same as evolutionary algorithm):\n\nCompute a, the output logit difference: two forward passes (patching into target model component).\nCompute b, the gradient of the output at x and at y: two backward passes.\nProject the continuous embeddings (x,y) to their nearest tokens (x', y'): time complexity O(2 * d_vocab * d_model).\nCompute c, the similarity between the projected tokens x' and y'.\nCompute d, the perplexity of x and y for the model: time complexity O(2 * seq_len).\n\nCompute the gradients of the losses of each objective.\nUpdate each continuous embedding (x, y) using MGDA.\n\nAdvantages of a gradient-based approach: * Simpler algorithm. No need to calculate additional metrics related to the Pareto frontier, as MGDA provably converges to a Pareto stationary point. * Efficiency. Sener and Koltun (2019) provide an optimiser based on MGDA that can be computed via a single backward pass.\nDisadvantages of a gradient-based approach: * Potentially less variety in terms of contrastive pair results. * Tricky to perform gradient descent on objective b, which depends on the gradient. * Solution: approximate the output gradients near the contrastive inputs by taking the output at some distance away from the input.\n\n\nCode\nfrom transformer_lens.utils import get_device\nfrom transformer_lens import HookedTransformer\n\ndevice = get_device()\nmodel = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n\nstart_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n\n\nLoaded pretrained model gpt2-small into HookedTransformer\n\n\n\n\nCode\nfrom counterfactual_generation import counterfactuals\n\ntarget_hook_name = get_act_name(\"result\", 7)\ntarget_layer = model.hook_dict[target_hook_name]\n\nx_cf, y_cf = counterfactuals(start_prompt, model, target_layer, 3, iterations=10)\nprint(x_cf)\nprint(y_cf)\n\n\nOutputs:  John vs.  John\nOutput logit diff: tensor([0.], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 1.0\nInput similarity: 0.0\nNegative perplexity: -25.68834114074707\nOutputs:  John vs.  John\nOutput logit diff: tensor([0.], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 1.0\nInput similarity: 1.0\nNegative perplexity: -25.68834114074707\nOutputs:  John vs.  John\nOutput logit diff: tensor([0.], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 1.0\nInput similarity: 3.0\nNegative perplexity: -25.68834114074707\nOutputs:  John vs.  John\nOutput logit diff: tensor([0.], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 1.0\nInput similarity: 2.1330606937408447\nNegative perplexity: -25.68834114074707\nOutputs:  and vs.  and\nOutput logit diff: tensor([0.0375], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 0.9999998807907104\nInput similarity: 0.0\nNegative perplexity: -38.735374450683594\nOutputs: \n vs. \n\nOutput logit diff: tensor([-0.4186], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 1.0\nInput similarity: 2.0\nNegative perplexity: -63.98436737060547\nOutputs:  ( vs.  (\nOutput logit diff: tensor([0.1483], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 0.9999998807907104\nInput similarity: 3.0\nNegative perplexity: -143.70297241210938\nOutputs:  and vs.  and\nOutput logit diff: tensor([0.0202], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 0.9999998807907104\nInput similarity: 2.999997138977051\nNegative perplexity: -397.3916015625\nOutputs:  and vs.  and\nOutput logit diff: tensor([-0.0292], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 0.9999998807907104\nInput similarity: 3.0\nNegative perplexity: -2182.201171875\nOutputs:  and vs.  and\nOutput logit diff: tensor([0.0176], grad_fn=&lt;UnsqueezeBackward0&gt;)\nOutput semantic similarity: 0.9999998807907104\nInput similarity: 0.0\nNegative perplexity: -1418.828125\n['After John and condesc goes to \" store, Mary went a bottle of milk\"']\n['After restrain and insects goes to a store, Sarah did a cups of milk to']\n\n\n\n\nCode\nx_cf_answer = model.generate(x_cf, max_new_tokens=1, do_sample=False)\ny_cf_answer = model.generate(y_cf, max_new_tokens=1, do_sample=False)\n\nprint(x_cf_answer)\nprint(y_cf_answer)\n\n\n\n\n\n\n\n\nAfter John and condesc goes to \" store, Mary went a bottle of milk\" and\nAfter restrain and insects goes to a store, Sarah did a cups of milk to her\n\n\n\n\nCode\n# TODO: show metrics for attribution\n\n\n\n\nCode\nfrom counterfactual_generation import run_model_with_cache\n\nstart_tokens = model.to_tokens(start_prompt, prepend_bos=False)\n\nx_logits, _ = model.run_with_cache(start_tokens)\n_, idx = x_logits[0, -1].max(-1)\n\nprint(\"Correct:\", model.to_string(idx))\n\nstart_tokens = model.to_tokens(start_prompt, prepend_bos=False)\nstart_embed = model.embed(start_tokens)\nstart_residual, _ = model.get_residual(start_embed, 0)\nx_logits, _ = run_model_with_cache(model, target_layer.name, start_residual, start_at_layer=0)\n_, idx = x_logits[0, -1].max(-1)\n\nprint(\"Generated:\", model.to_string(idx))\n\n\nCorrect:  John\nGenerated:  John\n\n\n\n\nCode\nfrom counterfactual_generation import compute_similarity\n\ntoken_a = model.to_single_token(\"John\")\ntoken_b = model.to_single_token(\"to\")\nembed_a = model.embed(token_a)\nembed_b = model.embed(token_b)\n\ncompute_similarity(embed_a, embed_b)\n\n\ntensor(0.2823, grad_fn=&lt;MeanBackward0&gt;)"
  },
  {
    "objectID": "toy_experiments.html",
    "href": "toy_experiments.html",
    "title": "Integrated Gradients vs Activation Patching",
    "section": "",
    "text": "Objective: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\nBackground:\n\nBoth integrated gradients (IG) and activation patching (AP) are used extensively in explainability and interpretability.\n\nIG is path-based: integrates gradients along a path from a baseline to the input.\nAP is perturbation-based: directly measures causal effect of replacing activations.\n\nActivation patching can be approximated cheaply using methods inspired by integrated gradients; see Hanna et al (2024)\nGradient-based attribution methods can be used to identify important model components as an alternative to activation patching; see Ferrando and Voita (2024)\nActivation patching is often used as the “gold standard” to evaluate attribution methods\n\nMotivation:\n\nUnderstand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\nInvestigate if discrepancies help uncover different hidden model behaviours\nUnderstand when and why linear approximations to activation patching fail\nInvestigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is “incorrect”)\n\nSome ideas:\n\nComponents which might not be identified by activation patching: generic components which are used for both clean and corrupted examples in activation patching\nComponents which might not be identified by integrated gradients: backup attention heads, components which only influence the output when interacting with other components, i.e. OR circuits\n\n\n\nWe load a pre-trained toy transformer which performs balanced bracket classification. The model has three layers, each with two attention heads and one MLP layer of 56 neurons.\n\n\n\nCode\nimport importlib\n\nimport torch\nfrom captum.attr import LayerIntegratedGradients, IntegratedGradients\nimport numpy as np\nfrom transformer_lens.utils import get_act_name\nfrom transformer_lens import ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nimport toy_transformers.toy_bracket_transformer as tt\nimportlib.reload(tt)\nfrom toy_transformers.toy_bracket_transformer import load_toy_bracket_transformer, test_loaded_bracket_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntokenizer, model = load_toy_bracket_transformer()\n\n# Explicitly calculate and expose the result for each attention head and MLP input\nmodel.set_use_attn_result(True)\nmodel.set_use_hook_mlp_in(True)"
  },
  {
    "objectID": "toy_experiments.html#background-motivation-and-set-up",
    "href": "toy_experiments.html#background-motivation-and-set-up",
    "title": "Integrated Gradients vs Activation Patching",
    "section": "",
    "text": "Objective: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\nBackground:\n\nBoth integrated gradients (IG) and activation patching (AP) are used extensively in explainability and interpretability.\n\nIG is path-based: integrates gradients along a path from a baseline to the input.\nAP is perturbation-based: directly measures causal effect of replacing activations.\n\nActivation patching can be approximated cheaply using methods inspired by integrated gradients; see Hanna et al (2024)\nGradient-based attribution methods can be used to identify important model components as an alternative to activation patching; see Ferrando and Voita (2024)\nActivation patching is often used as the “gold standard” to evaluate attribution methods\n\nMotivation:\n\nUnderstand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\nInvestigate if discrepancies help uncover different hidden model behaviours\nUnderstand when and why linear approximations to activation patching fail\nInvestigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is “incorrect”)\n\nSome ideas:\n\nComponents which might not be identified by activation patching: generic components which are used for both clean and corrupted examples in activation patching\nComponents which might not be identified by integrated gradients: backup attention heads, components which only influence the output when interacting with other components, i.e. OR circuits\n\n\n\nWe load a pre-trained toy transformer which performs balanced bracket classification. The model has three layers, each with two attention heads and one MLP layer of 56 neurons.\n\n\n\nCode\nimport importlib\n\nimport torch\nfrom captum.attr import LayerIntegratedGradients, IntegratedGradients\nimport numpy as np\nfrom transformer_lens.utils import get_act_name\nfrom transformer_lens import ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nimport toy_transformers.toy_bracket_transformer as tt\nimportlib.reload(tt)\nfrom toy_transformers.toy_bracket_transformer import load_toy_bracket_transformer, test_loaded_bracket_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntokenizer, model = load_toy_bracket_transformer()\n\n# Explicitly calculate and expose the result for each attention head and MLP input\nmodel.set_use_attn_result(True)\nmodel.set_use_hook_mlp_in(True)"
  }
]