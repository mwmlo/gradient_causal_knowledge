{
 "cells": [
  {
   "cell_type": "raw",
   "id": "581fbc13",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Testing for AND/OR circuits in GPT2-Small\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ef359",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "- Hypothesis: another explanation for discrepancies is that integrated gradients may flag latent components (which only activate in AND/OR circuits). For example, a later attention head may depend on an earlier one.\n",
    "- Method: we run activation patching from corrupt to clean, and from clean to corrupt. We also run integrated gradients from corrupt to clean, and from clean to corrupt. AND/OR components will be flagged in integrated gradients in both directions, but only flagged in one direction for patching.\n",
    "    - Clean→corrupt should pick up OR circuits, Corrupt→clean should pick up AND circuits.\n",
    "    - AP asymmetry score: difference between AP scores in two directions, normalised by max score.\n",
    "    - IG asymmetry score: difference between IG scores in two directions, normalised by max score.\n",
    "    - AND/OR candidates should have low IG asymmetry and high AP asymmetry.\n",
    "\n",
    "- Implications: if true, we could use IG to detect results which would have required two activation patching passes in different directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import Task, TaskDataset, logit_diff_metric, run_from_layer_fn, plot_attn_comparison, plot_correlation\n",
    "from split_ig import SplitLayerIntegratedGradients\n",
    "from attribution_methods import integrated_gradients, activation_patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e39ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch from clean to corrupt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
