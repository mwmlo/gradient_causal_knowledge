{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d7d728",
   "metadata": {},
   "source": [
    "# Saturated Gradients\n",
    "\n",
    "From the ablation experiments, we can see that IG assigns higher attribution scores than AP, but some of these attribution scores are overestimated. AP also underestimates the attribution scores for some heads!\n",
    "\n",
    "- IG has more true positives, but also more false positives: IG has higher recall, but AP has higher precision.\n",
    "- Overall the results between the methods are very similar.\n",
    "\n",
    "What causes false positives in IG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acddeb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set up\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_device, get_act_name\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from attribution_methods import run_from_layer_fn, compute_layer_to_output_attributions\n",
    "from testing import Task, TaskDataset, logit_diff_metric, identify_outliers, test_single_ablated_performance\n",
    "from plotting import plot_attn, plot_attn_comparison, plot_correlation, plot_correlation_comparison, plot_bar_chart\n",
    "\n",
    "from split_ig import SplitIntegratedGradients, SplitLayerIntegratedGradients, compute_layer_to_output_attributions_split_ig, split_integrated_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0597e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = get_device()\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d112f7f",
   "metadata": {},
   "source": [
    "## Output shapes\n",
    "\n",
    "Hypothesis: outliers overestimated by IG are due to the shape of output curve in between the baseline and inputs to IG.\n",
    "\n",
    "- IG calculates change in loss based on integrating gradients between two input values.\n",
    "- A high attribution score could be caused by strong gradients (sensitivity) up until an intermediate input value (in between the two input values). In this case, the highlighted component would be important for the task \"in between\" (represented by different counterfactual inputs) instead of the target task.\n",
    "\n",
    "![Overestimation](reference/overestimation.png)\n",
    "\n",
    "To test this, we can visualise the gradients for intervals which are summed up by IG. We focus on attention head (9, 6) because it is highlighted more strongly by IG than by AP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260e8d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([-0.0307, -0.9269, -0.4937,  2.2320,  0.6754,  4.0447, -0.1785,  1.1947,\n",
      "         1.1514,  1.7507], device='cuda:0')\n",
      "Corrupted logit difference: tensor([-0.0387, -0.9451, -0.5103,  2.2153,  0.6299, -3.2074, -0.1823,  1.1766,\n",
      "        -3.0072,  1.7392], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=10)\n",
    "\n",
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1093c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr._utils.approximation_methods import approximation_parameters\n",
    "\n",
    "n_steps = 50\n",
    "\n",
    "def visualise_attn_interpolated_outputs(target_layer_num, target_pos):\n",
    "    hook_name = get_act_name(\"result\", target_layer_num)\n",
    "    visualise_interpolated_integrated_gradients(hook_name, target_layer_num, target_pos)\n",
    "\n",
    "\n",
    "def visualise_mlp_interpolated_outputs(target_layer_num, target_pos):\n",
    "    hook_name = get_act_name(\"pos\", target_layer_num)\n",
    "    visualise_interpolated_integrated_gradients(hook_name, target_layer_num, target_pos)    \n",
    "\n",
    "\n",
    "def visualise_interpolated_integrated_gradients(hook_name, target_layer_num, target_pos):\n",
    "    target_layer = model.hook_dict[hook_name]\n",
    "    layer_clean_input = clean_cache[hook_name] # Baseline\n",
    "\n",
    "    # Only corrupt at target head\n",
    "    layer_corrupt_input = layer_clean_input.clone()\n",
    "    layer_corrupt_input[:, :, target_pos] = corrupted_cache[hook_name][:, :, target_pos]\n",
    "\n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(model, clean_input, target_layer, x, logit_diff_metric, labels)\n",
    "    _, alphas_func = approximation_parameters(\"gausslegendre\")\n",
    "    alphas = alphas_func(n_steps)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(True):\n",
    "        interpolated_inputs = [layer_clean_input + alpha * (layer_corrupt_input - layer_clean_input) for alpha in alphas]\n",
    "        outputs = [forward_fn(i) for i in interpolated_inputs]\n",
    "\n",
    "    plt.title(f\"Model output at interpolated gradients: {(target_layer_num, target_pos)}\")\n",
    "    plt.plot(alphas, [o.item() for o in outputs])\n",
    "    plt.xlabel(\"Interpolation coefficient\")\n",
    "    plt.ylabel(\"Output (logit difference)\")\n",
    "    plt.ylim(0, 6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d759c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_mlp = torch.load(\"results/aligned/ioi/ig_mlp.pt\")\n",
    "ig_attn = torch.load(\"results/aligned/ioi/ig_attn.pt\")\n",
    "\n",
    "ap_mlp = torch.load(\"results/aligned/ioi/ap_mlp.pt\")\n",
    "ap_attn = torch.load(\"results/aligned/ioi/ap_attn.pt\")\n",
    "\n",
    "# Identify disagreements between the two attribution methods\n",
    "\n",
    "scaled_ig_attn = ig_attn * 1e5\n",
    "scaled_ig_attn = ig_attn * 1e5\n",
    "attn_outliers = []\n",
    "for i in range(ig_attn.size(0)):\n",
    "    outliers = identify_outliers(scaled_ig_attn[i], ap_attn[i])\n",
    "    attn_outliers.append(outliers)\n",
    "\n",
    "scaled_ig_mlp = ig_mlp * 1e5\n",
    "mlp_outliers = []\n",
    "for i in range(ig_mlp.size(0)):\n",
    "    outliers = identify_outliers(scaled_ig_mlp[i], ap_mlp[i])\n",
    "    mlp_outliers.append(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b567aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 209.44 MiB is free. Process 253904 has 1.74 GiB memory in use. Process 915710 has 6.47 GiB memory in use. Including non-PyTorch memory, this process has 2.23 GiB memory in use. Of the allocated memory 1.92 GiB is allocated by PyTorch, and 133.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer, idx \u001b[38;5;129;01min\u001b[39;00m attn_outliers[\u001b[32m0\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mvisualise_attn_interpolated_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, \u001b[38;5;129;01min\u001b[39;00m mlp_outliers[\u001b[32m0\u001b[39m]:\n\u001b[32m      5\u001b[39m     visualise_mlp_interpolated_outputs(layer, idx)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mvisualise_attn_interpolated_outputs\u001b[39m\u001b[34m(target_layer_num, target_pos)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualise_attn_interpolated_outputs\u001b[39m(target_layer_num, target_pos):\n\u001b[32m      6\u001b[39m     hook_name = get_act_name(\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m, target_layer_num)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mvisualise_interpolated_integrated_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mvisualise_interpolated_integrated_gradients\u001b[39m\u001b[34m(hook_name, target_layer_num, target_pos)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     29\u001b[39m     interpolated_inputs = [layer_clean_input + alpha * (layer_corrupt_input - layer_clean_input) \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     outputs = [\u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m interpolated_inputs]\n\u001b[32m     32\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel output at interpolated gradients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(target_layer_num,\u001b[38;5;250m \u001b[39mtarget_pos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m plt.plot(alphas, [o.item() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mvisualise_interpolated_integrated_gradients.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     21\u001b[39m layer_corrupt_input[:, :, target_pos] = corrupted_cache[hook_name][:, :, target_pos]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Take the model starting from the target layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m forward_fn = \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mrun_from_layer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_diff_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m _, alphas_func = approximation_parameters(\u001b[33m\"\u001b[39m\u001b[33mgausslegendre\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m alphas = alphas_func(n_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradient_causal_knowledge/attribution_methods.py:34\u001b[39m, in \u001b[36mrun_from_layer_fn\u001b[39m\u001b[34m(model, original_input, patch_layer, patch_output, metric, metric_labels, reset_hooks_end)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m     30\u001b[39m         patch_output.shape == act.shape\n\u001b[32m     31\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPatch shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_output.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != activation shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m patch_output + \u001b[32m0\u001b[39m * act  \u001b[38;5;66;03m# Trick to ensure gradients propagate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_hook\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset_hooks_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_hooks_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m diff = metric(logits, metric_labels)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m diff\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/transformer_lens/hook_points.py:456\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     logging.warning(\n\u001b[32m    452\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/mwl21/fypvenv/lib/python3.12/site-packages/transformer_lens/components/abstract_attention.py:316\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    311\u001b[39m     z = einops.rearrange(\n\u001b[32m    312\u001b[39m         z, \u001b[33m\"\u001b[39m\u001b[33mbatch pos head_index d_head -> batch pos head_index d_head 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m     )\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# Multiply the z tensor by the W_O tensor, summing over the d_head dimension\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     unhooked_result = (\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m).sum(-\u001b[32m2\u001b[39m)\n\u001b[32m    318\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.hook_result(unhooked_result)  \u001b[38;5;66;03m# [batch, pos, head_index, d_model]\u001b[39;00m\n\u001b[32m    319\u001b[39m out = (\n\u001b[32m    320\u001b[39m     einops.reduce(result, \u001b[33m\"\u001b[39m\u001b[33mbatch position index model->batch position model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m     + \u001b[38;5;28mself\u001b[39m.b_O\n\u001b[32m    322\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 474.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 209.44 MiB is free. Process 253904 has 1.74 GiB memory in use. Process 915710 has 6.47 GiB memory in use. Including non-PyTorch memory, this process has 2.23 GiB memory in use. Of the allocated memory 1.92 GiB is allocated by PyTorch, and 133.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for layer, idx in attn_outliers[0]:\n",
    "    visualise_attn_interpolated_outputs(layer, idx)\n",
    "\n",
    "for layer_idx, in mlp_outliers[0]:\n",
    "    visualise_mlp_interpolated_outputs(layer, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf0a69",
   "metadata": {},
   "source": [
    "## Split Integrated Gradients\n",
    "\n",
    "The shape of outputs in integrated gradients suggests that IG may overestimate some attribution values due to saturated gradients at interpolated inputs. To confirm this, we run SplitIG (which cuts off the interpolated inputs if the gradients are saturated) and examine the level of agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff3e80",
   "metadata": {},
   "source": [
    "### Sanity check: no splitting\n",
    "\n",
    "We check that Split IG with a split ratio of 1 (i.e. no splitting) produces the same result as regular IG, for a random attention head (5, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = get_act_name(\"result\", 5)\n",
    "target_layer = model.hook_dict[hook_name]\n",
    "prev_layer_hook = get_act_name(\"z\", 5)\n",
    "prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "layer_clean_input = clean_cache[prev_layer_hook]\n",
    "layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "# Shape [batch, seq_len, d_head, d_model]\n",
    "left_ig, _, _ = compute_layer_to_output_attributions_split_ig(\n",
    "    clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, logit_diff_metric, labels, ratio=1)\n",
    "\n",
    "original_attributions = compute_layer_to_output_attributions(\n",
    "    model, clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, logit_diff_metric, labels\n",
    ")\n",
    "\n",
    "assert torch.allclose(left_ig, original_attributions.detach().cpu()), f\"Split IG does not produce expected IG result\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e643246",
   "metadata": {},
   "source": [
    "Verify that Split IG at ratio 1 produces the same outputs as standard IG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4211087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "n_samples = clean_tokens.size(0)\n",
    "forward_fn = lambda x: run_from_layer_fn(model, clean_tokens, prev_layer, x, logit_diff_metric, labels)\n",
    "\n",
    "split_ig = SplitLayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "split_ig_attributions, _, _, interpolated_inputs = split_ig.attribute(inputs=layer_corrupt_input,\n",
    "                                baselines=layer_clean_input,\n",
    "                                internal_batch_size=n_samples, # Needs to match patching shape\n",
    "                                attribute_to_layer_input=False,\n",
    "                                return_convergence_delta=False)\n",
    "# split_ig_attributions = split_ig_attributions.reshape((n_samples, 50,) + split_ig_attributions.shape[1:])\n",
    "split_ig_attributions = np.reshape(split_ig_attributions.detach().cpu().numpy(), (n_samples, 50,) + split_ig_attributions.shape[1:], order='F')\n",
    "split_ig_attributions = torch.tensor(split_ig_attributions).to(device).sum(dim=1)\n",
    "\n",
    "ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "ig_attributions = ig_embed.attribute(inputs=layer_corrupt_input,\n",
    "                                baselines=layer_clean_input, \n",
    "                                internal_batch_size=n_samples,\n",
    "                                attribute_to_layer_input=False,\n",
    "                                return_convergence_delta=False)\n",
    "\n",
    "assert torch.allclose(split_ig_attributions, ig_attributions), f\"Split IG does not produces same output as IG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fead1",
   "metadata": {},
   "source": [
    "### Run Split IG\n",
    "\n",
    "We run Split IG on the same IOI dataset, using a split ratio of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbc309",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_split_ig_mlp, ioi_split_ig_attn = split_integrated_gradients(\n",
    "    model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, labels, ratio=0.9\n",
    ")\n",
    "\n",
    "torch.save(ioi_split_ig_mlp, \"results/saturated/ioi_split_ig_mlp.pt\")\n",
    "torch.save(ioi_split_ig_attn, \"results/saturated/ioi_split_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4930119",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81684f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_ig_mlp = torch.load(\"saved_results/ioi_ig_mlp.pt\")\n",
    "ioi_ig_attn = torch.load(\"saved_results/ioi_ig_attn.pt\")\n",
    "\n",
    "ioi_split_ig_mlp = torch.load(\"results/saturated/ioi_split_ig_mlp.pt\")\n",
    "ioi_split_ig_attn = torch.load(\"results/saturated/ioi_split_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attn_comparison(ioi_ig_attn[:3].unsqueeze(0), ioi_split_ig_attn[:3].unsqueeze(0), model, \"Integrated Gradients\", \"Split Integrated Gradients (0.9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(ioi_ig_mlp, ioi_split_ig_mlp, \"Integrated Gradients Attribution Scores\", \"Split IG Attribution Scores\", \"Attribution Scores for Neurons in IOI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation(ioi_ig_attn, ioi_split_ig_attn, \"Integrated Gradients Attribution Scores\", \"Split IG Attribution Scores\", \"Attribution Scores for Attention Heads in IOI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad02f9",
   "metadata": {},
   "source": [
    "## Elimination of noise\n",
    "\n",
    "We want to check if noisy (method-exclusive) components are eliminated under split IG.\n",
    "\n",
    "To do this, we compare the attribution scores of method-exclusive components under IG and Split IG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae22633",
   "metadata": {},
   "source": [
    "### Method-exclusive components in Split IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_outliers_split_ig = {(layer, idx): ioi_split_ig_attn[layer][idx] for layer, idx in attn_outliers}\n",
    "mlp_outliers_split_ig = {(layer, idx): ioi_split_ig_mlp[layer][idx] for layer, idx in mlp_outliers}\n",
    "\n",
    "plot_bar_chart(attn_outliers_split_ig, \"Attention Heads\", \"Split IG Attribution Scores\", \"Split IG Attribution Scores for Attention Head Outliers in IOI\")\n",
    "plot_bar_chart(mlp_outliers_split_ig, \"MLP Neurons\", \"Split IG Attribution Scores\", \"Split IG Attribution Scores for MLP Neuron Outliers in IOI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_attn = ioi_split_ig_attn - ioi_ig_attn\n",
    "diff_mlp = ioi_split_ig_mlp - ioi_ig_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_outliers_discrepancies = {(layer, idx): diff_attn[layer][idx] for layer, idx in attn_outliers}\n",
    "mlp_outliers_discrepancies = {(layer, idx): diff_mlp[layer][idx] for layer, idx in mlp_outliers}\n",
    "\n",
    "plot_bar_chart(attn_outliers_discrepancies, \"Attention Heads\", \"Discrepancy in Attribution Scores\", \"Discrepancy between IG and Split IG for Attention Heads\")\n",
    "plot_bar_chart(mlp_outliers_discrepancies, \"MLP Neurons\", \"Discrepancy in Attribution Scores\", \"Discrepancy between IG and Split IG for MLP Neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9d288",
   "metadata": {},
   "source": [
    "### Ablation of IG exclusive components\n",
    "\n",
    "To evaluate the noisiness of IG versus Split IG, we ablate IG-exclusive and SIG-exclusive components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aa5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean activations over a corrupt dataset\n",
    "\n",
    "attn_outlier_hooks = [get_act_name(\"result\", layer_idx) for layer_idx, _ in attn_outliers]\n",
    "mlp_outlier_hooks = [get_act_name(\"post\", layer_idx) for layer_idx, _ in mlp_outliers]\n",
    "\n",
    "test_dataset = TaskDataset(Task.IOI)\n",
    "random_dataloader = test_dataset.to_dataloader(batch_size=100, shuffle=True)\n",
    "random_prompts, _, _ = next(iter(random_dataloader))\n",
    "\n",
    "prompts_tokens = model.to_tokens(random_prompts)\n",
    "_, prompt_cache = model.run_with_cache(\n",
    "    prompts_tokens, \n",
    "    names_filter=lambda x: x in attn_outlier_hooks or x in mlp_outlier_hooks\n",
    ")\n",
    "\n",
    "mean_corrupt_activations = {}\n",
    "for key in prompt_cache.keys():\n",
    "    mean_values_over_prompts = torch.mean(prompt_cache[key], dim=0, keepdim=True)\n",
    "    mean_corrupt_activations[key] = torch.mean(mean_values_over_prompts, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify components highlighted by IG but not by Split IG\n",
    "attn_ig_outliers = identify_outliers(ioi_ig_attn, ioi_split_ig_attn, only_collect_x_outliers=True)\n",
    "mlp_ig_outliers = identify_outliers(ioi_ig_mlp, ioi_split_ig_mlp, only_collect_x_outliers=True)\n",
    "\n",
    "# Ablate the components highlighted by IG but not by Split IG\n",
    "ig_only_attn_ablated_scores = dict()\n",
    "for layer, idx in attn_ig_outliers:\n",
    "    performance = test_single_ablated_performance(model, layer, idx, mean_corrupt_activations, Task.IOI, is_attn=True)\n",
    "    ig_only_attn_ablated_scores[(layer, idx)] = performance\n",
    "\n",
    "ig_only_mlp_ablated_scores = dict()\n",
    "for layer, idx in mlp_ig_outliers:\n",
    "    performance = test_single_ablated_performance(model, layer, idx, mean_corrupt_activations, Task.IOI, is_attn=False)\n",
    "    ig_only_mlp_ablated_scores[(layer, idx)] = performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify components highlighted by Split IG but not by IG\n",
    "split_ig_only_attn_outliers = identify_outliers(ioi_split_ig_attn, ioi_ig_attn, only_collect_x_outliers=True)\n",
    "split_ig_only_mlp_outliers = identify_outliers(ioi_split_ig_mlp, ioi_ig_mlp, only_collect_x_outliers=True)\n",
    "\n",
    "# Ablate the components highlighted by Split IG but not by IG\n",
    "split_ig_only_attn_ablated_scores = dict()\n",
    "for layer, idx in split_ig_only_attn_outliers:\n",
    "    performance = test_single_ablated_performance(model, layer, idx, mean_corrupt_activations, Task.IOI, is_attn=True)\n",
    "    split_ig_only_attn_ablated_scores[(layer, idx)] = performance\n",
    "    \n",
    "split_ig_only_mlp_ablated_scores = dict()\n",
    "for layer, idx in split_ig_only_mlp_outliers:\n",
    "    performance = test_single_ablated_performance(model, layer, idx, mean_corrupt_activations, Task.IOI, is_attn=False)\n",
    "    split_ig_only_mlp_ablated_scores[(layer, idx)] = performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3bc7",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(ig_only_attn_ablated_scores, \"Abalted Attention Heads\", \"Model Performance\", \"Ablated Performance for Attention Heads Highlighted only by IG\")\n",
    "plot_bar_chart(ig_only_mlp_ablated_scores, \"Abalted MLP Neurons\", \"Model Performance\", \"Ablated Performance for MLP Neurons Highlighted only by IG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcaa7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_chart(split_ig_only_attn_ablated_scores, \"Abalted Attention Heads\", \"Model Performance\", \"Ablated Performance for Attention Heads Highlighted only by Split IG\")\n",
    "plot_bar_chart(split_ig_only_mlp_ablated_scores, \"Abalted MLP Neurons\", \"Model Performance\", \"Ablated Performance for MLP Neurons Highlighted only by Split IG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
