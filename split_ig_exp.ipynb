{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bb270891",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Split Integrated Gradients vs Activation Patching in GPT2-Small\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        toc: true\n",
    "        toc-location: left\n",
    "        embed-resources: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09058431",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The shape of outputs in integrated gradients suggests that IG may overestimate some attribution values due to saturated gradients at interpolated inputs. To confirm this, we run SplitIG (which cuts off the interpolated inputs if the gradients are saturated) and examine the level of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc03d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2868519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "from transformer_lens.utils import get_act_name, get_device\n",
    "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import Task, TaskDataset, logit_diff_metric, run_from_layer_fn, plot_attn_comparison, plot_correlation\n",
    "from split_ig import SplitLayerIntegratedGradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98bf3b",
   "metadata": {},
   "source": [
    "## Set up Split IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6655748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer_to_output_attributions_split_ig(\n",
    "    original_input, \n",
    "    layer_input, \n",
    "    layer_baseline, \n",
    "    target_layer, \n",
    "    prev_layer, \n",
    "    metric, \n",
    "    metric_labels, \n",
    "    ratio=0.95\n",
    "):\n",
    "    n_samples = original_input.size(0)\n",
    "    \n",
    "    # Take the model starting from the target layer\n",
    "    forward_fn = lambda x: run_from_layer_fn(model, original_input, prev_layer, x, metric, metric_labels)\n",
    "\n",
    "    # Attribute to the target_layer's output\n",
    "    split_ig = SplitLayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n",
    "    attributions = split_ig.attribute(inputs=layer_input,\n",
    "                                    baselines=layer_baseline,\n",
    "                                    internal_batch_size=n_samples,\n",
    "                                    attribute_to_layer_input=False,\n",
    "                                    return_convergence_delta=False)\n",
    "    grads, outputs, steps, scaled_features = attributions\n",
    "\n",
    "    # Calculate threshold for split integrated gradients\n",
    "    baseline_out = forward_fn(layer_baseline)\n",
    "    final_out = forward_fn(layer_input)\n",
    "    threshold = baseline_out + ratio * (final_out - baseline_out)\n",
    "    threshold_index = None\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] > threshold:\n",
    "            threshold_index = i\n",
    "            break\n",
    "\n",
    "    # Grads have already been scaled and multiplied by inputs\n",
    "    left_ig = grads[:threshold_index]\n",
    "    right_ig = grads[threshold_index:]\n",
    "\n",
    "    return left_ig.sum(dim=0,keepdim=True), right_ig.sum(dim=0,keepdim=True), threshold_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea62e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_integrated_gradients(model: HookedTransformer, clean_tokens: torch.Tensor, clean_cache: ActivationCache, corrupted_cache: ActivationCache, metric: callable, metric_labels):\n",
    "    n_samples = clean_tokens.size(0)\n",
    "    \n",
    "    # Gradient attribution for neurons in MLP layers\n",
    "    mlp_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.d_mlp)\n",
    "    # Gradient attribution for attention heads\n",
    "    attn_results = torch.zeros(n_samples, model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "    # Calculate integrated gradients for each layer\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        # Gradient attribution on heads\n",
    "        hook_name = get_act_name(\"result\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"z\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "\n",
    "        # Shape [batch, seq_len, d_head, d_model]\n",
    "        left_ig, right_ig, _ = compute_layer_to_output_attributions_split_ig(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, metric_labels)\n",
    "\n",
    "        # Calculate attribution score based on mean over each embedding, for each token\n",
    "        per_token_score = left_ig.mean(dim=3)\n",
    "        score = per_token_score.mean(dim=1)\n",
    "        attn_results[:, layer] = score\n",
    "\n",
    "        # Gradient attribution on MLP neurons\n",
    "        hook_name = get_act_name(\"post\", layer)\n",
    "        target_layer = model.hook_dict[hook_name]\n",
    "        prev_layer_hook = get_act_name(\"mlp_in\", layer)\n",
    "        prev_layer = model.hook_dict[prev_layer_hook]\n",
    "\n",
    "        layer_clean_input = clean_cache[prev_layer_hook]\n",
    "        layer_corrupt_input = corrupted_cache[prev_layer_hook]\n",
    "        \n",
    "        # Shape [batch, seq_len, d_model]\n",
    "        left_ig, right_ig, _ = compute_layer_to_output_attributions_split_ig(\n",
    "            clean_tokens, layer_corrupt_input, layer_clean_input, target_layer, prev_layer, metric, metric_labels)\n",
    "        score = left_ig.mean(dim=1)\n",
    "        mlp_results[:, layer] = score\n",
    "\n",
    "    return mlp_results, attn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872d9db",
   "metadata": {},
   "source": [
    "# Split IG and IOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565578bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = get_device()\n",
    "# device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# Explicitly calculate and expose the result for each attention head\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad33fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_dataset = TaskDataset(Task.IOI)\n",
    "ioi_dataloader = ioi_dataset.to_dataloader(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eff2fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit difference: tensor([2.3827], device='cuda:0')\n",
      "Corrupted logit difference: tensor([-2.3610], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "clean_input, corrupted_input, labels = next(iter(ioi_dataloader))\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_input)\n",
    "corrupted_tokens = model.to_tokens(corrupted_input)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "clean_logit_diff = logit_diff_metric(clean_logits, labels)\n",
    "print(f\"Clean logit difference: {clean_logit_diff}\")\n",
    "\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "corrupted_logit_diff = logit_diff_metric(corrupted_logits, labels)\n",
    "print(f\"Corrupted logit difference: {corrupted_logit_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94259add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "23\n",
      "torch.Size([23, 18, 3072]) torch.Size([27, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "42\n",
      "torch.Size([42, 18, 3072]) torch.Size([8, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 12, 768]) torch.Size([50, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "43\n",
      "torch.Size([43, 18, 12, 768]) torch.Size([7, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "42\n",
      "torch.Size([42, 18, 3072]) torch.Size([8, 18, 3072])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "43\n",
      "torch.Size([43, 18, 12, 768]) torch.Size([7, 18, 12, 768])\n",
      "('forward_fn', 'inputs', 'target_ind', 'additional_forward_args', 'get_outputs', 'scattered_inputs', 'scattered_input', 'layer_forward_hook', 'hooks', 'layers', 'layer_idx', 'layer', 'hook', 'output', 'grads')\n",
      "Split IG attribution...\n",
      "4\n",
      "<class 'tuple'> 4\n",
      "0\n",
      "torch.Size([0, 18, 3072]) torch.Size([50, 18, 3072])\n"
     ]
    }
   ],
   "source": [
    "ioi_split_ig_mlp, ioi_split_ig_attn = split_integrated_gradients(model, clean_tokens, clean_cache, corrupted_cache, logit_diff_metric, labels)\n",
    "\n",
    "torch.save(ioi_split_ig_mlp, \"saved_results/ioi_split_ig_mlp.pt\")\n",
    "torch.save(ioi_split_ig_attn, \"saved_results/ioi_split_ig_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4db256ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_split_ig_mlp = torch.load(\"saved_results/ioi_split_ig_mlp.pt\")\n",
    "ioi_split_ig_attn = torch.load(\"saved_results/ioi_split_ig_attn.pt\")\n",
    "\n",
    "ioi_ap_mlp = torch.load(\"saved_results/ioi_ap_mlp.pt\")\n",
    "ioi_ap_attn = torch.load(\"saved_results/ioi_ap_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f5cdfff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_attn_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mioi_split_ig_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioi_ap_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIOI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m plot_correlation(ioi_split_ig_mlp, ioi_ap_mlp, ioi_split_ig_attn, ioi_ap_attn, Task.IOI)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gradient_causal_knowledge/utils.py:116\u001b[39m, in \u001b[36mplot_attn_comparison\u001b[39m\u001b[34m(ig_attn_results, ap_attn_results, task, model)\u001b[39m\n\u001b[32m    112\u001b[39m     plt.tight_layout()\n\u001b[32m    113\u001b[39m     plt.show()\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_attn_comparison\u001b[39m(ig_attn_results, ap_attn_results, task: Task, model: HookedTransformer):\n\u001b[32m    118\u001b[39m     n_results = ig_attn_results.size(\u001b[32m0\u001b[39m)\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m n_results == ap_attn_results.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "plot_attn_comparison(ioi_split_ig_attn, ioi_ap_attn, Task.IOI, model)\n",
    "plot_correlation(ioi_split_ig_mlp, ioi_ap_mlp, ioi_split_ig_attn, ioi_ap_attn, Task.IOI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
