[
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "",
    "text": "Colab: exercises | solutions\nStreamlit page\nPlease send any problems / bugs on the #errata channel in the Slack group, and ask any questions on the dedicated channels for this chapter of material."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#motivation",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#motivation",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Motivation",
    "text": "Motivation\nIn A Mathematical Framework for Transformer Circuits, we got a lot of traction interpreting toy language models - that is, transformers trained in exactly the same way as larger models, but with only 1 or 2 layers. It seems likely that there’s a lot of low-hanging fruit left to pluck when studying toy language models!\nSo, why care about studying toy language models? The obvious reason is that it’s way easier to get traction. In particular, the inputs and outputs of a model are intrinsically interpretable, and in a toy model there’s just not as much space between the inputs and outputs for weird complexity to build up. But the obvious objection to the above is that, ultimately, we care about understanding real models (and ideally extremely large ones like GPT-3), and learning to interpret toy models is not the actual goal. This is a pretty valid objection, but there are two natural ways that studying toy models can be valuable:\nThe first is by finding fundamental circuits that recur in larger models, and motifs that allow us to easily identify these circuits in larger models. A key underlying question here is that of universality: does each model learn its own weird way of completing its task, or are there some fundamental principles and algorithms that all models converge on?\nThe second is by forming a better understanding of how to reverse engineer models - what are the right intuitions and conceptual frameworks, what tooling and techniques do and do not work, and what weird limitations we might be faced with. For instance, the work in A Mathematical Framework presents ideas like the residual stream as the central object, and the significance of the QK-Circuits and OV-Circuits, which seem to generalise to many different models. We’ll also see an example later in these exercises which illustrates how MLPs can be thought of as a collection of neurons which activate on different features, just like many seem to in language models. But there’s also ways it can be misleading, and some techniques that work well in toy models seem to generalise less well."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#the-purpose-structure-of-these-exercises",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#the-purpose-structure-of-these-exercises",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "The purpose / structure of these exercises",
    "text": "The purpose / structure of these exercises\nAt a surface level, these exercises are designed to guide you through a partial interpretation of the bidirectional model trained on bracket classification. But it’s also designed to make you a better interpretability researcher! As a result, most exercises will be doing a combination of:\n\nShowing you some new feature/component of the circuit, and\nTeaching you how to use tools and interpret results in a broader mech interp context.\n\nAs you’re going through these exercises, it’s easy to get lost in the fiddly details of the techniques you’re implementing or the things you’re computing. Make sure you keep taking a high-level view, asking yourself what questions you’re currently trying to ask and how you’ll interpret the output you’re getting, as well as how the tools you’re currently using are helping guide you towards a better understanding of the model."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#content-learning-objectives",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#content-learning-objectives",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Content & Learning Objectives",
    "text": "Content & Learning Objectives\n\n1️⃣ Bracket classifier\nThis section describes how transformers can be used for classification, and the details of how this works in TransformerLens (using permanent hooks). It also takes you through the exercise of hand-writing a solution to the balanced brackets problem.\nThis section mainly just lays the groundwork; it is very light on content.\n\nLearning objctives\n\nUnderstand how transformers can be used for classification.\nUnderstand how to implement specific kinds of transformer behaviour (e.g. masking of padding tokens) via permanent hooks in TransformerLens.\nStart thinking about the kinds of algorithmic solutions a transformer is likely to find for problems such as these, given its inductive biases.\n\n\n\n\n2️⃣ Moving backwards\nHere, you’ll perform logit attribution, and learn how to work backwards through particular paths of a model to figure out which components matter most for the final classification probabilities.\nThis is the first time you’ll have to deal with LayerNorm in your models.\nThis section should be familiar if you’ve done logit attribution for induction heads (although these exercises are slightly more challenging from a coding perspective). The LayerNorm-based exercises are a bit fiddly!\n\nLearning objctives\n\nUnderstand how to perform logit attribution.\nUnderstand how to work backwards through a model to identify which components matter most for the final classification probabilities.\nUnderstand how LayerNorm works, and look at some ways to deal with it in your models.\n\n\n\n\n3️⃣ Total elevation circuit\nThis section is quite challenging both from a coding and conceptual perspective, because you need to link the results of your observations and interventions to concrete hypotheses about how the model works.\nIn the largest section of the exercises, you’ll examine the attention patterns in different heads, and interpret them as performing some human-understandable algorithm (e.g. copying, or aggregation). You’ll use your observations to make deductions about how a particular type of balanced brackets failure mode (mismatched number of left and right brackets) is detected by your model.\nThis is the first time you’ll have to deal with MLPs in your models.\n\nLearning objctives\n\nPractice connecting distinctive attention patterns to human-understandable algorithms, and making deductions about model behaviour.\nUnderstand how MLPs can be viewed as a collection of neurons.\nBuild up to a full picture of the total elevation circuit and how it works.\n\n\n\n\n4️⃣ Bonus exercises\nLastly, there are a few optional bonus exercises which build on the previous content (e.g. having you examine different parts of the model, or use your understanding of how the model works to generate adversarial examples).\nThis final section is less guided, although the suggested exercises are similar in flavour to the previous section.\n\nLearning objctives\n\nUse your understanding of how the model works to generate adversarial examples.\nTake deeper dives into specific anomalous features of the model."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#setup-dont-read-just-run",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#setup-dont-read-just-run",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Setup (don’t read, just run!)",
    "text": "Setup (don’t read, just run!)\n\ntry:\n    import google.colab # type: ignore\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nimport os, sys\nchapter = \"chapter1_transformer_interp\"\nrepo = \"ARENA_3.0\"\n\nif IN_COLAB:\n    if not os.path.exists(f\"/content/{chapter}\"):\n        # Install packages\n        %pip install einops\n        %pip install jaxtyping\n        %pip install transformer_lens\n        %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n\n        # Code to download the necessary files (e.g. solutions, test funcs)\n        !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/arena_pre_v4.zip\n        !unzip /content/arena_pre_v4.zip 'ARENA_3.0-arena_pre_v4/chapter1_transformer_interp/exercises/*'\n        sys.path.append(f\"/content/{repo}-arena_pre_v4/{chapter}/exercises\")\n        os.remove(\"/content/arena_pre_v4.zip\")\n        os.rename(f\"{repo}-arena_pre_v4/{chapter}\", chapter)\n        os.rmdir(f\"{repo}-arena_pre_v4\")\n        os.chdir(f\"{chapter}/exercises\")\nelse:\n    raise Exception(\"If running from VSCode, you should copy code from the Streamlit page, not the Colab.\")\n\n\nimport json\nimport sys\nfrom functools import partial\nfrom pathlib import Path\n\nimport circuitsvis as cv\nimport einops\nimport torch as t\nfrom IPython.display import display\nfrom jaxtyping import Bool, Float, Int\nfrom sklearn.linear_model import LinearRegression\nfrom torch import Tensor, nn\nfrom tqdm import tqdm\nfrom transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig, utils\nfrom transformer_lens.hook_points import HookPoint\n\n# Make sure exercises are in the path\nexercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\nsection_dir = exercises_dir / \"part51_balanced_bracket_classifier\"\nif str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n\nimport plotly_utils\nfrom plotly_utils import hist, bar, imshow\nimport part7_balanced_bracket_classifier.tests as tests\nfrom part7_balanced_bracket_classifier.brackets_datasets import SimpleTokenizer, BracketsDataset\n\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n\nMAIN = __name__ == \"__main__\"\n\n/usr/local/lib/python3.10/dist-packages/accelerate/utils/imports.py:209: UserWarning: `ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\n  warnings.warn("
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#todays-toy-model",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#todays-toy-model",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Today’s Toy Model",
    "text": "Today’s Toy Model\nToday we’ll study a small transformer that is trained to only classify whether a sequence of parentheses is balanced or not. It’s small so we can run experiments quickly, but big enough to perform well on the task. The weights and architecture are provided for you.\n\nCausal vs bidirectional attention\nThe key difference between this and the GPT-style models you will have implemented already is the attention mechanism.\nGPT uses causal attention, where the attention scores get masked wherever the source token comes after the destination token. This means that information can only flow forwards in a model, never backwards (which is how we can train our model in parallel - our model’s output is a series of distributions over the next token, where each distribution is only able to use information from the tokens that came before). This model uses bidirectional attention, where the attention scores aren’t masked based on the relative positions of the source and destination tokens. This means that information can flow in both directions, and the model can use information from the future to predict the past.\n\n\nUsing transformers for classification\nGPT is trained via gradient descent on the cross-entropy loss between its predictions for the next token and the actual next tokens. Models designed to perform classification are trained in a very similar way, but instead of outputting probability distributions over the next token, they output a distribution over class labels. We do this by having an unembedding matrix of size [d_model, num_classifications], and only using a single sequence position (usually the 0th position) to represent our classification probabilities.\nBelow is a schematic to compare the model architectures and how they’re used:\n\nNote that, just because the outputs at all other sequence positions are discarded, doesn’t mean those sequence positions aren’t useful. They will almost certainly be the sites of important intermediate calculations. But it does mean that the model will always have to move the information from those positions to the 0th position in order for the information to be used for classification.\n\n\nA note on softmax\nFor each bracket sequence, our (important) output is a vector of two values: (l0, l1), representing the model’s logit distribution over (unbalanced, balanced). Our model was trained by minimizing the cross-entropy loss between these logits and the true labels. Interestingly, since logits are translation invariant, the only value we actually care about is the difference between our logits, l0 - l1. This is the model’s log likelihood ratio of the sequence being unbalanced vs balanced. Later on, we’ll be able to use this logit_diff to perform logit attribution in our model.\n\n\nMasking padding tokens\nThe image on the top-right is actually slightly incomplete. It doesn’t show how our model handles sequences of differing lengths. After all, during training we need to have all sequences be of the same length so we can batch them together in a single tensor. The model manages this via two new tokens: the end token and the padding token.\nThe end token goes at the end of every bracket sequence, and then we add padding tokens to the end until the sequence is up to some fixed length. For instance, this model was trained on bracket sequences of up to length 40, so if we wanted to classify the bracket string (()) then we would pad it to the length-42 sequence:\n[start] + ( + ( + ) + ) + [end] + [pad] + [pad] + ... + [pad]\nWhen we calculate the attention scores, we mask them at all (query, key) positions where the key is a padding token. This makes sure that information doesn’t flow from padding tokens to other tokens in the sequence (just like how GPT’s causal masking makes sure that information doesn’t flow from future tokens to past tokens).\n\nNote that the attention scores aren’t masked when the query is a padding token and the key isn’t. In theory, this means that information can be stored in the padding token positions. However, because the padding token key positions are always masked, this information can’t flow back into the rest of the sequence, so it never affects the final output. (Also, note that if we masked query positions as well, we’d get numerical errors, since we’d be taking softmax across a row where every element is minus infinity, which is not well-defined!)\n\n\nAside on how this relates to BERT\n\nThis is all very similar to how the bidirectional transformer BERT works:\n\nBERT has the [CLS] (classification) token rather than [start]; but it works exactly the same.\nBERT has the [SEP] (separation) token rather than [end]; this has a similar function but also serves a special purpose when it is used in NSP (next sentence prediction).\n\nIf you’re interested in reading more on this, you can check out this link.\n\nWe’ve implemented this type of masking for you, using TransformerLens’s permanent hooks feature. We will discuss the details of this below (permanent hooks are a recent addition to TransformerLens which we havent’ covered yet, and they’re useful to understand).\n\n\nOther details\nHere is a summary of all the relevant architectural details:\n\nPositional embeddings are sinusoidal (non-learned).\nIt has hidden_size (aka d_model, aka embed_dim) of 56.\nIt has bidirectional attention, like BERT.\nIt has 3 attention layers and 3 MLPs.\nEach attention layer has two heads, and each head has headsize (aka d_head) of hidden_size / num_heads = 28.\nThe MLP hidden layer has 56 neurons (i.e. its linear layers are square matrices).\nThe input of each attention layer and each MLP is first layernormed, like in GPT.\nThere’s a LayerNorm on the residual stream after all the attention layers and MLPs have been added into it (this is also like GPT).\nOur embedding matrix W_E has five rows: one for each of the tokens [start], [pad], [end], (, and ) (in that order).\nOur unembedding matrix W_U has two columns: one for each of the classes unbalanced and balanced (in that order).\n\nWhen running our model, we get output of shape [batch, seq_len, 2], and we then take the [:, 0, :] slice to get the output for the [start] token (i.e. the classification logits).\nWe can then softmax to get our classification probabilities.\n\nActivation function is ReLU.\n\nTo refer to attention heads, we’ll again use the shorthand layer.head where both layer and head are zero-indexed. So 2.1 is the second attention head (index 1) in the third layer (index 2).\n\n\nSome useful diagrams\nHere is a high-level diagram of your model’s architecture:\n\nHere is a link to a diagram of the archicture of a single model layer (which includes names of activations, as well as a list of useful methods for indexing into the model).\nI’d recommend having both these images open in a different tab.\n\n\nDefining the model\nHere, we define the model according to the description we gave above.\n\nVOCAB = \"()\"\n\ncfg = HookedTransformerConfig(\n    n_ctx=42,\n    d_model=56,\n    d_head=28,\n    n_heads=2,\n    d_mlp=56,\n    n_layers=3,\n    attention_dir=\"bidirectional\", # defaults to \"causal\"\n    act_fn=\"relu\",\n    d_vocab=len(VOCAB)+3, # plus 3 because of end and pad and start token\n    d_vocab_out=2, # 2 because we're doing binary classification\n    use_attn_result=True,\n    device=device,\n    use_hook_tokens=True\n)\n\nmodel = HookedTransformer(cfg).eval()\n\nstate_dict = t.load(section_dir / \"brackets_model_state_dict.pt\")\nmodel.load_state_dict(state_dict)\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#tokenizer",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#tokenizer",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Tokenizer",
    "text": "Tokenizer\nThere are only five tokens in our vocabulary: [start], [pad], [end], (, and ) in that order. See earlier sections for a reminder of what these tokens represent.\nYou have been given a tokenizer SimpleTokenizer(\"()\") which will give you some basic functions. Try running the following to see what they do:\n\ntokenizer = SimpleTokenizer(\"()\")\n\n# Examples of tokenization\n# (the second one applies padding, since the sequences are of different lengths)\nprint(tokenizer.tokenize(\"()\"))\nprint(tokenizer.tokenize([\"()\", \"()()\"]))\n\n# Dictionaries mapping indices to tokens and vice versa\nprint(tokenizer.i_to_t)\nprint(tokenizer.t_to_i)\n\n# Examples of decoding (all padding tokens are removed)\nprint(tokenizer.decode(t.tensor([[0, 3, 4, 2, 1, 1]])))\n\ntensor([[0, 3, 4, 2]])\ntensor([[0, 3, 4, 2, 1, 1],\n        [0, 3, 4, 3, 4, 2]])\n{3: '(', 4: ')', 0: '[start]', 1: '[pad]', 2: '[end]'}\n{'(': 3, ')': 4, '[start]': 0, '[pad]': 1, '[end]': 2}\n['()']\n\n\n\nImplementing our masking\nNow that we have the tokenizer, we can use it to write hooks that mask the padding tokens. If you understand how the padding works, then don’t worry if you don’t follow all the implementational details of this code.\n\n\nClick to see a diagram explaining how this masking works (should help explain the code below)\n\n\n\n\ndef add_perma_hooks_to_mask_pad_tokens(model: HookedTransformer, pad_token: int) -&gt; HookedTransformer:\n\n    # Hook which operates on the tokens, and stores a mask where tokens equal [pad]\n    def cache_padding_tokens_mask(tokens: Float[Tensor, \"batch seq\"], hook: HookPoint) -&gt; None:\n        hook.ctx[\"padding_tokens_mask\"] = einops.rearrange(tokens == pad_token, \"b sK -&gt; b 1 1 sK\")\n\n    # Apply masking, by referencing the mask stored in the `hook_tokens` hook context\n    def apply_padding_tokens_mask(\n        attn_scores: Float[Tensor, \"batch head seq_Q seq_K\"],\n        hook: HookPoint,\n    ) -&gt; None:\n        attn_scores.masked_fill_(model.hook_dict[\"hook_tokens\"].ctx[\"padding_tokens_mask\"], -1e5)\n        if hook.layer() == model.cfg.n_layers - 1:\n            del model.hook_dict[\"hook_tokens\"].ctx[\"padding_tokens_mask\"]\n\n    # Add these hooks as permanent hooks (i.e. they aren't removed after functions like run_with_hooks)\n    for name, hook in model.hook_dict.items():\n        if name == \"hook_tokens\":\n            hook.add_perma_hook(cache_padding_tokens_mask)\n        elif name.endswith(\"attn_scores\"):\n            hook.add_perma_hook(apply_padding_tokens_mask)\n\n    return model\n\nmodel.reset_hooks(including_permanent=True)\nmodel = add_perma_hooks_to_mask_pad_tokens(model, tokenizer.PAD_TOKEN)"
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#dataset",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#dataset",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Dataset",
    "text": "Dataset\nEach training example consists of [start], up to 40 parens, [end], and then as many [pad] as necessary.\nIn the dataset we’re using, half the sequences are balanced, and half are unbalanced. Having an equal distribution is on purpose to make it easier for the model.\nRemember to download the brackets_data.json file from this Google Drive link if you haven’t already.\n\nN_SAMPLES = 5000\nwith open(section_dir / \"brackets_data.json\") as f:\n    data_tuples: list[tuple[str, bool]] = json.load(f)\n    print(f\"loaded {len(data_tuples)} examples\")\nassert isinstance(data_tuples, list)\ndata_tuples = data_tuples[:N_SAMPLES]\ndata = BracketsDataset(data_tuples).to(device)\ndata_mini = BracketsDataset(data_tuples[:100]).to(device)\n\nloaded 100000 examples\n\n\nYou are encouraged to look at the code for BracketsDataset (scroll up to the setup code at the top - but make sure to not look to closely at the solutions!) to see what methods and properties the data object has.\n\nData visualisation\nAs is good practice, let’s examine the dataset and plot the distribution of sequence lengths (e.g. as a histogram). What do you notice?\n\nhist(\n    [len(x) for x, _ in data_tuples],\n    nbins=data.seq_length,\n    title=\"Sequence lengths of brackets in dataset\",\n    labels={\"x\": \"Seq len\"}\n)\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nFeatures of dataset\n\nThe most striking feature is that all bracket strings have even length. We constructed our dataset this way because if we had odd-length strings, the model would presumably have learned the heuristic “if the string is odd-length, it’s unbalanced”. This isn’t hard to learn, and we want to focus on the more interesting question of how the transformer is learning the structure of bracket strings, rather than just their length.\nBonus exercise (optional) - can you describe an algorithm involving a single attention head which the model could use to distinguish between even and odd-length bracket strings?\n\n\nAnswer\n\nThe algorithm might look like:\n\nQK circuit causes head to attend from seqpos=0 to the largest non-masked sequence position (e.g. we could have the key-query dot products of positional embeddings q[0] @ k[i] be a decreasing function of i = 0, 1, 2, ...)\nOV circuit maps the parity component of positional embeddings to a prediction, i.e. all odd positions would be mapped to an “unbalanced” prediction, and even positions to a “balanced” prediction\n\nAs an extra exercise, can you construct such a head by hand?\n\n\nNow that we have all the pieces in place, we can try running our model on the data and generating some predictions.\n\n# Define and tokenize examples\nexamples = [\"()()\", \"(())\", \"))((\", \"()\", \"((()()()()))\", \"(()()()(()(())()\", \"()(()(((())())()))\"]\nlabels = [True, True, False, True, True, False, True]\ntoks = tokenizer.tokenize(examples)\n\n# Get output logits for the 0th sequence position (i.e. the [start] token)\nlogits = model(toks)[:, 0]\n\n# Get the probabilities via softmax, then get the balanced probability (which is the second element)\nprob_balanced = logits.softmax(-1)[:, 1]\n\n# Display output\nprint(\"Model confidence:\\n\" + \"\\n\".join([f\"{ex:18} : {prob:&lt;8.4%} : label={int(label)}\" for ex, prob, label in zip(examples, prob_balanced, labels)]))\n\nModel confidence:\n()()               : 99.9986% : label=1\n(())               : 99.9989% : label=1\n))((               : 0.0005%  : label=0\n()                 : 99.9987% : label=1\n((()()()()))       : 99.9987% : label=1\n(()()()(()(())()   : 0.0006%  : label=0\n()(()(((())())())) : 99.9982% : label=1\n\n\nWe can also run our model on the whole dataset, and see how many brackets are correctly classified.\n\ndef run_model_on_data(model: HookedTransformer, data: BracketsDataset, batch_size: int = 200) -&gt; Float[Tensor, \"batch 2\"]:\n    '''Return probability that each example is balanced'''\n    all_logits = []\n    for i in tqdm(range(0, len(data.strs), batch_size)):\n        toks = data.toks[i : i + batch_size]\n        logits = model(toks)[:, 0]\n        all_logits.append(logits)\n    all_logits = t.cat(all_logits)\n    assert all_logits.shape == (len(data), 2)\n    return all_logits\n\ntest_set = data\nn_correct = (run_model_on_data(model, test_set).argmax(-1).bool() == test_set.isbal).sum()\nprint(f\"\\nModel got {n_correct} out of {len(data)} training examples correct!\")\n\n100%|██████████| 25/25 [00:00&lt;00:00, 151.43it/s]\n\n\n\nModel got 5000 out of 5000 training examples correct!"
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#algorithmic-solutions",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#algorithmic-solutions",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Algorithmic Solutions",
    "text": "Algorithmic Solutions\n\nExercise - handwritten solution (for loop)\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵🔵⚪⚪\n\nYou shouldn't spend more than ~10 minutes on this exercise.\n\nThis exercise and the next one should both be relatively easy (especially if you've already solved this problem on LeetCode before!), and they're very important for the rest of the exercises.\nA nice property of using such a simple problem is we can write a correct solution by hand. Take a minute to implement this using a for loop and if statements.\n\ndef is_balanced_forloop(parens: str) -&gt; bool:\n    '''\n    Return True if the parens are balanced.\n\n    Parens is just the ( and ) characters, no begin or end tokens.\n    '''\n    # SOLUTION\n    cumsum = 0\n    for paren in parens:\n        cumsum += 1 if paren == \"(\" else -1\n        if cumsum &lt; 0:\n            return False\n\n    return cumsum == 0\n\n\nfor (parens, expected) in zip(examples, labels):\n    actual = is_balanced_forloop(parens)\n    assert expected == actual, f\"{parens}: expected {expected} got {actual}\"\nprint(\"is_balanced_forloop ok!\")\n\nis_balanced_forloop ok!\n\n\n\n\nExercise - handwritten solution (vectorized)\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵🔵⚪⚪\n\nYou shouldn't spend more than ~10 minutes on this exercise.\nA transformer has an inductive bias towards vectorized operations, because at each sequence position the same weights “execute”, just on different data. So if we want to “think like a transformer”, we want to get away from procedural for/if statements and think about what sorts of solutions can be represented in a small number of transformer weights.\nBeing able to represent a solutions in matrix weights is necessary, but not sufficient to show that a transformer could learn that solution through running SGD on some input data. It could be the case that some simple solution exists, but a different solution is an attractor when you start from random initialization and use current optimizer algorithms.\n\ndef is_balanced_vectorized(tokens: Float[Tensor, \"seq_len\"]) -&gt; bool:\n    '''\n    Return True if the parens are balanced.\n\n    tokens is a vector which has start/pad/end indices (0/1/2) as well as left/right brackets (3/4)\n    '''\n    # SOLUTION\n    # Convert start/end/padding tokens to zero, and left/right brackets to +1/-1\n    table = t.tensor([0, 0, 0, 1, -1])\n    change = table[tokens]\n    # Get altitude by taking cumulative sum\n    altitude = t.cumsum(change, -1)\n    # Check that the total elevation is zero and that there are no negative altitudes\n    no_total_elevation_failure = altitude[-1] == 0\n    no_negative_failure = altitude.min() &gt;= 0\n\n    return (no_total_elevation_failure & no_negative_failure).item()\n\n\nfor (tokens, expected) in zip(tokenizer.tokenize(examples), labels):\n    actual = is_balanced_vectorized(tokens)\n    assert expected == actual, f\"{tokens}: expected {expected} got {actual}\"\nprint(\"is_balanced_vectorized ok!\")\n\nis_balanced_vectorized ok!\n\n\n\n\nHint\n\nOne solution is to map begin, pad, and end tokens to zero, map open paren to 1 and close paren to -1. Then take the cumulative sum, and check the two conditions which are necessary and sufficient for the bracket string to be balanced."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#the-models-solution",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#the-models-solution",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "The Model’s Solution",
    "text": "The Model’s Solution\nIt turns out that the model solves the problem like this:\nAt each position i, the model looks at the slice starting at the current position and going to the end: seq[i:]. It then computes (count of closed parens minus count of open parens) for that slice to generate the output at that position.\nWe’ll refer to this output as the “elevation” at i, or equivalently the elevation for each suffix seq[i:].\nThe sequence is imbalanced if one or both of the following is true:\n\nelevation[0] is non-zero\nany(elevation &lt; 0)\n\nFor English readers, it’s natural to process the sequence from left to right and think about prefix slices seq[:i] instead of suffixes, but the model is bidirectional and has no idea what English is. This model happened to learn the equally valid solution of going right-to-left.\nWe’ll spend today inspecting different parts of the network to try to get a first-pass understanding of how various layers implement this algorithm. However, we’ll also see that neural networks are complicated, even those trained for simple tasks, and we’ll only be able to explore a minority of the pieces of the puzzle."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#moving-back-to-the-residual-stream",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#moving-back-to-the-residual-stream",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Moving back to the residual stream",
    "text": "Moving back to the residual stream\nThe final part of the model is the classification head, which has three stages - the final layernorm, the unembedding, and softmax, at the end of which we get our probabilities.\n\nNote - for simplicity, we’ll ignore the batch dimension in the following discussion.\nSome notes on the shapes of the objects in the diagram:\n\nx_2 is the vector in the residual stream after layer 2’s attention heads and MLPs. It has shape (seq_len, d_model).\nfinal_ln_output has shape (seq_len, d_model).\nW_U has shape (d_model, 2), and so logits has shape (seq_len, 2).\nWe get P(unbalanced) by taking the 0th element of the softmaxed logits, for sequence position 0.\n\n\nStage 1: Translating through softmax\nLet’s get P(unbalanced) as a function of the logits. Luckily, this is easy. Since we’re doing the softmax over two elements, it simplifies to the sigmoid of the difference of the two logits:\n\\[\n\\text{softmax}(\\begin{bmatrix} \\text{logit}_0 \\\\ \\text{logit}_1 \\end{bmatrix})_0 = \\frac{e^{\\text{logit}_0}}{e^{\\text{logit}_0} + e^{\\text{logit}_1}} = \\frac{1}{1 + e^{\\text{logit}_1 - \\text{logit}_0}} = \\text{sigmoid}(\\text{logit}_0 - \\text{logit}_1)\n\\]\nSince sigmoid is monotonic, a large value of \\(\\hat{y}_0\\) follows from logits with a large \\(\\text{logit}_0 - \\text{logit}_1\\). From now on, we’ll only ask “What leads to a large difference in logits?”\n\n\nStage 2: Translating through linear\nThe next step we encounter is the decoder: logits = final_LN_output @ W_U, where\n\nW_U has shape (d_model, 2)\nfinal_LN_output has shape (seq_len, d_model)\n\nWe can now put the difference in logits as a function of \\(W\\) and \\(x_{\\text{linear}}\\) like this:\nlogit_diff = (final_LN_output @ W_U)[0, 0] - (final_LN_output @ W_U)[0, 1]\n\n           = final_LN_output[0, :] @ (W_U[:, 0] - W_U[:, 1])\n(recall that the (i, j)th element of matrix AB is A[i, :] @ B[:, j])\nSo a high difference in the logits follows from a high dot product of the output of the LayerNorm with the corresponding unembedding vector. We’ll call this the post_final_ln_dir, i.e. the unbalanced direction for values in the residual stream after the final layernorm.\n\n\nExercise - get the post_final_ln_dir\nDifficulty: 🔴⚪⚪⚪⚪\nImportance: 🔵🔵🔵⚪⚪\n\nYou shouldn't spend more than ~5 minutes on this exercise.\nIn the function below, you should compute this vector (this should just be a one-line function).\n\ndef get_post_final_ln_dir(model: HookedTransformer) -&gt; Float[Tensor, \"d_model\"]:\n    '''\n    Returns the direction in which final_ln_output[0, :] should point to maximize P(unbalanced)\n    '''\n    # SOLUTION\n    return model.W_U[:, 0] - model.W_U[:, 1]\n\n\ntests.test_get_post_final_ln_dir(get_post_final_ln_dir, model)\n\nAll tests in `test_get_post_final_ln_dir` passed!\n\n\n\n\nStage 3: Translating through LayerNorm\nWe want to find the unbalanced direction before the final layer norm, since this is where we can write the residual stream as a sum of terms. LayerNorm messes with this sort of direction analysis, since it is nonlinear. For today, however, we will approximate it with a linear fit. This is good enough to allow for interesting analysis (see for yourself that the \\(R^2\\) values are very high for the fit)!\nWith a linear approximation to LayerNorm, which I’ll use the matrix L_final for, we can translate “What is the dot product of the output of the LayerNorm with the unbalanced-vector?” to a question about the input to the LN. We simply write:\nfinal_ln_output[0, :] = final_ln(x_linear[0, :])\n\n                      = L_final @ x_linear[0, :]\n\n\nAn aside on layernorm\n\nLayernorm isn’t actually linear. It’s a combination of a nonlinear function (subtracting mean and dividing by std dev) with a linear one (a learned affine transformation).\nHowever, in this case it turns out to be a decent approximation to use a linear fit. The reason we’ve included layernorm in these exercises is to give you an idea of how nonlinear functions can complicate our analysis, and some simple hacky ways that we can deal with them.\nWhen applying this kind of analysis to LLMs, it’s sometimes harder to abstract away layernorm as just a linear transformation. For instance, many large transformers use layernorm to “clear” parts of their residual stream, e.g. they learn a feature 100x as large as everything else and use it with layer norm to clear the residual stream of everything but that element. Clearly, this kind of behaviour is not well-modelled by a linear fit.\n\n\n\nSummary\nWe can use the logit diff as a measure of how strongly our model is classifying a bracket string as unbalanced (higher logit diff = more certain that the string is unbalanced).\nWe can approximate logit diff as a linear function of pre_final_ln_dir (because the unembedding is linear, and the layernorm is approximately linear). This means we can approximate logit diff as the dot product of post_final_ln_dir with the residual stream value before the final layernorm. If we could find this post_final_ln_dir, then we could start to answer other questions like which components’ output had the highest dot product with this value.\nThe diagram below shows how we can step back through the model to find our unbalanced direction pre_final_ln_dir. Notation: \\(x_2\\) refers to the residual stream value after layer 2’s attention heads and MLPs (i.e. just before the last layernorm), and \\(L_{final}\\) is the linear approximation of the final layernorm.\n\n\n\nExercise - get the pre_final_ln_dir\nDifficulty: 🔴🔴🔴🔴⚪\nImportance: 🔵🔵🔵⚪⚪\n\nYou shouldn't spend more than 20-30 minutes on the following exercises.\nIdeally, we would calculate pre_final_ln_dir directly from the model’s weights, like we did for post_final_ln_dir. Unfortunately, it’s not so easy in this case, because in order to get our linear approximation L_final, we need to fit a linear regression with actual data that gets passed through the model.\nBelow, you should implement the function get_ln_fit to fit a linear regression to the inputs and outputs of one of your model’s layernorms, and then get_pre_final_ln_dir which estimates the value of pre_final_ln_dir (as annotated in the diagram above).\nWe’ve given you a few helper functions:\n\nget_activation(s), which use the run_with_cache function to return one or several activations for a given batch of tokens\nLN_hook_names, which takes a layernorm in the model (e.g. model.ln_final) and returns the names of the hooks immediately before or after the layernorm. This will be useful in the get_activation(s) function, when you want to refer to these values (since your linear regression will be fitted on the inputs and outputs to your model’s layernorms).\n\nWhen it comes to fitting the regression, we recommend using the sklearn LinearRegression class to find a linear fit to the inputs and outputs of your model’s layernorms. You should include a fit coefficient in your regression (this is the default for LinearRegression).\nNote, we have the seq_pos argument because sometimes we’ll want to fit the regression over all sequence positions and sometimes we’ll only care about some and not others (e.g. for the final layernorm in the model, we only care about the 0th position because that’s where we take the prediction from; all other positions are discarded).\n\ndef get_activations(\n    model: HookedTransformer, toks: Int[Tensor, \"batch seq\"], names: list[str]\n) -&gt; ActivationCache:\n    \"\"\"Uses hooks to return activations from the model, in the form of an ActivationCache.\"\"\"\n    names_list = [names] if isinstance(names, str) else names\n    _, cache = model.run_with_cache(\n        toks,\n        return_type=None,\n        names_filter=lambda name: name in names_list,\n    )\n    return cache\n\n\ndef get_activation(model: HookedTransformer, toks: Int[Tensor, \"batch seq\"], name: str):\n    \"\"\"Gets a single activation.\"\"\"\n    return get_activations(model, toks, [name])[name]\n\n\ndef LN_hook_names(layernorm: nn.Module) -&gt; tuple[str, str]:\n    \"\"\"\n    Returns the names of the hooks immediately before and after a given layernorm.\n\n    Example:\n        model.final_ln -&gt; (\"blocks.2.hook_resid_post\", \"ln_final.hook_normalized\")\n    \"\"\"\n    if layernorm.name == \"ln_final\":\n        input_hook_name = utils.get_act_name(\"resid_post\", 2)\n        output_hook_name = \"ln_final.hook_normalized\"\n    else:\n        layer, ln = layernorm.name.split(\".\")[1:]\n        input_hook_name = utils.get_act_name(\"resid_pre\" if ln==\"ln1\" else \"resid_mid\", layer)\n        output_hook_name = utils.get_act_name('normalized', layer, ln)\n\n    return input_hook_name, output_hook_name\n\n\ndef get_ln_fit(\n    model: HookedTransformer, data: BracketsDataset, layernorm: nn.Module, seq_pos: Optional[int] = None\n) -&gt; tuple[LinearRegression, float]:\n    \"\"\"\n    Fits a linear regression, where the inputs are the values just before the layernorm given by the\n    input argument `layernorm`, and the values to predict are the layernorm's outputs.\n\n    if `seq_pos` is None, find best fit aggregated over all sequence positions. Otherwise, fit only\n    for the activations at `seq_pos`.\n\n    Returns: A tuple of a (fitted) sklearn LinearRegression object and the r^2 of the fit.\n    \"\"\"\n    activations_dict = get_activations(model, data.toks, [input_hook_name, output_hook_name])\n    inputs = utils.to_numpy(activations_dict[input_hook_name])\n    outputs = utils.to_numpy(activations_dict[output_hook_name])\n\n    if seq_pos is None:\n        inputs = einops.rearrange(inputs, \"batch seq d_model -&gt; (batch seq) d_model\")\n        outputs = einops.rearrange(outputs, \"batch seq d_model -&gt; (batch seq) d_model\")\n    else:\n        inputs = inputs[:, seq_pos, :]\n        outputs = outputs[:, seq_pos, :]\n\n    final_ln_fit = LinearRegression().fit(inputs, outputs)\n\n    r2 = final_ln_fit.score(inputs, outputs)\n\n    return (final_ln_fit, r2)\n\n\ntests.test_get_ln_fit(get_ln_fit, model, data_mini)\n\n_, r2 = get_ln_fit(model, data, layernorm=model.ln_final, seq_pos=0)\nprint(f\"r^2 for LN_final, at sequence position 0: {r2:.4f}\")\n_, r2 = get_ln_fit(model, data, layernorm=model.blocks[1].ln1, seq_pos=None)\nprint(f\"r^2 for LN1, layer 1, over all sequence positions: {r2:.4f}\")\n\n\ndef get_pre_final_ln_dir(model: HookedTransformer, data: BracketsDataset) -&gt; Float[Tensor, \"d_model\"]:\n    \"\"\"\n    Returns the direction in residual stream (pre ln_final, at sequence position 0) which\n    most points in the direction of making an unbalanced classification.\n    \"\"\"\n    post_final_ln_dir = get_post_final_ln_dir(model)\n\n    final_ln_fit = get_ln_fit(model, data, layernorm=model.ln_final, seq_pos=0)[0]\n    final_ln_coefs = t.from_numpy(final_ln_fit.coef_).to(device)\n\n    return final_ln_coefs.T @ post_final_ln_dir\n\n\ntests.test_get_pre_final_ln_dir(get_pre_final_ln_dir, model, data_mini)\n\nAll tests in `test_get_ln_fit` passed!\nr^2 for LN_final, at sequence position 0: 0.9820\nr^2 for LN1, layer 1, over all sequence positions: 0.9753\nAll tests in `test_get_pre_final_ln_dir` passed!\n\n\n\n\nHelp - I’m not sure how to fit the linear regression.\n\nIf inputs and outputs are both tensors of shape (samples, d_model), then LinearRegression().fit(inputs, outputs) returns the fit object which should be the first output of your function.\nYou can get the Rsquared with the .score method of the fit object.\n\n\n\nHelp - I’m not sure how to deal with the different seq_pos cases.\n\nIf seq_pos is an integer, you should take the vectors corresponding to just that sequence position. In other words, you should take the [:, seq_pos, :] slice of your [batch, seq_pos, d_model]-size tensors.\nIf seq_pos = None, you should rearrange your tensors into (batch seq_pos) d_model, because you want to run the regression on all sequence positions at once.\n\n\n3. Calculating pre_final_ln_dir\nArmed with our linear fit, we can now identify the direction in the residual stream before the final layer norm that most points in the direction of unbalanced evidence.\n\ndef get_pre_final_ln_dir(model: HookedTransformer, data: BracketsDataset) -&gt; Float[Tensor, \"d_model\"]:\n    '''\n    Returns the direction in residual stream (pre ln_final, at sequence position 0) which\n    most points in the direction of making an unbalanced classification.\n    '''\n    post_final_ln_dir = get_post_final_ln_dir(model)\n\n    final_ln_fit = get_ln_fit(model, data, layernorm=model.ln_final, seq_pos=0)[0]\n    final_ln_coefs = t.from_numpy(final_ln_fit.coef_).to(device)\n\n    return final_ln_coefs.T @ post_final_ln_dir\n\n\n\ntests.test_get_pre_final_ln_dir(get_pre_final_ln_dir, model, data_mini)\n\nAll tests in `test_get_pre_final_ln_dir` passed!"
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#writing-the-residual-stream-as-a-sum-of-terms",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#writing-the-residual-stream-as-a-sum-of-terms",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Writing the residual stream as a sum of terms",
    "text": "Writing the residual stream as a sum of terms\nAs we’ve seen in previous exercises, it’s much more natural to think about the residual stream as a sum of terms, each one representing a different path through the model. Here, we have ten components which write to the residual stream: the direct path (i.e. the embeddings), and two attention heads and one MLP on each of the three layers. We can write the residual stream as a sum of these terms.\n\nOnce we do this, we can narrow in on the components who are making direct contributions to the classification, i.e. which are writing vectors to the residual stream which have a high dot produce with the pre_final_ln_dir for unbalanced brackets relative to balanced brackets.\nIn order to answer this question, we need the following tools: - A way to break down the input to the LN by component. - A tool to identify a direction in the embedding space that causes the network to output ‘unbalanced’ (we already have this)\n\nExercise - breaking down the residual stream by component\nDifficulty: 🔴🔴🔴⚪⚪\nImportance: 🔵🔵⚪⚪⚪\n\nYou shouldn't spend more than 15-20 minutes on this exercise.\n\nIt isn't very conceptually important; the hardest part is getting all the right activation names & rearranging / stacking the tensors in the correct way.\nUse your get_activations function to create a tensor of shape [num_components, dataset_size, seq_pos], where the number of components = 10.\nThis is a termwise representation of the input to the final layer norm from each component (recall that we can see each head as writing something to the residual stream, which is eventually fed into the final layer norm). The order of the components in your function’s output should be the same as shown in the diagram above (i.e. in chronological order of how they’re added to the residual stream).\n(The only term missing from the sum of these is the W_O-bias from each of the attention layers).\n\n\nAside on why this bias term is missing.\n\nMost other libraries store W_O as a 2D tensor of shape [num_heads * d_head, d_model]. In this case, the sum over heads is implicit in our calculations when we apply the matrix W_O. We then add b_O, which is a vector of length d_model.\nTransformerLens stores W_O as a 3D tensor of shape [num_heads, d_head, d_model] so that we can easily compute the output of each head separately. Since TransformerLens is designed to be compatible with other libraries, we need the bias to also be shape d_model, which means we have to sum over heads before we add the bias term. So none of the output terms for our individual heads will include the bias term.\nIn practice this doesn’t matter here, since the bias term is the same for balanced and unbalanced brackets. When doing attribution, for each of our components, we only care about the component in the unbalanced direction of the vector they write to the residual stream for balanced vs unbalanced sequences - the bias is the same on all inputs.\n\n\ndef get_out_by_components(model: HookedTransformer, data: BracketsDataset) -&gt; Float[Tensor, \"component batch seq_pos emb\"]:\n    '''\n    Computes a tensor of shape [10, dataset_size, seq_pos, emb] representing the output of the model's components when run on the data.\n    The first dimension is  [embeddings, head 0.0, head 0.1, mlp 0, head 1.0, head 1.1, mlp 1, head 2.0, head 2.1, mlp 2]\n    '''\n    # SOLUTION\n    embedding_hook_names = [\"hook_embed\", \"hook_pos_embed\"]\n    head_hook_names = [utils.get_act_name(\"result\", layer) for layer in range(model.cfg.n_layers)]\n    mlp_hook_names = [utils.get_act_name(\"mlp_out\", layer) for layer in range(model.cfg.n_layers)]\n\n    all_hook_names = embedding_hook_names + head_hook_names + mlp_hook_names\n    activations = get_activations(model, data.toks, all_hook_names)\n\n    out = (activations[\"hook_embed\"] + activations[\"hook_pos_embed\"]).unsqueeze(0)\n\n    for head_hook_name, mlp_hook_name in zip(head_hook_names, mlp_hook_names):\n        out = t.concat([\n            out,\n            einops.rearrange(activations[head_hook_name], \"batch seq heads emb -&gt; heads batch seq emb\"),\n            activations[mlp_hook_name].unsqueeze(0)\n        ])\n\n    return out\n\n\ntests.test_get_out_by_components(get_out_by_components, model, data_mini)\n\nAll tests in `test_get_out_by_components` passed!\n\n\nNow, you can test your function by confirming that input to the final layer norm is the sum of the output of each component and the output projection biases.\n\nbiases = model.b_O.sum(0)\nout_by_components = get_out_by_components(model, data)\nsummed_terms = out_by_components.sum(dim=0) + biases\n\nfinal_ln_input_name, final_ln_output_name = LN_hook_names(model.ln_final)\nfinal_ln_input = get_activation(model, data.toks, final_ln_input_name)\n\nt.testing.assert_close(summed_terms, final_ln_input)\nprint(\"Tests passed!\")\n\nTests passed!\n\n\n\n\nHint\n\nStart by getting all the activation names in a list. You will need utils.get_act_name(\"result\", layer) to get the activation names for the attention heads’ output, and utils.get_act_name(\"mlp_out\", layer) to get the activation names for the MLPs’ output.\nOnce you’ve done this, and run the get_activations function, it’s just a matter of doing some reshaping and stacking. Your embedding and mlp activations will have shape (batch, seq_pos, d_model), while your attention activations will have shape (batch, seq_pos, head_idx, d_model).\n\n\n\nWhich components matter?\nTo figure out which components are directly important for the the model’s output being “unbalanced”, we can see which components tend to output a vector to the position-0 residual stream with higher dot product in the unbalanced direction for actually unbalanced inputs.\nThe idea is that, if a component is important for correctly classifying unbalanced inputs, then its vector output when fed unbalanced bracket strings will have a higher dot product in the unbalanced direction than when it is fed balanced bracket strings.\nIn this section, we’ll plot histograms of the dot product for each component. This will allow us to observe which components are significant.\nFor example, suppose that one of our components produced bimodal output like this:\n\nThis would be strong evidence that this component is important for the model’s output being unbalanced, since it’s pushing the unbalanced bracket inputs further in the unbalanced direction (i.e. the direction which ends up contributing to the inputs being classified as unbalanced) relative to the balanced inputs.\n\n\nExercise - compute output in unbalanced direction for each component\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵🔵🔵⚪\n\nYou shouldn't spend more than 10-15 minutes on this exercise.\n\nIt's very important to conceptually understand what object you are computing here. The actual computation is just a few lines of code involving indexing and einsums.\nIn the code block below, you should compute a (10, batch)-size tensor called out_by_component_in_unbalanced_dir. The [i, j]th element of this tensor should be the dot product of the ith component’s output with the unbalanced direction, for the jth sequence in your dataset.\nYou should normalize it by subtracting the mean of the dot product of this component’s output with the unbalanced direction on balanced samples - this will make sure the histogram corresponding to the balanced samples is centered at 0 (like in the figure above), which will make it easier to interpret. Remember, it’s only the difference between the dot product on unbalanced and balanced samples that we care about (since adding a constant to both logits doesn’t change the model’s probabilistic output).\nWe’ve given you a hists_per_comp function which will plot these histograms for you - all you need to do is calculate the out_by_component_in_unbalanced_dir object and supply it to that function.\n\n# YOUR CODE HERE - define the object `out_by_component_in_unbalanced_dir`\n# Get output by components, at sequence position 0 (which is used for classification)\nout_by_components_seq0 = out_by_components[:, :, 0, :] # [component=10 batch d_model]\n# Get the unbalanced direction for tensors being fed into the final layernorm\npre_final_ln_dir = get_pre_final_ln_dir(model, data) # [d_model]\n# Get the size of the contributions for each component\nout_by_component_in_unbalanced_dir = einops.einsum(\n    out_by_components_seq0,\n    pre_final_ln_dir,\n    \"comp batch d_model, d_model -&gt; comp batch\",\n)\n# Subtract the mean\nout_by_component_in_unbalanced_dir -= out_by_component_in_unbalanced_dir[:, data.isbal].mean(dim=1).unsqueeze(1)\n\n\ntests.test_out_by_component_in_unbalanced_dir(out_by_component_in_unbalanced_dir, model, data)\n\nplotly_utils.hists_per_comp(\n    out_by_component_in_unbalanced_dir,\n    data, xaxis_range=[-10, 20]\n)\n\nAll tests in `test_out_by_component_in_unbalanced_dir` passed!\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nHint\n\nStart by defining these two objects:\n\nThe output by components at sequence position zero, i.e. a tensor of shape (component, batch, d_model)\nThe pre_final_ln_dir vector, which has length d_model\n\nThen create magnitudes by calculating an appropriate dot product.\nDon’t forget to subtract the mean for each component across all the balanced samples (you can use the boolean data.isbal as your index).\n\n\n\nWhich heads do you think are the most important, and can you guess why that might be?\n\nThe heads in layer 2 (i.e. 2.0 and 2.1) seem to be the most important, because the unbalanced brackets are being pushed much further to the right than the balanced brackets.\nWe might guess that some kind of composition is going on here. The outputs of layer 0 heads can’t be involved in composition because they in effect work like a one-layer transformer. But the later layers can participate in composition, because their inputs come from not just the embeddings, but also the outputs of the previous layer. This means they can perform more complex computations.\n\n\n\nHead influence by type of failures\nThose histograms showed us which heads were important, but it doesn’t tell us what these heads are doing, however. In order to get some indication of that, let’s focus in on the two heads in layer 2 and see how much they write in our chosen direction on different types of inputs. In particular, we can classify inputs by if they pass the ‘overall elevation’ and ‘nowhere negative’ tests.\nWe’ll also ignore sentences that start with a close paren, as the behaviour is somewhat different on them (they can be classified as unbalanced immediately, so they don’t require more complicated logic).\n\n\nExercise - classify bracket strings by failure type\nDifficulty: 🔴🔴🔴⚪⚪\nImportance: 🔵🔵⚪⚪⚪\n\nYou shouldn't spend more than 15-20 minutes on this exercise.\n\nThese exercises should be pretty straightforward; you'll be able to use much of your code from previous exercises. They are also quite fiddly, so you should look at the solutions if you are stuck.\nDefine, so that the plotting works, the following objects:\n\nnegative_failure\n\nThis is an (N_SAMPLES,) boolean vector that is true for sequences whose elevation (when reading from right to left) ever dips negative, i.e. there’s an open paren that is never closed. |\n\ntotal_elevation_failure\n\nThis is an (N_SAMPLES,) boolean vector that is true for sequences whose total elevation is not exactly 0. In other words, for sentences with uneven numbers of open and close parens. |\n\nh20_in_unbalanced_dir\n\nThis is an (N_SAMPLES,) float vector equal to head 2.0’s contribution to the position-0 residual stream in the unbalanced direction, normalized by subtracting its average unbalancedness contribution to this stream over balanced sequences. |\n\nh21_in_unbalanced_dir\n\nSame as above but head 2.1 |\n\n\nFor the first two of these, you will find it helpful to refer back to your is_balanced_vectorized code (although remember you’re reading right to left here - this will change your results!).\nYou can get the last two of these by directly indexing from your out_by_component_in_unbalanced_dir tensor.\n\ndef is_balanced_vectorized_return_both(\n    toks: Float[Tensor, \"batch seq\"]\n) -&gt; tuple[Bool[Tensor, \"batch\"], Bool[Tensor, \"batch\"]]:\n\n    # SOLUTION\n    table = t.tensor([0, 0, 0, 1, -1]).to(device)\n    change = table[toks.to(device)].flip(-1)\n    altitude = t.cumsum(change, -1)\n    total_elevation_failure = altitude[:, -1] != 0\n    negative_failure = altitude.max(-1).values &gt; 0\n    return total_elevation_failure, negative_failure\n\n\ntotal_elevation_failure, negative_failure = is_balanced_vectorized_return_both(data.toks)\n\nh20_in_unbalanced_dir = out_by_component_in_unbalanced_dir[7]\nh21_in_unbalanced_dir = out_by_component_in_unbalanced_dir[8]\n\ntests.test_total_elevation_and_negative_failures(data, total_elevation_failure, negative_failure)\n\nAll tests in `test_total_elevation_and_negative_failures` passed!\n\n\nOnce you’ve passed the tests, you can run the code below to generate your plot.\n\nfailure_types_dict = {\n    \"both failures\": negative_failure & total_elevation_failure,\n    \"just neg failure\": negative_failure & ~total_elevation_failure,\n    \"just total elevation failure\": ~negative_failure & total_elevation_failure,\n    \"balanced\": ~negative_failure & ~total_elevation_failure\n}\n\nplotly_utils.plot_failure_types_scatter(\n    h20_in_unbalanced_dir,\n    h21_in_unbalanced_dir,\n    failure_types_dict,\n    data\n)\n\n\n\n\n                                \n                                            \n\n\n\n\nLook at the graph and think about what the roles of the different heads are!\n\n\nRead after thinking for yourself\n\nThe primary thing to take away is that 2.0 is responsible for checking the overall counts of open and close parentheses, and that 2.1 is responsible for making sure that the elevation never goes negative.\nAside: the actual story is a bit more complicated than that. Both heads will often pick up on failures that are not their responsibility, and output in the ‘unbalanced’ direction. This is in fact incentived by log-loss: the loss is slightly lower if both heads unanimously output ‘unbalanced’ on unbalanced sequences rather than if only the head ‘responsible’ for it does so. The heads in layer one do some logic that helps with this, although we’ll not cover it today.\nOne way to think of it is that the heads specialized on being very reliable on their class of failures, and then sometimes will sucessfully pick up on the other type.\n\nIn most of the rest of these exercises, we’ll focus on the overall elevation circuit as implemented by head 2.0. As an additional way to get intuition about what head 2.0 is doing, let’s graph its output against the overall proportion of the sequence that is an open-paren.\n\nplotly_utils.plot_contribution_vs_open_proportion(\n    h20_in_unbalanced_dir,\n    \"Head 2.0 contribution vs proportion of open brackets '('\",\n    failure_types_dict,\n    data\n)\n\n\n\n\n                                \n                                            \n\n\n\n\nYou can also compare this to head 2.1:\n\nplotly_utils.plot_contribution_vs_open_proportion(\n    h21_in_unbalanced_dir,\n    \"Head 2.1 contribution vs proportion of open brackets '('\",\n    failure_types_dict,\n    data\n)"
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#attention-pattern-of-the-responsible-head",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#attention-pattern-of-the-responsible-head",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Attention pattern of the responsible head",
    "text": "Attention pattern of the responsible head\nWhich tokens is 2.0 paying attention to when the query is an open paren at token 0? Recall that we focus on sequences that start with an open paren because sequences that don’t can be ruled out immediately, so more sophisticated behavior is unnecessary.\n\nExercise - get attention probabilities\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵⚪⚪⚪\n\nYou shouldn't spend more than 5-10 minutes on this exercise.\n\nThis exercise just involves the `get_activations` helper func, and some indexing.\nWrite a function that extracts the attention patterns for a given head when run on a batch of inputs.\n\ndef get_attn_probs(model: HookedTransformer, data: BracketsDataset, layer: int, head: int) -&gt; t.Tensor:\n    '''\n    Returns: (N_SAMPLES, max_seq_len, max_seq_len) tensor that sums to 1 over the last dimension.\n    '''\n    # SOLUTION\n    return get_activation(model, data.toks, utils.get_act_name(\"pattern\", layer))[:, head, :, :]\n\n\ntests.test_get_attn_probs(get_attn_probs, model, data_mini)\n\nAll tests in `test_get_attn_probs` passed!\n\n\nOnce you’ve passed the tests, you can plot your results:\n\nattn_probs_20 = get_attn_probs(model, data, 2, 0) # [batch seqQ seqK]\nattn_probs_20_open_query0 = attn_probs_20[data.starts_open].mean(0)[0]\n\nbar(\n    attn_probs_20_open_query0,\n    title=\"Avg Attention Probabilities for query 0, first token '(', head 2.0\",\n    width=700, template=\"simple_white\"\n)\n\n\n\n\n                                \n                                            \n\n\n\n\nYou should see an average attention of around 0.5 on position 1, and an average of about 0 for all other tokens. So 2.0 is just moving information from residual stream 1 to residual stream 0. In other words, 2.0 passes residual stream 1 through its W_OV circuit (after LayerNorming, of course), weighted by some amount which we’ll pretend is constant. Importantly, this means that the necessary information for classification must already have been stored in sequence position 1 before this head. The plot thickens!\n\n\nIdentifying meaningful direction before this head\nIf we make the simplification that the vector moved to sequence position 0 by head 2.0 is just layernorm(x[1]) @ W_OV (where x[1] is the vector in the residual stream before head 2.0, at sequence position 1), then we can do the same kind of logit attribution we did before. Rather than decomposing the input to the final layernorm (at sequence position 0) into the sum of ten components and measuring their contribution in the “pre final layernorm unbalanced direction”, we can decompose the input to head 2.0 (at sequence position 1) into the sum of the seven components before head 2.0, and measure their contribution in the “pre head 2.0 unbalanced direction”.\nHere is an annotated diagram to help better explain exactly what we’re doing.\n\n\nExercise - calculate the pre-head 2.0 unbalanced direction\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵🔵⚪⚪\n\nYou shouldn't spend more than 15-20 minutes on these exercises.\n\nThe second function should be conceptually similar to `get_pre_final_ln_dir` from earlier.\nBelow, you’ll be asked to calculate this pre_20_dir, which is the unbalanced direction for inputs into head 2.0 at sequence position 1 (based on the fact that vectors at this sequence position are copied to position 0 by head 2.0, and then used in prediction).\nFirst, you’ll implement the function get_WOV, to get the OV matrix for a particular layer and head. Recall that this is the product of the W_O and W_V matrices. Then, you’ll use this function to write get_pre_20_dir.\n\ndef get_WOV(model: HookedTransformer, layer: int, head: int) -&gt; Float[Tensor, \"d_model d_model\"]:\n    '''\n    Returns the W_OV matrix for a particular layer and head.\n    '''\n    # SOLUTION\n    return model.W_V[layer, head] @ model.W_O[layer, head]\n\n\ndef get_pre_20_dir(model, data) -&gt; Float[Tensor, \"d_model\"]:\n    '''\n    Returns the direction propagated back through the OV matrix of 2.0\n    and then through the layernorm before the layer 2 attention heads.\n    '''\n    # SOLUTION\n    W_OV = get_WOV(model, 2, 0)\n\n    layer2_ln_fit, r2 = get_ln_fit(model, data, layernorm=model.blocks[2].ln1, seq_pos=1)\n    layer2_ln_coefs = t.from_numpy(layer2_ln_fit.coef_).to(device)\n\n    pre_final_ln_dir = get_pre_final_ln_dir(model, data)\n\n    return layer2_ln_coefs.T @ W_OV @ pre_final_ln_dir\n\n\ntests.test_get_pre_20_dir(get_pre_20_dir, model, data_mini)\n\nAll tests in `test_get_pre_20_dir` passed!\n\n\n\n\n\nExercise - compute component magnitudes\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵🔵⚪⚪\n\nYou shouldn't spend more than 10-15 minutes on these exercises.\n\nThis exercise should be somewhat similar to the last time you computed component magnitudes.\nNow that you’ve got the pre_20_dir, you can calculate magnitudes for each of the components that came before. You can refer back to the diagram above if you’re confused. Remember to subtract the mean for each component for balanced inputs.\n\n# YOUR CODE HERE - define `out_by_component_in_pre_20_unbalanced_dir` (for all components before head 2.0)\npre_layer2_outputs_seqpos1 = out_by_components[:-3, :, 1, :]\nout_by_component_in_pre_20_unbalanced_dir = einops.einsum(\n    pre_layer2_outputs_seqpos1,\n    get_pre_20_dir(model, data),\n    \"comp batch emb, emb -&gt; comp batch\",\n)\nout_by_component_in_pre_20_unbalanced_dir -= out_by_component_in_pre_20_unbalanced_dir[:, data.isbal].mean(-1, keepdim=True)\n\n\ntests.test_out_by_component_in_pre_20_unbalanced_dir(out_by_component_in_pre_20_unbalanced_dir, model, data)\n\nplotly_utils.hists_per_comp(\n    out_by_component_in_pre_20_unbalanced_dir,\n    data, xaxis_range=(-5, 12)\n)\n\nAll tests in `test_out_by_component_in_pre_20_unbalanced_dir` passed!\n\n\n\n\n\n                                \n                                            \n\n\n\n\nWhat do you observe?\n\n\nSome things to notice\n\nOne obvious note - the embeddings graph shows an output of zero, in other words no effect on the classification. This is because the input for this path is just the embedding vector in the 0th sequence position - in other words the [START] token’s embedding, which is the same for all inputs.\n\nMore interestingly, we can see that mlp0 and especially mlp1 are very important. This makes sense – one thing that mlps are especially capable of doing is turning more continuous features (‘what proportion of characters in this input are open parens?’) into sharp discontinuous features (‘is that proportion exactly 0.5?’).\nFor example, the sum \\(\\operatorname{ReLU}(x-0.5) + \\operatorname{ReLU}(0.5-x)\\) evaluates to the nonlinear function \\(|x-0.5|\\), which is zero if and only if \\(x=0.5\\). This is one way our model might be able to classify all bracket strings as unbalanced unless they had exactly 50% open parens.\n\n\nHead 1.1 also has some importance, although we will not be able to dig into this today. It turns out that one of the main things it does is incorporate information about when there is a negative elevation failure into this overall elevation branch. This allows the heads to agree the prompt is unbalanced when it is obviously so, even if the overall count of opens and closes would allow it to be balanced.\n\nIn order to get a better look at what mlp0 and mlp1 are doing more thoughly, we can look at their output as a function of the overall open-proportion.\n\nplotly_utils.mlp_attribution_scatter(out_by_component_in_pre_20_unbalanced_dir, data, failure_types_dict)\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nMLPs as key-value pairs\nWhen we implemented transformers from scratch, we observed that MLPs can be thought of as key-value pairs. To recap this briefly:\n\nWe can write the MLP’s output as \\(f(x^T W^{in})W^{out}\\), where \\(W^{in}\\) and \\(W^{out}\\) are the different weights of the MLP (ignoring biases), \\(f\\) is the activation function, and \\(x\\) is a vector in the residual stream. This can be rewritten as:\n\\[\nf(x^T W^{in}) W^{out} = \\sum_{i=1}^{d_{mlp}} f(x^T W^{in}_{[:, i]}) W^{out}_{[i, :]}\n\\]\nWe can view the vectors \\(W^{in}_{[:, i]}\\) as the input directions, and \\(W^{out}_{[i, :]}\\) as the output directions. We say the input directions are activated by certain textual features, and when they are activated, vectors are written in the corresponding output direction. This is very similar to the concept of keys and values in attention layers, which is why these vectors are also sometimes called keys and values (e.g. see the paper Transformer Feed-Forward Layers Are Key-Value Memories).\n\nIncluding biases, the full version of this formula is:\n\\[\nMLP(x) = \\sum_{i=1}^{d_{mlp}}f(x^T W^{in}_{[:, i]} + b^{in}_i) W^{out}_{[i,:]} + b^{out}\n\\]\nDiagram illustrating this (without biases):\n\n\n\nExercise - get output by neuron\nDifficulty: 🔴🔴🔴🔴⚪\nImportance: 🔵🔵🔵🔵⚪\n\nYou shouldn't spend more than 25-35 minutes on these exercises.\n\nIt's important to understand exactly what the MLP is doing, and how to work with it.\nThe function get_out_by_neuron should return the given MLP’s output per neuron. In other words, the output has shape [batch, seq, neurons, d_model], where out[b, s, i] is the vector \\(f(\\vec x^T W^{in}_{[:,i]} + b^{in}_i)W^{out}_{[i,:]}\\) (and summing over i would give you the actual output of the MLP). We ignore \\(b^{out}\\) here, because it isn’t attributable to any specific neuron.\nWhen you have this output, you can use get_out_by_neuron_in_20_dir to calculate the output of each neuron in the unbalanced direction for the input to head 2.0 at sequence position 1. Note that we’re only considering sequence position 1, because we’ve observed that head 2.0 is mainly just copying info from position 1 to position 0. This is why we’ve given you the seq argument in the get_out_by_neuron function, so you don’t need to store more information than is necessary.\n\ndef get_out_by_neuron(\n    model: HookedTransformer,\n    data: BracketsDataset,\n    layer: int,\n    seq: int | None = None\n) -&gt; Float[Tensor, \"batch *seq neuron d_model\"]:\n    '''\n    If seq is None, then out[batch, seq, i, :] = f(x[batch, seq].T @ W_in[:, i] + b_in[i]) @ W_out[i, :],\n    i.e. the vector which is written to the residual stream by the ith neuron (where x is the input to the\n    residual stream (i.e. shape (batch, seq, d_model)).\n\n    If seq is not None, then out[batch, i, :] = f(x[batch, seq].T @ W_in[:, i]) @ W_out[i, :], i.e. we just\n    look at the sequence position given by argument seq.\n\n    (Note, using * in jaxtyping indicates an optional dimension)\n    '''\n    # SOLUTION\n    # Get the W_out matrix for this MLP\n    W_out = model.W_out[layer] # [neuron d_model]\n\n    # Get activations of the layer just after the activation function, i.e. this is f(x.T @ W_in)\n    f_x_W_in = get_activation(model, data.toks, utils.get_act_name('post', layer)) # [batch seq neuron]\n\n    # f_x_W_in are activations, so they have batch and seq dimensions - this is where we index by seq if necessary\n    if seq is not None:\n        f_x_W_in = f_x_W_in[:, seq, :] # [batch neuron]\n\n    # Calculate the output by neuron (i.e. so summing over the `neurons` dimension gives the output of the MLP)\n    out = einops.einsum(\n        f_x_W_in,\n        W_out,\n        \"... neuron, neuron d_model -&gt; ... neuron d_model\",\n    )\n    return out\n\n\ndef get_out_by_neuron_in_20_dir(model: HookedTransformer, data: BracketsDataset, layer: int) -&gt; Float[Tensor, \"batch neurons\"]:\n    '''\n    [b, s, i]th element is the contribution of the vector written by the ith neuron to the residual stream in the\n    unbalanced direction (for the b-th element in the batch, and the s-th sequence position).\n\n    In other words we need to take the vector produced by the `get_out_by_neuron` function, and project it onto the\n    unbalanced direction for head 2.0 (at seq pos = 1).\n    '''\n    # SOLUTION\n    # Get neuron output at sequence position 1\n    out_by_neuron_seqpos1 = get_out_by_neuron(model, data, layer, seq=1)\n\n\n    # For each neuron, project the vector it writes to residual stream along the pre-2.0 unbalanced direction\n    return einops.einsum(\n        out_by_neuron_seqpos1,\n        get_pre_20_dir(model, data),\n        \"batch neuron d_model, d_model -&gt; batch neuron\"\n    )\n\n\ntests.test_get_out_by_neuron(get_out_by_neuron, model, data_mini)\ntests.test_get_out_by_neuron_in_20_dir(get_out_by_neuron_in_20_dir, model, data_mini)\n\nAll tests in `test_get_out_by_neuron` passed!\nAll tests in `test_get_out_by_neuron_in_20_dir` passed!\n\n\n\n\nHint\n\nFor the get_out_by_neuron function, define \\(f(\\vec x^T W^{in}_{[:,i]} + b^{in}_i)\\) and \\(W^{out}_{[i,:]}\\) separately, then multiply them together. The former is the activation corresponding to the name \"post\", and you can access it using your get_activations function. The latter are just the model weights, and you can access it using model.W_out.\nAlso, remember to keep in mind the distinction between activations and parameters. \\(f(\\vec x^T W^{in}_{[:,i]} + b^{in}_i)\\) is an activation; it has a batch and seq_len dimension. \\(W^{out}_{[i,:]}\\) is a parameter; it has no batch or seq_len dimension.\n\n\n\nExercise - implement the same function, using less memory\nDifficulty: 🔴🔴🔴⚪⚪\nImportance: 🔵🔵⚪⚪⚪\n\nYou shouldn't spend more than 10-15 minutes on this exercise.\n\nUnderstanding the solution is more important than doing this exercise, so you should look at the solution rather than doing the exercise if you feel like it.\nThis exercise isn’t as important as the previous one, and you can skip it if you don’t find this interesting (although you’re still recommended to look at the solutions, so you understand what’s going on here.)\nIf the only thing we want from the MLPs are their contribution in the unbalanced direction, then we can actually do this without having to store the out_by_neuron_in_20_dir object. Try and find this method, and implement it below.\n\ndef get_out_by_neuron_in_20_dir_less_memory(model: HookedTransformer, data: BracketsDataset, layer: int) -&gt; Float[Tensor, \"batch neurons\"]:\n    '''\n    Has the same output as `get_out_by_neuron_in_20_dir`, but uses less memory (because it never stores\n    the output vector of each neuron individually).\n    '''\n    # SOLUTION\n    W_out = model.W_out[layer] # [neurons d_model]\n\n    f_x_W_in = get_activation(model, data.toks, utils.get_act_name('post', layer))[:, 1, :] # [batch neurons]\n\n    pre_20_dir = get_pre_20_dir(model, data) # [d_model]\n\n    # Multiply along the d_model dimension\n    W_out_in_20_dir = W_out @ pre_20_dir # [neurons]\n    # Multiply elementwise, over neurons (we're broadcasting along the batch dim)\n    out_by_neuron_in_20_dir = f_x_W_in * W_out_in_20_dir # [batch neurons]\n\n    return out_by_neuron_in_20_dir\n\n\ntests.test_get_out_by_neuron_in_20_dir_less_memory(get_out_by_neuron_in_20_dir_less_memory, model, data_mini)\n\nAll tests in `test_get_out_by_neuron_in_20_dir_less_memory` passed!\n\n\n\n\nHint\n\nThe key is to change the order of operations.\nFirst, project each of the output directions onto the pre-2.0 unbalanced direction in order to get their components (i.e. a vector of length d_mlp, where the i-th element is the component of the vector \\(W^{out}_{[i,:]}\\) in the unbalanced direction). Then, scale these contributions by the activations \\(f(\\vec x^T W^{in}_{[:,i]} + b^{in}_i)\\).bold text\n\n\n\nInterpreting the neurons\nNow, try to identify several individual neurons that are especially important to 2.0.\nFor instance, you can do this by seeing which neurons have the largest difference between how much they write in our chosen direction on balanced and unbalanced sequences (especially unbalanced sequences beginning with an open paren).\nUse the plot_neurons function to get a sense of what an individual neuron does on differen open-proportions.\nOne note: now that we are deep in the internals of the network, our assumption that a single direction captures most of the meaningful things going on in this overall-elevation circuit is highly questionable. This is especially true for using our 2.0 direction to analyize the output of mlp0, as one of the main ways this mlp has influence is through more indirect paths (such as mlp0 -&gt; mlp1 -&gt; 2.0) which are not the ones we chose our direction to capture. Thus, it is good to be aware that the intuitions you get about what different layers or neurons are doing are likely to be incomplete.\nNote - we’ve supplied the default argument renderer=\"browser\", which causes the plots to open in a browser rather than in VSCode. This often works better, with less lag (especially in notebooks), but you can remove this if you prefer.\n\nfor layer in range(2):\n    # Get neuron significances for head 2.0, sequence position #1 output\n    neurons_in_unbalanced_dir = get_out_by_neuron_in_20_dir_less_memory(model, data, layer)[utils.to_numpy(data.starts_open), :]\n    # Plot neurons' activations\n    plotly_utils.plot_neurons(neurons_in_unbalanced_dir, model, data, failure_types_dict, layer)\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nSome observations:\n\nThe important neurons in layer 1 can be put into three broad categories:\n\nSome neurons detect when the open-proportion is greater than 1/2. As a few examples, look at neurons 1.53, 1.39, 1.8 in layer 1. There are some in layer 0 as well, such as 0.33 or 0.43. Overall these seem more common in Layer 1.\nSome neurons detect when the open-proportion is less than 1/2. For instance, neurons 0.21, and 0.7. These are much more rare in layer 1, but you can see some such as 1.50 and 1.6.\nThe network could just use these two types of neurons, and compose them to measure if the open-proportion exactly equals 1/2 by adding them together. But we also see in layer 1 that there are many neurons that output this composed property. As a few examples, look at 1.10 and 1.3.\n\nIt’s much harder for a single neuron in layer 0 to do this by themselves, given that ReLU is monotonic and it requires the output to be a non-monotonic function of the open-paren proportion. It is possible, however, to take advantage of the layernorm before mlp0 to approximate this – 0.19 and 0.34 are good examples of this.\n\n\nNote, there are some neurons which appear to work in the opposite direction (e.g. 0.0). It’s unclear exactly what the function of these neurons is (especially since we’re only analysing one particular part of one of our model’s circuits, so our intuitions about what a particular neuron does might be incomplete). However, what is clear and unambiguous from this plot is that our neurons seem to be detecting the open proportion of brackets, and responding differently if the proportion is strictly more / strictly less than 1/2. And we can see that a large number of these seem to have their main impact via being copied in head 2.0.\n\nBelow: plots of neurons 0.21 and 1.53. You can observe the patterns described above."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#understanding-how-the-open-proportion-is-calculated---head-0.0",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#understanding-how-the-open-proportion-is-calculated---head-0.0",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Understanding how the open-proportion is calculated - Head 0.0",
    "text": "Understanding how the open-proportion is calculated - Head 0.0\nUp to this point we’ve been working backwards from the logits and through the internals of the network. We’ll now change tactics somewhat, and start working from the input embeddings forwards. In particular, we want to understand how the network calcuates the open-proportion of the sequence in the first place!\nThe key will end up being head 0.0. Let’s start by examining its attention pattern.\n\n0.0 Attention Pattern\nWe want to play around with the attention patterns in our heads. For instance, we’d like to ask questions like “what do the attention patterns look like when the queries are always left-parens?”. To do this, we’ll write a function that takes in a parens string, and returns the q and k vectors (i.e. the values which we take the inner product of to get the attention scores).\n\n\nExercise - extracting queries and keys using hooks\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵🔵⚪⚪⚪\n\nYou shouldn't spend more than ~15 minutes on this exercise.\n\nAgain, this exercise just involves using your `get_activations` function.\n\ndef get_q_and_k_for_given_input(\n    model: HookedTransformer,\n    tokenizer: SimpleTokenizer,\n    parens: str,\n    layer: int,\n) -&gt; tuple[Float[Tensor, \"seq n_heads d_model\"], Float[Tensor,  \"seq n_heads d_model\"]]:\n    '''\n    Returns the queries and keys for the given parens string, for all attention heads in the given layer.\n    '''\n    # SOLUTION\n    q_name = utils.get_act_name(\"q\", layer)\n    k_name = utils.get_act_name(\"k\", layer)\n\n    activations = get_activations(\n        model,\n        tokenizer.tokenize(parens),\n        [q_name, k_name]\n    )\n\n    return activations[q_name][0], activations[k_name][0]\n\n\ntests.test_get_q_and_k_for_given_input(get_q_and_k_for_given_input, model, tokenizer)\n\nAll tests in `test_get_q_and_k_for_given_input` passed!\n\n\n\n\nActivation Patching\nNow, we’ll introduce the valuable tool of activation patching. This was first introduced in David Bau and Kevin Meng’s excellent ROME paper, there called causal tracing.\nThe setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then intervene on a specific activation and patch in the corresponding activation from the clean run (i.e. replace the corrupted activation with the clean activation), and then continue the run.\nOne of the common use-cases for activation patching is to compare the model’s performance in clean vs patched runs. If the performance degrades with patching, this is a strong signal that the place you patched in is important for the model’s computation. The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what’s going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent.\nHowever, here our path patching serves a much simpler purpose - we’ll be patching at the query vectors of head 0.0 with values from a sequence of all left-parens, and at the key vectors with the average values from all left and all right parens. This allows us to get a sense for the average attention patterns paid by left-brackets to the rest of the sequence.\nWe’ll write functions to do this for both heads in layer 0, because it will be informative to compare the two.\n\nlayer = 0\nall_left_parens = \"\".join([\"(\" * 40])\nall_right_parens = \"\".join([\")\" * 40])\n\nmodel.reset_hooks()\nq0_all_left, k0_all_left = get_q_and_k_for_given_input(model, tokenizer, all_left_parens, layer)\nq0_all_right, k0_all_right = get_q_and_k_for_given_input(model, tokenizer, all_right_parens, layer)\nk0_avg = (k0_all_left + k0_all_right) / 2\n\n\n# Define hook function to patch in q or k vectors\ndef hook_fn_patch_qk(\n    value: Float[Tensor, \"batch seq head d_head\"],\n    hook: HookPoint,\n    new_value: Float[Tensor, \"... seq d_head\"],\n    head_idx: int | None = None\n) -&gt; None:\n    if head_idx is not None:\n        value[..., head_idx, :] = new_value[..., head_idx, :]\n    else:\n        value[...] = new_value[...]\n\n\n# Define hook function to display attention patterns (using plotly)\ndef hook_fn_display_attn_patterns(\n    pattern: Float[Tensor, \"batch heads seqQ seqK\"],\n    hook: HookPoint,\n    head_idx: int = 0\n) -&gt; None:\n    avg_head_attn_pattern = pattern.mean(0)\n    labels = [\"[start]\", *[f\"{i+1}\" for i in range(40)], \"[end]\"]\n    display(cv.attention.attention_heads(\n        tokens=labels,\n        attention=avg_head_attn_pattern,\n        attention_head_names=[\"0.0\", \"0.1\"],\n        max_value=avg_head_attn_pattern.max(),\n        mask_upper_tri=False, # use for bidirectional models\n    ))\n\n\n# Run our model on left parens, but patch in the average key values for left vs right parens\n# This is to give us a rough idea how the model behaves on average when the query is a left paren\nmodel.run_with_hooks(\n    tokenizer.tokenize(all_left_parens).to(device),\n    return_type=None,\n    fwd_hooks=[\n        (utils.get_act_name(\"k\", layer), partial(hook_fn_patch_qk, new_value=k0_avg)),\n        (utils.get_act_name(\"pattern\", layer), hook_fn_display_attn_patterns),\n    ]\n)\n\n\n    \n\n\n\n\nQuestion - what are the noteworthy features of head 0.0 in this plot?\n\nThe most noteworthy feature is the diagonal pattern - most query tokens pay almost zero attention to all the tokens that come before it, but much greater attention to those that come after it. For most query token positions, this attention paid to tokens after itself is roughly uniform. However, there are a few patches (especially for later query positions) where the attention paid to tokens after itself is not uniform. We will see that these patches are important for generating adversarial examples.\nWe can also observe roughly the same pattern when the query is a right paren (try running the last bit of code above, but using all_right_parens instead of all_left_parens), although the pattern is less pronounced.\n\nWe are most interested in the attention pattern at query position 1, because this is the position we move information to that is eventually fed into attention head 2.0, then moved to position 0 and used for prediction.\n(Note - we’ve chosen to focus on the scenario when the first paren is an open paren, because the model actually deals with bracket strings that open with a right paren slightly differently - these are obviously unbalanced, so a complicated mechanism is unnecessary.)\nLet’s plot a bar chart of the attention probability paid by the the open-paren query at position 1 to all the other positions. Here, rather than patching in both the key and query from artificial sequences, we’re running the model on our entire dataset and patching in an artificial value for just the query (all open parens). Both methods are reasonable here, since we’re just looking for a general sense of how our query vector at position 1 behaves when it’s an open paren.\n\ndef hook_fn_display_attn_patterns_for_single_query(\n    pattern: Float[Tensor, \"batch heads seqQ seqK\"],\n    hook: HookPoint,\n    head_idx: int = 0,\n    query_idx: int = 1\n):\n    bar(\n        utils.to_numpy(pattern[:, head_idx, query_idx].mean(0)),\n        title=f\"Average attn probabilities on data at posn 1, with query token = '('\",\n        labels={\"index\": \"Sequence position of key\", \"value\": \"Average attn over dataset\"},\n        height=500, width=800, yaxis_range=[0, 0.1], template=\"simple_white\"\n    )\n\ndata_len_40 = BracketsDataset.with_length(data_tuples, 40).to(device)\n\nmodel.reset_hooks()\nmodel.run_with_hooks(\n    data_len_40.toks[data_len_40.isbal],\n    return_type=None,\n    fwd_hooks=[\n        (utils.get_act_name(\"q\", 0), partial(hook_fn_patch_qk, new_value=q0_all_left)),\n        (utils.get_act_name(\"pattern\", 0), hook_fn_display_attn_patterns_for_single_query),\n    ]\n)\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nQuestion - what is the interpretation of this attention pattern?\n\nThis shows that the attention pattern is almost exactly uniform over all tokens. This means the vector written to sequence position 1 will be approximately some scalar multiple of the vectors at each source position, transformerd via the matrix \\(W_{OV}^{0.0}\\).\n\n\n\nProposing a hypothesis\nBefore we connect all the pieces together, let’s list the facts that we know about our model so far (going chronologically from our observations):\n\n\nAttention head 2.0 seems to be largely responsible for classifying brackets as unbalanced when they have non-zero net elevation (i.e. have a different number of left and right parens).\nAttention head 2.0 attends strongly to the sequence position \\(i=1\\), in other words it’s pretty much just moving the residual stream vector from position 1 to position 0 (and applying matrix \\(W_{OV}\\)).\nSo there must be earlier components of the model which write to sequence position 1, in a way which influences the model to make correct classifications (via the path through head 2.0).\nThere are several neurons in MLP0 and MLP1 which seem to calculate a nonlinear function of the open parens proportion - some of them are strongly activating when the proportion is strictly greater than \\(1/2\\), others when it is strictly smaller than \\(1/2\\).\nIf the query token in attention head 0.0 is an open paren, then it attends to all key positions after \\(i\\) with roughly equal magnitude.\nIn particular, this holds for the sequence position \\(i=1\\), which attends approximately uniformly to all sequence positions.\n\n\nBased on all this, can you formulate a hypothesis for how the elevation circuit works, which ties all three of these observations together?\n\n\nHypothesis\n\nThe hypothesis might go something like this:\n\nIn the attention calculation for head 0.0, the position-1 query token is doing some kind of aggregation over brackets. It writes to the residual stream information representing the difference between the number of left and right brackets - in other words, the net elevation. &gt; Remember that one-layer attention heads can pretty much only do skip-trigrams, e.g. of the form keep ... in -&gt; mind. They can’t capture three-way interactions flexibly, in other words they can’t compute functions like “whether the number of left and right brackets is equal”. (To make this clearer, consider how your model’s behaviour would differ on the inputs (), (( and )) if it was just one-layer). So aggregation over left and right brackets is pretty much all we can do.\nNow that sequence position 1 contains information about the elevation, the MLP reads this information, and some of its neurons perform nonlinear operations to give us a vector which conatains “boolean” information about whether the number of left and right brackets is equal. &gt; Recall that MLPs are great at taking linear functions (like the difference between number of left and right brackets) and converting it to boolean information. We saw something like this was happening in our plots above, since most of the MLPs’ neurons’ behaviour was markedly different above or below the threshold of 50% left brackets.\nFinally, now that the 1st sequence position in the residual stream stores boolean information about whether the net elevation is zero, this information is read by head 2.0, and the output of this head is used to classify the sequence as balanced or unbalanced. &gt; This is based on the fact that we already saw head 2.0 is strongly attending to the 1st sequence position, and that it seems to be implementing the elevation test.\n\n\nAt this point, we’ve pretty much empirically verified all the observations above. One thing we haven’t really proven yet is that (1) is working as we’ve described above. We want to verify that head 0.0 is calculating some kind of difference between the number of left and right brackets, and writing this information to the residual stream. In the next section, we’ll find a way to test this hypothesis.\n\n\nThe 0.0 OV circuit\nWe want to understand what the 0.0 head is writing to the residual stream. In particular, we are looking for evidence that it is writing information about the net elevation.\nWe’ve already seen that query position 1 is attending approximately uniformly to all key positions. This means that (ignoring start and end tokens) the vector written to position 1 is approximately:\n\\[\n\\begin{aligned}\nh(x) &\\approx \\frac{1}{n} \\sum_{i=1}^n \\left(\\left(L {\\color{orange}x}\\right)^T W_{OV}^{0.0}\\right)_i \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\color{orange}{x}_i^T L^T W_{OV}^{0.0} \\\\\n\\end{aligned}\n\\]\nwhere \\(L\\) is the linear approximation for the layernorm before the first attention layer, and \\(x\\) is the (seq_len, d_model)-size residual stream consisting of vectors \\(\\color{orange}{x}_i\\) for each sequence position \\(i\\).\nWe can write \\(\\color{orange}{x}_j = \\color{orange}{pos}_j + \\color{orange}{tok}_j\\), where \\(\\color{orange}{pos}_j\\) and \\(\\color{orange}{tok}_j\\) stand for the positional and token embeddings respectively. So this gives us:\n\\[\n\\begin{aligned}\nh(x) &\\approx \\frac{1}{n} \\left( \\sum_{i=1}^n \\color{orange}{pos}_i^T L^T W_{OV}^{0.0} + \\sum_{i=1}^n \\color{orange}{tok}_i^T L^T W_{OV}^{0.0}\\right) \\\\\n&= \\frac{1}{n} \\left( \\sum_{i=1}^n \\color{orange}{pos}_i^T L^T W_{OV}^{0.0} + n_L \\color{orange}{\\vec v_L} + n_R \\color{orange}{\\vec v_R}\\right)\n\\end{aligned}\n\\]\nwhere \\(n_L\\) and \\(n_R\\) are the number of left and right brackets respectively, and \\(\\color{orange}{\\vec v_L}, \\color{orange}{\\vec v_R}\\) are the images of the token embeddings for left and right parens respectively under the image of the layernorm and OV circuit:\n\\[\n\\begin{aligned}\n\\color{orange}{\\vec v_L} &= \\color{orange}{LeftParen}^T L^T W_{OV}^{0.0} \\\\\n\\color{orange}{\\vec v_R} &= \\color{orange}{RightParen}^T L^T W_{OV}^{0.0}\n\\end{aligned}\n\\]\nwhere \\(\\color{orange}{LeftParen}\\) and \\(\\color{orange}{RightParen}\\) are the token embeddings for left and right parens respectively.\nFinally, we have an ability to formulate a test for our hypothesis in terms of the expression above:\n\nIf head 0.0 is performing some kind of aggregation, then we should see that \\(\\color{orange}{\\vec v_L}\\) and \\(\\color{orange}{\\vec v_R}\\) are vectors pointing in opposite directions. In other words, head 0.0 writes some scalar multiple of vector \\(v\\) to the residual stream, and we can extract the information \\(n_L - n_R\\) by projecting in the direction of this vector. The MLP can then take this information and process it in a nonlinear way, writing information about whether the sequence is balanced to the residual stream.\n\n\n\nExercise - validate the hypothesis\nDifficulty: 🔴🔴🔴⚪⚪\nImportance: 🔵🔵⚪⚪⚪\n\nYou shouldn't spend more than 10-15 minutes on this exercise.\n\nIf you understand what the vectors represent, these exercises should be pretty straightforward.\nHere, you should show that the two vectors have cosine similarity close to -1, demonstrating that this head is “tallying” the open and close parens that come after it.\nYou can fill in the function embedding (to return the token embedding vector corresponding to a particular character, i.e. the vectors we’ve called \\(\\color{orange}{LeftParen}\\) and \\(\\color{orange}{RightParen}\\) above), which will help when computing these vectors.\n\ndef embedding(model: HookedTransformer, tokenizer: SimpleTokenizer, char: str) -&gt; Float[Tensor, \"d_model\"]:\n    assert char in (\"(\", \")\")\n    idx = tokenizer.t_to_i[char]\n    return model.W_E[idx]\n\n\n# YOUR CODE HERE - define v_L and v_R, as described above.\nW_OV = model.W_V[0, 0] @ model.W_O[0, 0]\n\nlayer0_ln_fit = get_ln_fit(model, data, layernorm=model.blocks[0].ln1, seq_pos=None)[0]\nlayer0_ln_coefs = t.from_numpy(layer0_ln_fit.coef_).to(device)\n\nv_L = embedding(model, tokenizer, \"(\") @ layer0_ln_coefs.T @ W_OV\nv_R = embedding(model, tokenizer, \")\") @ layer0_ln_coefs.T @ W_OV\n\n\nprint(\"Cosine similarity: \", t.cosine_similarity(v_L, v_R, dim=0).item())\n\nCosine similarity:  -0.997443437576294\n\n\n\n\nExtra technicality about the two vectors (optional)\n\nNote - we don’t actually require \\(\\color{orange}{\\vec v_L}\\) and \\(\\color{orange}{\\vec v_R}\\) to have the same magnitude for this idea to work. This is because, if we have \\({\\color{orange} \\vec v_L} \\approx - \\alpha {\\color{orange} \\vec v_R}\\) for some \\(\\alpha &gt; 0\\), then when projecting along the \\(\\color{orange}{\\vec v_L}\\) direction we will get \\(\\|{\\color{orange} \\vec v_L}\\| (n_L - \\alpha n_R) / n\\). This always equals \\(\\|{\\color{orange} \\vec v_L}\\| (1 - \\alpha) / 2\\) when the number of left and right brackets match, regardless of the sequence length. It doesn’t matter that this value isn’t zero; the MLPs’ neurons can still learn to detect when the vector’s component in this direction is more or less than this value by adding a bias term. The important thing is that (1) the two vectors are parallel and pointing in opposite directions, and (2) the projection in this direction for balanced sequences is always the same.\n\n\n\nExercise - cosine similarity of input directions (optional)\nDifficulty: 🔴🔴⚪⚪⚪\nImportance: 🔵⚪⚪⚪⚪\n\nYou shouldn't spend more than 10-15 minutes on this exercise.\nAnother way we can get evidence for this hypothesis - recall in our discussion of MLP neurons that \\(W^{in}_{[:,i]}\\) (the \\(i\\)th column of matrix \\(W^{in}\\), where \\(W^{in}\\) is the first linear layer of the MLP) is a vector representing the “in-direction” of the neuron. If these neurons are indeed measuring open/closed proportions in the way we think, then we should expect to see the vectors \\(v_R\\), \\(v_L\\) have high dot product with these vectors.\nInvestigate this by filling in the two functions below. cos_sim_with_MLP_weights returns the vector of cosine similarities between a vector and the columns of \\(W^{in}\\) for a given layer, and avg_squared_cos_sim returns the average squared cosine similarity between a vector \\(v\\) and a randomly chosen vector with the same size as \\(v\\) (we can choose this vector in any sensible way, e.g. sampling it from the iid normal distribution then normalizing it). You should find that the average squared cosine similarity per neuron between \\(v_R\\) and the in-directions for neurons in MLP0 and MLP1 is much higher than you would expect by chance.\n\ndef cos_sim_with_MLP_weights(model: HookedTransformer, v: Float[Tensor, \"d_model\"], layer: int) -&gt; Float[Tensor, \"d_mlp\"]:\n    '''\n    Returns a vector of length d_mlp, where the ith element is the cosine similarity between v and the\n    ith in-direction of the MLP in layer `layer`.\n\n    Recall that the in-direction of the MLPs are the columns of the W_in matrix.\n    '''\n    # SOLUTION\n    v_unit = v / v.norm()\n    W_in_unit = model.W_in[layer] / model.W_in[layer].norm(dim=0)\n\n    return einops.einsum(v_unit, W_in_unit, \"d_model, d_model d_mlp -&gt; d_mlp\")\n\n\ndef avg_squared_cos_sim(v: Float[Tensor, \"d_model\"], n_samples: int = 1000) -&gt; float:\n    '''\n    Returns the average (over n_samples) cosine similarity between v and another randomly chosen vector.\n\n    We can create random vectors from the standard N(0, I) distribution.\n    '''\n    # SOLUTION\n    v2 = t.randn(n_samples, v.shape[0]).to(device)\n    v2 /= v2.norm(dim=1, keepdim=True)\n\n    v1 = v / v.norm()\n\n    return (v1 * v2).pow(2).sum(1).mean().item()\n\n\nprint(\"Avg squared cosine similarity of v_R with ...\\n\")\n\ncos_sim_mlp0 = cos_sim_with_MLP_weights(model, v_R, 0)\nprint(f\"...MLP input directions in layer 0:  {cos_sim_mlp0.pow(2).mean():.6f}\")\n\ncos_sim_mlp1 = cos_sim_with_MLP_weights(model, v_R, 1)\nprint(f\"...MLP input directions in layer 1:  {cos_sim_mlp1.pow(2).mean():.6f}\")\n\ncos_sim_rand = avg_squared_cos_sim(v_R)\nprint(f\"...random vectors of len = d_model:  {cos_sim_rand:.6f}\")\n\nAvg squared cosine similarity of v_R with ...\n\n...MLP input directions in layer 0:  0.123855\n...MLP input directions in layer 1:  0.130125\n...random vectors of len = d_model:  0.017846\n\n\nAs an extra-bonus exercise, you can also compare the squared cosine similarities per neuron to your neuron contribution plots you made earlier (the ones with sliders). Do the neurons which have particularly high cosine similarity with \\(v_R\\) correspond to the neurons which write to the unbalanced direction of head 2.0 in a big way whenever the proportion of open parens is not 0.5? (This would provide further evidence that the main source of information about total open proportion of brackets which is used in the net elevation circuit is provided by the multiples of \\(v_R\\) and \\(v_L\\) written to the residual stream by head 0.0). You can go back to your old plots and check."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#summary-1",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#summary-1",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Summary",
    "text": "Summary\nGreat! Let’s stop and take stock of what we’ve learned about this circuit.\n\nHead 0.0 pays attention uniformly to the suffix following each token, tallying up the amount of open and close parens that it sees and writing that value to the residual stream. This means that it writes a vector representing the total elevation to residual stream 1. The MLPs in residual stream 1 then operate nonlinearly on this tally, writing vectors to the residual stream that distinguish between the cases of zero and non-zero total elevation. Head 2.0 copies this signal to residual stream 0, where it then goes through the classifier and leads to a classification as unbalanced. Our first-pass understanding of this behavior is complete.\n\nAn illustration of this circuit is given below. It’s pretty complicated with a lot of moving parts, so don’t worry if you don’t follow all of it!\nKey: the thick black lines and orange dotted lines show the paths through our transformer constituting the elevation circuit. The orange dotted lines indicate the skip connections. Each of the important heads and MLP layers are coloured bold. The three important parts of our circuit (head 0.0, the MLP layers, and head 2.0) are all give annotations explaining what they’re doing, and the evidence we found for this."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#investigating-the-bracket-transformer",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#investigating-the-bracket-transformer",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Investigating the bracket transformer",
    "text": "Investigating the bracket transformer\nHere, we have a few bonus exercises which build on the previous content (e.g. having you examine different parts of the model, or use your understanding of how the model works to generate adversarial examples).\nThis final section is less guided, although the suggested exercises are similar in flavour to the previous section.\n\nLearning objctives\n\nUse your understanding of how the model works to generate adversarial examples.\nTake deeper dives into specific anomalous features of the model.\n\n\nThe main bonus exercise we recommend you try is adversarial attacks. You’ll need to read the first section of the detecting anywhere-negative failures bonus exercise to get an idea for how the other half of the classification circuit works, but once you understand this you can jump ahead to the adversarial attacks section.\n\nDetecting anywhere-negative failures\nWhen we looked at our grid of attention patterns, we saw that not only did the first query token pay approximately uniform attention to all tokens following it, but so did most of the other tokens (to lesser degrees). This means that we can write the vector written to position \\(i\\) (for general \\(i\\geq 1\\)) as:\n\\[\n\\begin{aligned}\nh(x)_i &\\approx \\frac{1}{n-i+1} \\sum_{j=i}^n \\color{orange}{x}_j^T L^T W_{OV}^{0.0} \\\\\n&= \\frac{1}{n} \\left( \\sum_{i=1}^n \\color{orange}{pos}_i^T L^T W_{OV}^{0.0} + n_L^{(i)} \\color{orange}{\\vec v_L} + n_R^{(i)} \\color{orange}{\\vec v_R}\\right)\n\\end{aligned}\n\\]\nwhere \\(n_L^{(i)}\\) and \\(n_R^{(i)}\\) are the number of left and right brackets respectively in the substring formed from brackets[i: n] (i.e. this matches our definition of \\(n_L\\) and \\(n_R\\) when \\(i=1\\)).\nGiven what we’ve seen so far (that sequence position 1 stores tally information for all the brackets in the sequence), we can guess that each sequence position stores a similar tally, and is used to determine whether the substring consisting of all brackets to the right of this one has any elevation failures (i.e. making sure the total number of right brackets is at least as great as the total number of left brackets - recall it’s this way around because our model learned the equally valid right-to-left solution).\nRecall that the destination token only determines how much to pay attention to the source; the vector that is moved from the source to destination conditional on attention being paid to it is the same for all destination tokens. So the result about left-paren and right-paren vectors having cosine similarity of -1 also holds for all later sequence positions.\nHead 2.1 turns out to be the head for detecting anywhere-negative failures (i.e. it detects whether any sequence brackets[i: n] has strictly more right than left parentheses, and writes to the residual stream in the unbalanced direction if this is the case). Can you find evidence for this behaviour?\nOne way you could investigate this is to construct a parens string which “goes negative” at some points, and look at the attention probabilities for head 2.0 at destination position 0. Does it attend most strongly to those source tokens where the bracket goes negative, and is the corresponding vector written to the residual stream one which points in the unbalanced direction?\nYou could also look at the inputs to head 2.1, just like we did for head 2.0. Which components are most important, and can you guess why?\n\n\nAnswer\n\nYou should find that the MLPs are important inputs into head 2.1. This makes sense, because earlier we saw that the MLPs were converting tally information \\((n_L - \\alpha n_R)\\) into the boolean information \\((n_L = n_R)\\) at sequence position 1. Since MLPs act the same on all sequence positions, it’s reasonable to guess that they’re storing the boolean information \\((n_L^{(i)} &gt; n_R^{(i)})\\) at each sequence position \\(i\\), which is what we need to detect anywhere-negative failures.\n\n\n\nAdversarial attacks\nOur model gets around 1 in a ten thousand examples wrong on the dataset we’ve been using. Armed with our understanding of the model, can we find a misclassified input by hand? I recommend stopping reading now and trying your hand at applying what you’ve learned so far to find a misclassified sequence. If this doesn’t work, look at a few hints.\n\n\nHint 1\n\nWhat’s up with those weird patchy bits in the bottom-right corner of the attention patterns? Can we exploit this?\nRead the next hint for some more specific directions.\n\n\n\nHint 2\n\nWe observed that each left bracket attended approximately uniformly to each of the tokens to its right, and used this to detect elevation failures at any point. We also know that this approximately uniform pattern breaks down around query positions 27-31.\nWith this in mind, what kind of “just barely” unbalanced bracket string could we construct that would get classified as balanced by the model?\nRead the next hint for a suggested type of bracket string.\n\n\n\nHint 3\n\nWe want to construct a string that has a negative elevation at some point, but is balanced everywhere else. We can do this by using a sequence of the form A)(B, where A and B are balanced substrings. The positions of the open paren next to the B will thus be the only position in the whole sequence on which the elevation drops below zero, and it will drop just to -1.\nRead the next hint to get ideas for what A and B should be (the clue is in the attention pattern plot!).\n\n\n\nHint 4\n\nFrom the attention pattern plot, we can see that left parens in the range 27-31 attend bizarrely strongly to the tokens at position 38-40. This means that, if there is a negative elevation in or after the range 27-31, then the left bracket that should be detecting this negative elevation might miscount. In particular, if B = ((...)), this left bracket might heavily count the right brackets at the end, and less heavily weight the left brackets at the start of B, thus this left bracket might “think” that the sequence is balanced when it actually isn’t.\n\n\n\nSolution (for best currently-known advex)\n\nChoose A and B to each be a sequence of (((...))) terms with length \\(i\\) and \\(38-i\\) respectively (it makes sense to choose A like this also, because want the sequence to have maximal positive elevation everywhere except the single position where it’s negative). Then, maximize over \\(i = 2, 4, ...\\,\\). Unsurprisingly given the observations in the previous hint, we find that the best adversarial examples (all with balanced probability of above 98%) are \\(i=24, 26, 28, 30, 32\\). The best of these is \\(i=30\\), which gets 99.9856% balanced confidence.\ndef tallest_balanced_bracket(length: int) -&gt; str:\n    return \"\".join([\"(\" for _ in range(length)] + [\")\" for _ in range(length)])\n    \nexample = tallest_balanced_bracket(15) + \")(\" + tallest_balanced_bracket(4)\n\n\n\n# YOUR CODE HERE - update the examples list below, to find adversarial examples!\ndef tallest_balanced_bracket(length: int) -&gt; str:\n    return \"\".join([\"(\" for _ in range(length)] + [\")\" for _ in range(length)])\n\nexamples = [\"()\", \"(())\", \"))\"]\n\nexample = tallest_balanced_bracket(15) + \")(\" + tallest_balanced_bracket(4)\nexamples.append(example)\nm = max(len(ex) for ex in examples)\ntoks = tokenizer.tokenize(examples)\nprobs = model(toks)[:, 0].softmax(-1)[:, 1]\nprint(\"\\n\".join([f\"{ex:{m}} -&gt; {p:.4%} balanced confidence\" for (ex, p) in zip(examples, probs)]))\n\n()                                       -&gt; 99.9987% balanced confidence\n(())                                     -&gt; 99.9989% balanced confidence\n))                                       -&gt; 0.0121% balanced confidence\n((((((((((((((())))))))))))))))((((()))) -&gt; 99.9856% balanced confidence\n\n\n\n\nDealing with early closing parens\nWe mentioned that our model deals with early closing parens differently. One of our components in particular is responsible for classifying any sequence that starts with a closed paren as unbalnced - can you find the component that does this?\n\n\nHint\n\nIt’ll have to be one of the attention heads, since these are the only things which can move information from sequence position 1 to position 0 (and the failure mode we’re trying to detect is when the sequence has a closed paren in position 1).\nWhich of your attention heads was previously observed to move information from position 1 to position 0?\n\nCan you plot the outputs of this component when there is a closed paren at first position? Can you prove that this component is responsible for this behavior, and show exactly how it happens?"
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#suggested-capstone-projects",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#suggested-capstone-projects",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Suggested capstone projects",
    "text": "Suggested capstone projects\n\nTry more algorithmic problems\nInterpreting toy models is a good way to increase your confidence working with TransformerLens and basic interpretability methods. It’s maybe not the most exciting category of open problems in mechanistic interpretability, but can still be a useful exercise - and sometimes it can lead to interesting new insights about how interpretability tools can be used.\nIf you’re feeling like it, you can try to hop onto LeetCode and pick a suitable problem (we recommend the “Easy” section) to train a transformer and interpret its output. Here are a few suggestions to get you started (some of these were taken from LeetCode, others from Neel Nanda’s open problems post). They’re listed approximately from easier to harder, although this is just a guess since I haven’t personally interpreted these. Note, there are ways you could make any of these problems easier or harder with modifications - I’ve included some ideas inline.\n\nCalculating sequences with a Fibonacci-style recurrence relation (i.e. predicting the next element from the previous two)\nSearch Insert Position - an easier version would be when the target is always guaranteed to be in the list (you also wouldn’t need to worry about sorting in this case). The version without this guarantee is a very different problem, and would be much harder\nIs Subsequence - you should start with subsequences of length 1 (in which case this problem is pretty similar to the easier version of the previous problem), and work up from there\nMajority Element - you can try playing around with the data generation process to change the difficulty, e.g. sequences where there is no guarantee on the frequency of the majority element (i.e. you’re just looking for the token which appears more than any other token) would be much harder\nNumber of Equivalent Domino Pairs - you could restrict this problem to very short lists of dominos to make it easier (e.g. start with just 2 dominos!)\nLongest Substring Without Repeating Characters\nIsomorphic Strings - you could make it simpler by only allowing the first string to have duplicate characters, or by restricting the string length / vocabulary size\nPlus One - you might want to look at the “sum of numbers” algorithmic problem before trying this, and/or the grokking exercises in this chapter. Understanding this problem well might actually help you build up to interpreting the “sum of numbers” problem (I haven’t done this, so it’s very possible you could come up with a better interpretation of that monthly problem than mine, since I didn’t go super deep into the carrying mechanism)\nPredicting permutations, i.e. predicting the last 3 tokens of the 12-token sequence (17 3 11) (17 1 13) (11 2 4) (11 4 2) (i.e. the model has to learn what permutation function is being applied to the first group to get the second group, and then apply that permutation to the third group to correctly predict the fourth group). Note, this problem might require 3 layers to solve - can you see why?\nTrain models for automata tasks and interpret them - do your results match the theory?\nPredicting the output to simple code functions. E.g. predicting the 1 2 4 text in the following sequence (which could obviously be made harder with some obvious modifications, e.g. adding more variable definitions so the model has to attend back to the right one):\n\na = 1 2 3\na[2] = 4\na -&gt; 1 2 4\n\nGraph theory problems like this. You might have to get creative with the input format when training transformers on tasks like this!\n\nNote, ARENA runs a monthly algorithmic problems sequence, and you can get ideas from looking at past problems from this sequence. You can also use these repos to get some sample code for building & training a trnasformer on a toy model, and constructing a dataset for your particular problem."
  },
  {
    "objectID": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#suggested-paper-replications",
    "href": "reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).html#suggested-paper-replications",
    "title": "[1.5.1] Balanced Bracket Classifier (solutions)",
    "section": "Suggested paper replications",
    "text": "Suggested paper replications\n\n\nCausal Scrubbing\nCausal scrubbing is an algorithm developed by Redwood Research, which tries to create an automated metric for dweciding whether a computational subgraph corresponds to a circuit. Some reading on this:\n\nNeel’s dynalist notes (short)\nCausal Scrubbing: a method for rigorously testing interpretability hypotheses (full LessWrong post describing the algorithm)\n\nYou can also read Redwood’s full sequence here, where they mention applying it to the paren balancer\n\nPractical Pitfalls of Causal Scrubbing\n\nCan you write the causal scrubbing algorithm, and use it to replicate their results? You might want to start with induction heads before applying it to the bracket classifier.\nThis might be a good replication for you if:\n\nYou like high levels of rigour, rather than the more exploratory-style work we’ve largely focused on so far\nYou enjoyed these exercises, and feel like you have a good understanding of the kinds of circuits implemented by this bracket classifier\n(Ideally) you’ve done some investigation of the “detecting anywhere negative failures” bonus exercise suggested above\n\n\n\n\nA circuit for Python docstrings in a 4-layer attention-only transformer\nThis work was produced as part of the SERI ML Alignment Theory Scholars Program (Winter 2022) under the supervision of Neel Nanda. Similar to how the IOI paper searched for in some sense the simplest kind of circuit which required 3 layers, this work was looking for the simplest kind of circuit which required 4 layers. The task they investigated was the docstring task - can you predict parameters in the right order, in situations like this:\ndef port(self, load, size, files, last):\n    '''oil column piece\n\n    :param load: crime population\n    :param size: unit dark\n    :param\nThe token that follows should be files, and just like in the case of IOI we can deeply analyze how the transformer solves this task. Unlike IOI, we’re looking at a 4-layer transformer which was trained on code (not GPT2-Small), which makes a lot of the analysis cleaner (even though the circuit has more levels of composition than IOI does).\nFor an extra challenge, rather than replicating the authors’ results, you can try and perform this investigation yourself, without seeing what tools the authors of the paper used! Most will be similar to the ones you’ve used in the exercises so far.\nThis might be a good replication for you if:\n\nYou enjoyed most/all sections of these exercises, and want to practice using the tools you learned in a different context - specifically, a model which is less algorithmic and might not have as crisp a circuit as the bracket transformer\nYou’d prefer to do something with a bit more of a focus on real language models, but still don’t want to go all the way up to models as large as GPT2-Small\n\nNote, this replication is closer to [1.3] Indirect Object Identification than to these exercises. If you’ve got time before finishing this chapter then we recommend you try these exercises first, since they’ll be very helpful for giving you a set of tools which are more suitable for working with large models."
  },
  {
    "objectID": "notebook.html",
    "href": "notebook.html",
    "title": "Integrated Gradients vs Activation Patching",
    "section": "",
    "text": "Objective: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\nBackground:\n\nBoth integrated gradients (IG) and activation patching (AP) are used extensively in explainability and interpretability.\n\nIG is path-based: integrates gradients along a path from a baseline to the input.\nAP is perturbation-based: directly measures causal effect of replacing activations.\n\nActivation patching can be approximated cheaply using methods inspired by integrated gradients; see Hanna et al (2024)\nGradient-based attribution methods can be used to identify important model components as an alternative to activation patching; see Ferrando and Voita (2024)\nActivation patching is often used as the “gold standard” to evaluate attribution methods\n\nMotivation:\n\nUnderstand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\nInvestigate if discrepancies help uncover different hidden model behaviours\nUnderstand when and why linear approximations to activation patching fail\nInvestigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is “incorrect”)\n\nSome ideas:\n\nComponents which might not be identified by activation patching: generic components which are used for both clean and corrupted examples in activation patching\nComponents which might not be identified by integrated gradients: backup attention heads, components which only influence the output when interacting with other components, i.e. OR circuits\n\n\n\nWe load a pre-trained toy transformer which performs balanced bracket classification. The model has three layers, each with two attention heads and one MLP layer of 56 neurons.\n\n\n\nCode\nimport importlib\n\nimport torch\nfrom captum.attr import LayerIntegratedGradients\nimport numpy as np\nfrom transformer_lens.utils import get_act_name\nfrom transformer_lens import ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nimport toy_transformers.toy_bracket_transformer as tt\nimportlib.reload(tt)\nfrom toy_transformers.toy_bracket_transformer import load_toy_bracket_transformer, test_loaded_bracket_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntokenizer, model = load_toy_bracket_transformer()"
  },
  {
    "objectID": "notebook.html#background-motivation-and-set-up",
    "href": "notebook.html#background-motivation-and-set-up",
    "title": "Integrated Gradients vs Activation Patching",
    "section": "",
    "text": "Objective: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.\nBackground:\n\nBoth integrated gradients (IG) and activation patching (AP) are used extensively in explainability and interpretability.\n\nIG is path-based: integrates gradients along a path from a baseline to the input.\nAP is perturbation-based: directly measures causal effect of replacing activations.\n\nActivation patching can be approximated cheaply using methods inspired by integrated gradients; see Hanna et al (2024)\nGradient-based attribution methods can be used to identify important model components as an alternative to activation patching; see Ferrando and Voita (2024)\nActivation patching is often used as the “gold standard” to evaluate attribution methods\n\nMotivation:\n\nUnderstand when and why do IG and AP disagree: e.g. methodological limitations, or suitability to model tasks, etc.\nInvestigate if discrepancies help uncover different hidden model behaviours\nUnderstand when and why linear approximations to activation patching fail\nInvestigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is “incorrect”)\n\nSome ideas:\n\nComponents which might not be identified by activation patching: generic components which are used for both clean and corrupted examples in activation patching\nComponents which might not be identified by integrated gradients: backup attention heads, components which only influence the output when interacting with other components, i.e. OR circuits\n\n\n\nWe load a pre-trained toy transformer which performs balanced bracket classification. The model has three layers, each with two attention heads and one MLP layer of 56 neurons.\n\n\n\nCode\nimport importlib\n\nimport torch\nfrom captum.attr import LayerIntegratedGradients\nimport numpy as np\nfrom transformer_lens.utils import get_act_name\nfrom transformer_lens import ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nimport toy_transformers.toy_bracket_transformer as tt\nimportlib.reload(tt)\nfrom toy_transformers.toy_bracket_transformer import load_toy_bracket_transformer, test_loaded_bracket_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\ntokenizer, model = load_toy_bracket_transformer()"
  },
  {
    "objectID": "Integrated Gradients vs Activation Patching.html",
    "href": "Integrated Gradients vs Activation Patching.html",
    "title": "Comparing Integrated Gradients and Activation Patching",
    "section": "",
    "text": "We load a pre-trained toy transformer which performs balanced bracket classification.\nImportant note: the final classification for whether the sequence is balanced or not comes from position 0 of the output (all other outputs are discarded).\n\n\nimport importlib\n\nimport torch\nfrom captum.attr import LayerIntegratedGradients\nimport numpy as np\nfrom transformer_lens.utils import get_act_name\nfrom transformer_lens import ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nimport toy_transformers.toy_bracket_transformer as tt\nimportlib.reload(tt)\nfrom toy_transformers.toy_bracket_transformer import load_toy_bracket_transformer, test_loaded_bracket_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ntokenizer, model = load_toy_bracket_transformer()\n\nHookedTransformer(\n  (embed): Embed()\n  (hook_embed): HookPoint()\n  (pos_embed): PosEmbed()\n  (hook_pos_embed): HookPoint()\n  (hook_tokens): HookPoint()\n  (blocks): ModuleList(\n    (0-2): 3 x TransformerBlock(\n      (ln1): LayerNorm(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (ln2): LayerNorm(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_pattern): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n      )\n      (hook_attn_in): HookPoint()\n      (hook_q_input): HookPoint()\n      (hook_k_input): HookPoint()\n      (hook_v_input): HookPoint()\n      (hook_mlp_in): HookPoint()\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n  )\n  (ln_final): LayerNorm(\n    (hook_scale): HookPoint()\n    (hook_normalized): HookPoint()\n  )\n  (unembed): Unembed()\n)\nLoaded model"
  },
  {
    "objectID": "Integrated Gradients vs Activation Patching.html#set-up",
    "href": "Integrated Gradients vs Activation Patching.html#set-up",
    "title": "Comparing Integrated Gradients and Activation Patching",
    "section": "",
    "text": "We load a pre-trained toy transformer which performs balanced bracket classification.\nImportant note: the final classification for whether the sequence is balanced or not comes from position 0 of the output (all other outputs are discarded).\n\n\nimport importlib\n\nimport torch\nfrom captum.attr import LayerIntegratedGradients\nimport numpy as np\nfrom transformer_lens.utils import get_act_name\nfrom transformer_lens import ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nimport toy_transformers.toy_bracket_transformer as tt\nimportlib.reload(tt)\nfrom toy_transformers.toy_bracket_transformer import load_toy_bracket_transformer, test_loaded_bracket_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ntokenizer, model = load_toy_bracket_transformer()\n\nHookedTransformer(\n  (embed): Embed()\n  (hook_embed): HookPoint()\n  (pos_embed): PosEmbed()\n  (hook_pos_embed): HookPoint()\n  (hook_tokens): HookPoint()\n  (blocks): ModuleList(\n    (0-2): 3 x TransformerBlock(\n      (ln1): LayerNorm(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (ln2): LayerNorm(\n        (hook_scale): HookPoint()\n        (hook_normalized): HookPoint()\n      )\n      (attn): Attention(\n        (hook_k): HookPoint()\n        (hook_q): HookPoint()\n        (hook_v): HookPoint()\n        (hook_z): HookPoint()\n        (hook_attn_scores): HookPoint()\n        (hook_pattern): HookPoint()\n        (hook_result): HookPoint()\n      )\n      (mlp): MLP(\n        (hook_pre): HookPoint()\n        (hook_post): HookPoint()\n      )\n      (hook_attn_in): HookPoint()\n      (hook_q_input): HookPoint()\n      (hook_k_input): HookPoint()\n      (hook_v_input): HookPoint()\n      (hook_mlp_in): HookPoint()\n      (hook_attn_out): HookPoint()\n      (hook_mlp_out): HookPoint()\n      (hook_resid_pre): HookPoint()\n      (hook_resid_mid): HookPoint()\n      (hook_resid_post): HookPoint()\n    )\n  )\n  (ln_final): LayerNorm(\n    (hook_scale): HookPoint()\n    (hook_normalized): HookPoint()\n  )\n  (unembed): Unembed()\n)\nLoaded model"
  },
  {
    "objectID": "Integrated Gradients vs Activation Patching.html#integrated-gradients",
    "href": "Integrated Gradients vs Activation Patching.html#integrated-gradients",
    "title": "Comparing Integrated Gradients and Activation Patching",
    "section": "Integrated gradients",
    "text": "Integrated gradients\nWe need to choose an appropriate baseline to calculate Integrated Gradients from. An ideal baseline input will produce a final output which is close to zero, and meaningfully represents a lack of information (see Sundararajan et al. 2017).\nHere we test a series of inputs which are feasible baseline inputs, and check their final classification scores. The [START, PAD, END] token sequence with zero patching on target components seems to be the best baseline, because it produces a final output which is consistently closest to zero, and this sequence meaningfully represents an input with no information.\n\n# The prediction at the baseline should be near zero (see Sundararajan et al. 2017)\n# Here we test a series of inputs which could be a baseline, and check their final classification scores\n\ndef predict_balanced(x):\n    logits = model(x)[:, 0]\n    return logits.softmax(-1)[:, 1]\n\nbalanced_input = tokenizer.tokenize(\"()()\")\nprint(\"Input:\", balanced_input)\nprint(predict_balanced(balanced_input), \"\\n\")\n\nunbalanced_input = tokenizer.tokenize(\"(()(\")\nprint(\"Input:\", unbalanced_input)\nprint(predict_balanced(unbalanced_input), \"\\n\")\n\nall_padding = torch.full_like(balanced_input, tokenizer.PAD_TOKEN)\nprint(\"Input:\", all_padding)\nprint(predict_balanced(all_padding), \"\\n\")\n\nmask = np.isin(balanced_input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])\nstart_pad_end = balanced_input * mask + tokenizer.PAD_TOKEN * (1 - mask)\nprint(\"Input:\", start_pad_end)\nprint(predict_balanced(start_pad_end), \"\\n\")\n\nall_zeroes = torch.zeros_like(balanced_input)\nprint(\"Input:\", all_zeroes)\nprint(predict_balanced(all_zeroes), \"\\n\")\n\n\ndef patch_zero_hook(activations: torch.Tensor, hook: HookPoint):\n    # Replace activations with zeroes\n    activations = torch.zeros_like(activations)\n    return activations\n\nwith model.hooks(fwd_hooks=[(\"blocks.0.hook_mlp_out\", patch_zero_hook)]):\n    print(\"Patch zeroes in at target component\")\n    print(predict_balanced(balanced_input))\n    print(predict_balanced(unbalanced_input))\n    print(predict_balanced(all_padding))\n    print(predict_balanced(start_pad_end))\n    print(predict_balanced(all_zeroes))\n    \n\nInput: tensor([[0, 3, 4, 3, 4, 2]])\ntensor([1.0000], grad_fn=&lt;SelectBackward0&gt;) \n\nInput: tensor([[0, 3, 3, 4, 3, 2]])\ntensor([1.8627e-05], grad_fn=&lt;SelectBackward0&gt;) \n\nInput: tensor([[1, 1, 1, 1, 1, 1]])\ntensor([0.0411], grad_fn=&lt;SelectBackward0&gt;) \n\nInput: tensor([[0, 1, 1, 1, 1, 2]])\ntensor([3.6777e-06], grad_fn=&lt;SelectBackward0&gt;) \n\nInput: tensor([[0, 0, 0, 0, 0, 0]])\ntensor([1.5444e-05], grad_fn=&lt;SelectBackward0&gt;) \n\nPatch zeroes in at target component\ntensor([0.0074], grad_fn=&lt;SelectBackward0&gt;)\ntensor([6.3960e-05], grad_fn=&lt;SelectBackward0&gt;)\ntensor([0.0172], grad_fn=&lt;SelectBackward0&gt;)\ntensor([1.0772e-05], grad_fn=&lt;SelectBackward0&gt;)\ntensor([1.5005e-05], grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ndef patch_zero_hook(activations: torch.Tensor, hook: HookPoint):\n    # Replace activations with zeroes\n    activations = torch.zeros_like(activations)\n    return activations\n\n\ndef patch_zero_forward_fn(x, baseline, target_layer):\n    if torch.equal(x, baseline):\n        # Patch zeros into target layer when baseline measurement is given\n        output = model.run_with_hooks(x, fwd_hooks=[(target_layer.name, patch_zero_hook)])\n    else:\n        output = model(x)\n    logits = output[:, 0]\n    return logits.softmax(-1)[:, 1]\n\n\ndef compute_layer_to_output_attributions(input, target_layer, baseline):\n    forward_fn = lambda x: patch_zero_forward_fn(x, baseline, target_layer)\n    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n    # Calculate layer integrated gradients for class 0 (unbalanced),\n    attributions, approximation_error = ig_embed.attribute(inputs=input,\n                                                    baselines=baseline,\n                                                    attribute_to_layer_input=False,\n                                                    return_convergence_delta=True)\n    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error.item()}\")\n    return attributions\n\n\n# Gradient attribution for neurons in MLP layers\nmlp_ig_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Gradient attribution for attention heads\nattn_ig_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\n# Calculate integrated gradients for each layer\ninput = tokenizer.tokenize(\"()()\")\nmask = np.isin(input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])\nbaseline = input * mask + tokenizer.PAD_TOKEN * (1 - mask)\n\nfor layer in range(model.cfg.n_layers):\n    # Gradient attribution on heads\n    hook_name = get_act_name(\"result\", layer)\n    target_layer = model.hook_dict[hook_name]\n    attributions = compute_layer_to_output_attributions(input, target_layer, baseline) # shape [1, seq_len, d_head, d_model]\n    # Calculate attribution score based on mean over each embedding, for each token\n    per_token_score = attributions.mean(dim=3)\n    score = per_token_score.mean(dim=1)\n    attn_ig_results[layer] = score\n\n    # Gradient attribution on MLP neurons\n    target_layer = model.blocks[layer].hook_mlp_out\n    attributions = compute_layer_to_output_attributions(input, target_layer, baseline) # shape [1, seq_len, d_model]\n    score = attributions.mean(dim=1)\n    mlp_ig_results[layer] = score\n\n\nError (delta) for blocks.0.attn.hook_result attribution: -0.06375384330749512\n\nError (delta) for blocks.0.hook_mlp_out attribution: -0.0021299123764038086\n\nError (delta) for blocks.1.attn.hook_result attribution: -0.12871885299682617\n\nError (delta) for blocks.1.hook_mlp_out attribution: -0.019762754440307617\n\nError (delta) for blocks.2.attn.hook_result attribution: 0.0016323328018188477\n\nError (delta) for blocks.2.hook_mlp_out attribution: -0.9999805688858032\n\n\n\nbound = max(torch.max(mlp_ig_results), abs(torch.min(mlp_ig_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_ig_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Gradient Attribution (Integrated Gradients)\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbound = max(torch.max(attn_ig_results), abs(torch.min(attn_ig_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_ig_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Gradient Attribution (Integrated Gradients)\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\nComparison: unbalanced input sequence\nRepeat with unbalanced sequence for comparison.\n\n# Gradient attribution for neurons in MLP layers\nmlp_ig_unbalanced_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Gradient attribution for attention heads\nattn_ig_unbalanced_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\n# Calculate integrated gradients for each layer\ninput = tokenizer.tokenize(\"(()(\")\nmask = np.isin(input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])\nbaseline = input * mask + tokenizer.PAD_TOKEN * (1 - mask)\n\nfor layer in range(model.cfg.n_layers):\n    # Gradient attribution on heads\n    hook_name = get_act_name(\"result\", layer)\n    target_layer = model.hook_dict[hook_name]\n    attributions = compute_layer_to_output_attributions(input, target_layer, baseline)\n    # Calculate attribution score based on mean over each embedding, for each token\n    per_token_score = attributions.mean(dim=3)\n    score = per_token_score.mean(dim=1)\n    attn_ig_unbalanced_results[layer] = score\n\n    # Gradient attribution on MLP neurons\n    target_layer = model.blocks[layer].hook_mlp_out\n    attributions = compute_layer_to_output_attributions(input, target_layer, baseline)\n    score = attributions.mean(dim=1)\n    mlp_ig_unbalanced_results[layer] = score\n\n\nError (delta) for blocks.0.attn.hook_result attribution: 2.431000211799983e-05\n\nError (delta) for blocks.0.hook_mlp_out attribution: -7.003468454058748e-06\n\nError (delta) for blocks.1.attn.hook_result attribution: 2.229996425739955e-06\n\nError (delta) for blocks.1.hook_mlp_out attribution: -1.0247405953123234e-05\n\nError (delta) for blocks.2.attn.hook_result attribution: 0.0020253800321370363\n\nError (delta) for blocks.2.hook_mlp_out attribution: -1.7768754332792014e-05\n\n\nGradient attribution scores for MLP neurons do not depend on whether the sequence is balanced or unbalanced. This suggests that the MLP neurons may be involved in some universal processing of information, used downstream for final output classification.\nGradient attribution scores for attention heads change when the sequence is unbalanced. Notably, head 2.1 contributes significantly more highly to a positive “unbalanced” classification. This suggests that head 2.1 plays a strong role in the final output. The attribution scores for the other attention heads are less strong, suggesting a weaker role.\n\nbound = max(torch.max(mlp_ig_unbalanced_results), abs(torch.min(mlp_ig_unbalanced_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_ig_unbalanced_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Gradient Attribution (Integrated Gradients) - Unbalanced\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbound = max(torch.max(attn_ig_unbalanced_results), abs(torch.min(attn_ig_unbalanced_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_ig_unbalanced_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Gradient Attribution (Integrated Gradients) - Unbalanced\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nComparison: empty string attributions\n\n# Gradient attribution for neurons in MLP layers\nmlp_ig_empty_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Gradient attribution for attention heads\nattn_ig_empty_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\n# Calculate integrated gradients for each layer\ninput = tokenizer.tokenize(\"\")\nmask = np.isin(input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])\nbaseline = input * mask + tokenizer.PAD_TOKEN * (1 - mask)\n\nfor layer in range(model.cfg.n_layers):\n    # Gradient attribution on heads\n    hook_name = get_act_name(\"result\", layer)\n    target_layer = model.hook_dict[hook_name]\n    attributions = compute_layer_to_output_attributions(input, target_layer, baseline)\n    # Calculate attribution score based on mean over each embedding, for each token\n    per_token_score = attributions.mean(dim=3)\n    score = per_token_score.mean(dim=1)\n    attn_ig_empty_results[layer] = score\n\n    # Gradient attribution on MLP neurons\n    target_layer = model.blocks[layer].hook_mlp_out\n    attributions = compute_layer_to_output_attributions(input, target_layer, baseline)\n    score = attributions.mean(dim=1)\n    mlp_ig_empty_results[layer] = score\n\n\nError (delta) for blocks.0.attn.hook_result attribution: 0.0\n\nError (delta) for blocks.0.hook_mlp_out attribution: 0.0\n\nError (delta) for blocks.1.attn.hook_result attribution: 0.0\n\nError (delta) for blocks.1.hook_mlp_out attribution: 0.0\n\nError (delta) for blocks.2.attn.hook_result attribution: 0.0\n\nError (delta) for blocks.2.hook_mlp_out attribution: 0.0\n\n\n\nbound = max(torch.max(mlp_ig_empty_results), abs(torch.min(mlp_ig_empty_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_ig_empty_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Gradient Attribution (Integrated Gradients) - Empty\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbound = max(torch.max(attn_ig_empty_results), abs(torch.min(attn_ig_empty_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_ig_empty_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Gradient Attribution (Integrated Gradients) - Empty\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAverage of integrated gradients attributions\nTake the average of the integrated gradients scores for balanced and unbalanced inputs.\n\nmlp_ig_sum_results = (mlp_ig_results + mlp_ig_unbalanced_results) / 2\n\nbound = max(torch.max(mlp_ig_sum_results), abs(torch.min(mlp_ig_sum_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_ig_sum_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Gradient Attribution (Integrated Gradients) - Summed\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBaseline as corrupted activations\nInstead of using zero as the baseline activation, use the activation from the corrupt input in causal tracing.\n\ndef run_from_layer_fn(x, original_input, prev_layer):\n    # Instead of returning the current activations, return the given target layer outputs\n    output = model.run_with_hooks(\n        original_input,\n        fwd_hooks=[(prev_layer.name, lambda act, hook: x)]\n    )\n    logits = output[:, 0]\n    return logits.softmax(-1)[:, 1]\n\ndef compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer):\n    # Take the model starting from the target layer\n    forward_fn = lambda x: run_from_layer_fn(x, original_input, prev_layer)\n    ig_embed = LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs=True)\n    attributions, approximation_error = ig_embed.attribute(inputs=layer_input,\n                                                    baselines=layer_baseline, \n                                                    attribute_to_layer_input=False,\n                                                    return_convergence_delta=True)\n    print(f\"\\nError (delta) for {target_layer.name} attribution: {approximation_error.item()}\")\n    return attributions\n\n\n# Gradient attribution for neurons in MLP layers\nmlp_ig_corrupt_baseline_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Gradient attribution for attention heads\nattn_ig_corrupt_baseline_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\n# Calculate integrated gradients for each layer\nclean_input = tokenizer.tokenize(\"()()\")\n_, clean_cache = model.run_with_cache(clean_input)\n\ncorrupted_input = tokenizer.tokenize(\"(()(\")\n# mask = np.isin(input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])\n# corrupted_input = input * mask + tokenizer.PAD_TOKEN * (1 - mask)\n_, corrupted_cache = model.run_with_cache(corrupted_input)\n\nfor layer in range(model.cfg.n_layers):\n    # Gradient attribution on heads\n    hook_name = get_act_name(\"result\", layer)\n    target_layer = model.hook_dict[hook_name]\n    prev_layer_hook = get_act_name(\"z\", layer)\n    prev_layer = model.hook_dict[prev_layer_hook]\n\n    layer_clean_input = clean_cache[prev_layer_hook]\n    layer_corrupt_input = corrupted_cache[prev_layer_hook]\n\n    print(run_from_layer_fn(layer_corrupt_input, clean_input, prev_layer))\n\n    attributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_head, d_model]\n    # Calculate attribution score based on mean over each embedding, for each token\n    per_token_score = attributions.mean(dim=3)\n    score = per_token_score.mean(dim=1)\n    attn_ig_corrupt_baseline_results[layer] = score\n\n    # Gradient attribution on MLP neurons\n    hook_name = get_act_name(\"mlp_out\", layer)\n    target_layer = model.hook_dict[hook_name]\n    prev_layer_hook = get_act_name(\"post\", layer)\n    prev_layer = model.hook_dict[prev_layer_hook]\n\n    layer_clean_input = clean_cache[prev_layer_hook]\n    layer_corrupt_input = corrupted_cache[prev_layer_hook]\n    \n    attributions = compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) # shape [1, seq_len, d_model]\n    score = attributions.mean(dim=1)\n    mlp_ig_corrupt_baseline_results[layer] = score\n\ntensor([1.4471e-05], grad_fn=&lt;SelectBackward0&gt;)\n\nError (delta) for blocks.0.attn.hook_result attribution: -0.5329379439353943\n\nError (delta) for blocks.0.hook_mlp_out attribution: -0.010210990905761719\ntensor([0.0496], grad_fn=&lt;SelectBackward0&gt;)\n\nError (delta) for blocks.1.attn.hook_result attribution: -0.0002592802047729492\n\nError (delta) for blocks.1.hook_mlp_out attribution: 2.187490463256836e-05\ntensor([8.8178e-06], grad_fn=&lt;SelectBackward0&gt;)\n\nError (delta) for blocks.2.attn.hook_result attribution: -0.00015395879745483398\n\nError (delta) for blocks.2.hook_mlp_out attribution: 2.0354718799353577e-08\n\n\n\nbound = max(torch.max(mlp_ig_corrupt_baseline_results), abs(torch.min(mlp_ig_corrupt_baseline_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_ig_corrupt_baseline_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Gradient Attribution (Integrated Gradients): Corrupt Baseline\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\nbound = max(torch.max(attn_ig_corrupt_baseline_results), abs(torch.min(attn_ig_corrupt_baseline_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_ig_corrupt_baseline_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Gradient Attribution (Integrated Gradients): Corrupt Baseline\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "Integrated Gradients vs Activation Patching.html#causal-tracing",
    "href": "Integrated Gradients vs Activation Patching.html#causal-tracing",
    "title": "Comparing Integrated Gradients and Activation Patching",
    "section": "Causal tracing",
    "text": "Causal tracing\nNoising (a corrupt → clean patch) shows whether the patched activations were necessary to maintain the model behaviour. Therefore we patch corrupted activations into a clean run.\n\nfrom transformer_lens import HookedTransformer, ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nclean_input = tokenizer.tokenize(\"()()\")        # Balanced\ncorrupted_input = tokenizer.tokenize(\"(()(\")    # Unbalanced\n\n# We run on the corrupted prompt with the cache so we store activations to patch in later.\ncorrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)\nclean_logits = model(clean_input)\n\n# Get probability of overall sequence being balanced (class 1) from position 0\nclean_answer_logits = clean_logits[0, 0, 1]\ncorrupted_answer_logits = corrupted_logits[0, 0, 1]\nprint(f\"Balanced input score: {clean_answer_logits}\")\nprint(f\"Unbalanced input score: {corrupted_answer_logits}\")\n\nbaseline_diff = (corrupted_answer_logits - clean_answer_logits).item()\nprint(f\"Baseline clean-corrupted logit difference: {baseline_diff:.2f}\")\n\nBalanced input score: 5.693811416625977\nUnbalanced input score: -5.421151161193848\nBaseline clean-corrupted logit difference: -11.11\n\n\n\n# Patch neurons in MLP layers\nmlp_patch_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Patch attention heads\nattn_patch_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\ndef patch_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: int):\n    # Replace the activations for the target neuron with activations from the cached run.\n    cached_activations = cache[hook.name]\n    activations[:, :, neuron_idx] = cached_activations[:, :, neuron_idx]\n    return activations\n\ndef patch_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: int):\n    # Replace the activations for the target attention head with activations from the cached run.\n    cached_activations = cache[hook.name]\n    activations[:, :, head_idx, :] = cached_activations[:, :, head_idx, :]\n    return activations\n\n\nfor layer in range(model.cfg.n_layers):\n    # Activation patching on heads\n    for head in range(model.cfg.n_heads):\n        hook_name = get_act_name(\"result\", layer)\n        temp_hook = lambda act, hook: patch_attn_hook(act, hook, corrupted_cache, head)\n\n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        attn_patch_results[layer, head] = logit_diff / baseline_diff\n\n    # Activation patching on MLP neurons\n    for neuron in range(model.cfg.d_mlp):\n        hook_name = get_act_name(\"mlp_out\", layer)\n        temp_hook = lambda act, hook: patch_neuron_hook(act, hook, corrupted_cache, neuron)\n        \n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        mlp_patch_results[layer, neuron] = logit_diff / baseline_diff\n\n\nbound = max(torch.max(mlp_patch_results), abs(torch.min(mlp_patch_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_results, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Causal Tracing: Corrupt -&gt; Clean\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbound = max(torch.max(attn_patch_results), abs(torch.min(attn_patch_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_patch_results, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Causal Tracing: Corrupt -&gt; Clean\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\nPatching with different inputs\n\nfrom transformer_lens import HookedTransformer, ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nclean_tokens_v2 = \"()()\"\ncorrupted_tokens_v2 = \"))()\"\n\nclean_input_v2 = tokenizer.tokenize(clean_tokens_v2)        # Balanced\ncorrupted_input_v2 = tokenizer.tokenize(corrupted_tokens_v2)    # Unbalanced\n\n# We run on the corrupted prompt with the cache so we store activations to patch in later.\ncorrupted_logits_v2, corrupted_cache_v2 = model.run_with_cache(corrupted_input_v2)\nclean_logits_v2 = model(clean_input_v2)\n\n# Get probability of overall sequence being balanced (class 1) from position 0\nclean_answer_logits_v2 = clean_logits_v2[0, 0, 1]\ncorrupted_answer_logits_v2 = corrupted_logits_v2[0, 0, 1]\nprint(f\"Balanced input score: {clean_answer_logits_v2}\")\nprint(f\"Unbalanced input score: {corrupted_answer_logits_v2}\")\n\nbaseline_diff_v2 = (corrupted_answer_logits_v2 - clean_answer_logits_v2).item()\nprint(f\"Baseline clean-corrupted logit difference: {baseline_diff_v2:.2f}\")\n\n# Patch neurons in MLP layers\nmlp_patch_results_v2 = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Patch attention heads\nattn_patch_results_v2 = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\nfor layer in range(model.cfg.n_layers):\n    # Activation patching on heads\n    for head in range(model.cfg.n_heads):\n        hook_name = get_act_name(\"result\", layer)\n        temp_hook = lambda act, hook: patch_attn_hook(act, hook, corrupted_cache_v2, head)\n\n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input_v2)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits_v2).item()\n        # Normalise result by clean and corrupted logit difference\n        attn_patch_results_v2[layer, head] = logit_diff / baseline_diff_v2\n\n    # Activation patching on MLP neurons\n    for neuron in range(model.cfg.d_mlp):\n        hook_name = get_act_name(\"mlp_out\", layer)\n        temp_hook = lambda act, hook: patch_neuron_hook(act, hook, corrupted_cache_v2, neuron)\n        \n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input_v2)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits_v2).item()\n        # Normalise result by clean and corrupted logit difference\n        mlp_patch_results_v2[layer, neuron] = logit_diff / baseline_diff_v2\n\nBalanced input score: 5.693811416625977\nUnbalanced input score: -4.950194358825684\nBaseline clean-corrupted logit difference: -10.64\n\n\n\nbound = max(torch.max(mlp_patch_results_v2), abs(torch.min(mlp_patch_results_v2)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_results_v2, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(f\"MLP Neuron Causal Tracing: Corrupt {corrupted_tokens_v2} -&gt; Clean for {clean_tokens_v2}\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\nbound = max(torch.max(attn_patch_results_v2), abs(torch.min(attn_patch_results_v2)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_patch_results_v2, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(f\"Attention Head Causal Tracing: Corrupt {corrupted_tokens_v2} -&gt; Clean for {clean_tokens_v2}\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compare difference with alternative inputs\n\nmlp_patch_diff = mlp_patch_results - mlp_patch_results_v2\n\nbound = max(torch.max(mlp_patch_diff), abs(torch.min(mlp_patch_diff)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_diff, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(f\"MLP Neuron Causal Tracing: Difference from (()( and ))()\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Combine alternative inputs\n\nmlp_patch_sum = mlp_patch_results + mlp_patch_results_v2\n\nbound = max(torch.max(mlp_patch_sum), abs(torch.min(mlp_patch_sum)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_sum, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(f\"MLP Neuron Causal Tracing: Sum of (()( and ))()\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nComparison: patching in the opposite direction (denoising)\n\nfrom transformer_lens import HookedTransformer, ActivationCache\nfrom transformer_lens.hook_points import HookPoint\n\nclean_input = tokenizer.tokenize(\"()()\")        # Balanced\ncorrupted_input = tokenizer.tokenize(\"(()(\")    # Unbalanced\n\n# We run on the clean prompt with the cache so we store activations to patch in later.\nclean_logits, clean_cache = model.run_with_cache(clean_input)\ncorrupted_logits = model(corrupted_input)\n\n# Get probability of overall sequence being balanced (class 1) from position 0\nclean_answer_logits = clean_logits[0, 0, 1]\ncorrupted_answer_logits = corrupted_logits[0, 0, 1]\nprint(f\"Balanced input score: {clean_answer_logits}\")\nprint(f\"Unbalanced input score: {corrupted_answer_logits}\")\n\nbaseline_diff = (corrupted_answer_logits - clean_answer_logits).item()\nprint(f\"Baseline clean-corrupted logit difference: {baseline_diff:.2f}\")\n\nBalanced input score: 5.693811416625977\nUnbalanced input score: -5.421151161193848\nBaseline clean-corrupted logit difference: -11.11\n\n\n\n# Patch neurons in MLP layers\nmlp_patch_denoising_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Patch attention heads\nattn_patch_denoising_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\nfor layer in range(model.cfg.n_layers):\n    # Activation patching on heads\n    for head in range(model.cfg.n_heads):\n        hook_name = get_act_name(\"result\", layer)\n        temp_hook = lambda act, hook: patch_attn_hook(act, hook, clean_cache, head)\n\n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(corrupted_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - corrupted_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        attn_patch_denoising_results[layer, head] = logit_diff / baseline_diff\n\n    # Activation patching on MLP neurons\n    for neuron in range(model.cfg.d_mlp):\n        hook_name = get_act_name(\"mlp_out\", layer)\n        temp_hook = lambda act, hook: patch_neuron_hook(act, hook, clean_cache, neuron)\n        \n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(corrupted_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - corrupted_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        mlp_patch_denoising_results[layer, neuron] = logit_diff / baseline_diff\n\n\nbound = max(torch.max(mlp_patch_denoising_results), abs(torch.min(mlp_patch_denoising_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_denoising_results, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Causal Tracing: Clean -&gt; Corrupted\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbound = max(torch.max(attn_patch_denoising_results), abs(torch.min(attn_patch_denoising_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_patch_denoising_results, cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Causal Tracing: Clean -&gt; Corrupted\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nComparison: causal tracing with IG zero baseline\nInstead of patching in activation from unbalanced input, patch in zero ablation (same baseline as integrated gradients).\n\nfrom transformer_lens import HookedTransformer, ActivationCache\nfrom transformer_lens.hook_points import HookPoint\nfrom collections import defaultdict\n\nclean_input = tokenizer.tokenize(\"()()\")        # Balanced\nclean_logits = model(clean_input)\nclean_answer_logits = clean_logits[0, 0, 1]\n\nzero_attn_cache = defaultdict(lambda: torch.zeros((1, 1, model.cfg.n_heads, 1))) # Return zero for any attention head\nzero_mlp_cache = defaultdict(lambda: torch.zeros((1, 1, model.cfg.d_mlp))) # Return zero for any MLP neuron\n\n\n# Patch neurons in MLP layers\nmlp_patch_zero_baseline_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Patch attention heads\nattn_patch_zero_baseline_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\nfor layer in range(model.cfg.n_layers):\n    # Activation patching on heads\n    for head in range(model.cfg.n_heads):\n        hook_name = get_act_name(\"result\", layer)\n        temp_hook = lambda act, hook: patch_attn_hook(act, hook, zero_attn_cache, head)\n\n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        attn_patch_zero_baseline_results[layer, head] = logit_diff\n\n    # Activation patching on MLP neurons\n    for neuron in range(model.cfg.d_mlp):\n        hook_name = get_act_name(\"mlp_out\", layer)\n        temp_hook = lambda act, hook: patch_neuron_hook(act, hook, zero_mlp_cache, neuron)\n        \n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        mlp_patch_zero_baseline_results[layer, neuron] = logit_diff\n\n\nbound = max(torch.max(mlp_patch_zero_baseline_results), abs(torch.min(mlp_patch_zero_baseline_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_zero_baseline_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Causal Tracing: Zero -&gt; Clean\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\nbound = max(torch.max(attn_patch_zero_baseline_results), abs(torch.min(attn_patch_zero_baseline_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_patch_zero_baseline_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Causal Tracing: Zero -&gt; Corrupted\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPatching with mean value of corrupted dataset\n\nimport json\nfrom toy_transformers.brackets_datasets import BracketsDataset\n\n# Get corrupted datasets\nwith open(\"toy_transformers/brackets_data.json\") as f:\n    data_tuples = json.load(f)\n    data_tuples = data_tuples[:5000]\n    data = BracketsDataset(data_tuples)\n\ncorrupted_dataset = data.toks[~data.isbal]\ncorrupted_dataset_answer_logits, corrupted_dataset_cache = model.run_with_cache(corrupted_dataset)\n\n\ndef patch_mean_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: int):\n    # Replace the activations for the target neuron with activations from the cached run.\n    cached_activations = cache[hook.name].mean(dim=0)\n    activations[:, :, neuron_idx] = cached_activations[:, neuron_idx]\n    return activations\n\ndef patch_mean_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: int):\n    # Replace the activations for the target attention head with activations from the cached run.\n    cached_activations = cache[hook.name].mean(dim=0)\n    activations[:, :, head_idx, :] = cached_activations[:, head_idx, :]\n    return activations\n\n\nclean_input = tokenizer.tokenize(\"()()\")\n# Pad token size to match corrupted_dataset size\npadding = torch.full((1, model.cfg.n_ctx - clean_input.size(1)), tokenizer.PAD_TOKEN)\nprint(clean_input.shape, padding.shape)\nclean_input = torch.cat([clean_input, padding], dim=-1)\nprint(clean_input.shape)\n\nclean_logits = model(clean_input)\nclean_answer_logits = clean_logits[0, 0, 1]\ncorrupted_dataset_answer_logits = corrupted_dataset_answer_logits.mean(dim=0)\nbaseline_mean_diff = (corrupted_answer_logits - clean_answer_logits).item()\n\n# Patch neurons in MLP layers\nmlp_patch_mean_corrupt_results = torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)\n# Patch attention heads\nattn_patch_mean_corrupt_results = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n\nfor layer in range(model.cfg.n_layers):\n    # Activation patching on heads\n    for head in range(model.cfg.n_heads):\n        hook_name = get_act_name(\"result\", layer)\n        temp_hook = lambda act, hook: patch_mean_attn_hook(act, hook, corrupted_dataset_cache, head)\n\n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        attn_patch_mean_corrupt_results[layer, head] = logit_diff / baseline_mean_diff\n\n    # Activation patching on MLP neurons\n    for neuron in range(model.cfg.d_mlp):\n        hook_name = get_act_name(\"mlp_out\", layer)\n        temp_hook = lambda act, hook: patch_mean_neuron_hook(act, hook, corrupted_dataset_cache, neuron)\n        \n        with model.hooks(fwd_hooks=[(hook_name, temp_hook)]):\n            patched_logits = model(clean_input)\n\n        patched_answer_logits = patched_logits[0, 0, 1]\n        logit_diff = (patched_answer_logits - clean_answer_logits).item()\n        # Normalise result by clean and corrupted logit difference\n        mlp_patch_mean_corrupt_results[layer, neuron] = logit_diff / baseline_mean_diff\n\ntorch.Size([1, 6]) torch.Size([1, 36])\ntorch.Size([1, 42])\n\n\n\nbound = max(torch.max(mlp_patch_mean_corrupt_results), abs(torch.min(mlp_patch_mean_corrupt_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_patch_mean_corrupt_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"MLP Neuron Causal Tracing: Mean Corrupted -&gt; Clean\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()\n\nbound = max(torch.max(attn_patch_mean_corrupt_results), abs(torch.min(attn_patch_mean_corrupt_results)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(attn_patch_mean_corrupt_results.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Attention Head Causal Tracing: Mean Corrupted -&gt; Clean\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\n\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "Integrated Gradients vs Activation Patching.html#comparative-analysis",
    "href": "Integrated Gradients vs Activation Patching.html#comparative-analysis",
    "title": "Comparing Integrated Gradients and Activation Patching",
    "section": "Comparative analysis",
    "text": "Comparative analysis\n\nmlp_ig_results = mlp_ig_results.detach()\nattn_ig_results = attn_ig_results.detach()\n\n\nCorrelation between attribution scores\n\nThere is very little linear correlation between raw attribution scores for MLP neurons from Integrated Gradients and causal tracing.\n\n\n# Plot the attribution scores against each other. Correlation: y = x.\n\nx = mlp_ig_results.flatten().numpy()\ny = mlp_patch_results.flatten().numpy()\n\nplt.figure(figsize=(6,6))\nsns.regplot(x=x, y=y)\nplt.xlabel(\"Integrated Gradients MLP Attribution Scores\")\nplt.ylabel(\"Causal Tracing MLP Attribution Scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG and causal tracing attributions for neurons: {np.corrcoef(x, y)[0, 1]}\")\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG and causal tracing attributions for neurons: 0.7712230344174876\n\n\n\nx = attn_ig_results.flatten().numpy()\ny = attn_patch_results.flatten().numpy()\n\nplt.figure(figsize=(8,6))\nsns.regplot(x=x, y=y)\nplt.xlabel(\"Integrated Gradients Attention Attribution Scores\")\nplt.ylabel(\"Causal Tracing Attention Attribution Scores\")\nplt.show()\n\nprint(f\"Correlation coefficient between IG and causal tracing attributions for attention: {np.corrcoef(x, y)[0, 1]}\")\n\n\n\n\n\n\n\n\nCorrelation coefficient between IG and causal tracing attributions for attention: 0.9761361508593569\n\n\n\n\nAgreement between attribution scores\nThe Jaccard scores for MLP neuron attribution scores per transformer layer is low.\n\ndef get_top_k_by_abs(data, k):\n    _, indices = torch.topk(data.abs(), k)\n    return indices, torch.gather(data, 1, indices)\n\ndef get_attributions_above_threshold(data, percentile):\n    threshold = torch.min(data) + percentile * (torch.max(data) - torch.min(data))\n    masked_data = torch.where(data &gt; threshold, data, 0)\n    nonzero_indices = torch.nonzero(masked_data)\n    return nonzero_indices, masked_data\n\ntop_mlp_ig_indices, top_mlp_ig_results = get_attributions_above_threshold(mlp_ig_results, 0.3)\ntop_mlp_patch_indices, top_mlp_patch_results = get_attributions_above_threshold(mlp_patch_results, 0.3)\n\nprint(len(top_mlp_ig_indices), len(top_mlp_patch_indices))\n\n49 12\n\n\n\n# top_mlp_ig_sets = [set(row.tolist()) for row in top_mlp_ig_indices]\n# top_mlp_patch_sets = [set(row.tolist()) for row in top_mlp_patch_indices]\ntop_mlp_ig_sets = set([tuple(t.tolist()) for t in top_mlp_ig_indices])\ntop_mlp_patch_sets = set([tuple(t.tolist()) for t in top_mlp_patch_indices])\n\nintersection = top_mlp_ig_sets.intersection(top_mlp_patch_sets)\nunion = top_mlp_ig_sets.union(top_mlp_patch_sets)\njaccard = len(intersection) / len(union)\n\nprint(f\"Jaccard score for MLP neurons: {jaccard}\")\n\nJaccard score for MLP neurons: 0.22\n\n\n\ndef indices_set_to_binary_matrix(set_indices, shape):\n    binary_mat = torch.zeros(shape, dtype=torch.long)\n    for i, j in set_indices:\n        binary_mat[i, j] = 1\n    return binary_mat\n\n\nbinary_mat_intersections = indices_set_to_binary_matrix(intersection, mlp_ig_results.shape)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(binary_mat_intersections, cmap=\"Greys\")\nplt.title(\"Top MLP neurons in both attribution methods\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.show()\n\n\n\n\n\n\n\n\n\ntop_mlp_ig_exclusive = top_mlp_ig_sets.difference(top_mlp_patch_sets)\n\nbinary_mat_ig_exclusive = indices_set_to_binary_matrix(top_mlp_ig_exclusive, mlp_ig_results.shape)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(binary_mat_ig_exclusive, cmap=\"Greys\")\nplt.title(\"Top MLP neurons in only integrated gradients\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.show()\n\n\n\n\n\n\n\n\n\ntop_mlp_patch_exclusive = top_mlp_patch_sets.difference(top_mlp_ig_sets)\n\nbinary_mat_patch_exclusive = indices_set_to_binary_matrix(top_mlp_patch_exclusive, mlp_patch_results.shape)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(binary_mat_patch_exclusive, cmap=\"Greys\")\nplt.title(\"Top MLP neurons in only causal tracing\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMeasuring agreement: Tukey mean-difference plot\nAssumptions: the two attribution methods have the same precision, the precision is constant and does not depend on the “true” attribution score, and the difference between the two methods is constant.\nNOTE: since the scales of measurement may be different, this may not be applicable.\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nmlp_ig_results_1d = mlp_ig_results.flatten().numpy()\nmlp_patch_results_1d = mlp_patch_results.flatten().numpy()\n\n# Mean-difference plots\n\nmean = np.mean([mlp_ig_results_1d, mlp_patch_results_1d], axis=0)\ndiff = mlp_patch_results_1d - mlp_ig_results_1d\nmd = np.mean(diff) # Mean of the difference\nsd = np.std(diff, axis=0) # Standard deviation of the difference\n\nplt.figure(figsize=(10, 6))\nsns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\nplt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\nplt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\nplt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\nplt.xlabel(\"Mean of attribution scores per neuron\")\nplt.ylabel(\"Difference (activation patching - integrated gradients) per neuron\")\nplt.title(\"Mean-difference plot of attribution scores from integrated gradients and activation patching\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Mean difference plot with scaled data\n\nscaled_mlp_ig_results_1d = MaxAbsScaler().fit_transform(mlp_ig_results_1d.reshape(-1, 1))\nscaled_mlp_patch_results_1d = MaxAbsScaler().fit_transform(mlp_patch_results_1d.reshape(-1, 1))\n\nmean = np.mean([scaled_mlp_ig_results_1d, scaled_mlp_patch_results_1d], axis=0)\ndiff = scaled_mlp_patch_results_1d - scaled_mlp_ig_results_1d\nmd = np.mean(diff) # Mean of the difference\nsd = np.std(diff, axis=0) # Standard deviation of the difference\n\nplt.figure(figsize=(10, 6))\nsns.regplot(x=mean, y=diff, fit_reg=True, scatter=True)\nplt.axhline(md, color='gray', linestyle='--', label=\"Mean difference\")\nplt.axhline(md + 1.96*sd, color='pink', linestyle='--', label=\"1.96 SD of difference\")\nplt.axhline(md - 1.96*sd, color='lightblue', linestyle='--', label=\"-1.96 SD of difference\")\nplt.xlabel(\"Mean of attribution scores per neuron\")\nplt.ylabel(\"Difference (activation patching - integrated gradients) per neuron\")\nplt.title(\"Mean-difference plot of scaled attribution scores from integrated gradients and activation patching\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nMean difference is close to zero, indicating a lack of fixed bias: methods tend to agree.\nDeviation from the mean difference increases as the average attribution score increases, indicating proportional bias. Methods tend to agree on which neurons contribute less to the output, but disagree more on neurons which are identified as important by one/both of the methods. Specifically, for larger attribution scores, integrated gradients assigns greater attribution scores to neurons than causal tracing.\nThe scaled limits of agreement (95% of the differences between attribution scores) lie within approximately -0.25 to 0.2. This is a fairly small but still noticeable range of error, given that the scaled attribution scores lie between -1 and 1.\n\n\n\nDifference in scores for attention heads\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nscaled_attn_ig_results = MaxAbsScaler().fit_transform(attn_ig_results)\nscaled_attn_patch_results = MaxAbsScaler().fit_transform(attn_patch_results)\n\ndiff_attn_results = scaled_attn_ig_results - scaled_attn_patch_results\ndiff_attn_results_abs = np.abs(scaled_attn_ig_results) - np.abs(scaled_attn_patch_results)\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(diff_attn_results, cmap=\"RdBu\", vmin=-2, vmax=2)\nplt.title(\"Difference in attributions for attention heads\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.imshow(diff_attn_results_abs, cmap=\"RdBu\", vmin=-2, vmax=2)\nplt.title(\"Difference in (absolute) attributions for attention heads\")\n\nplt.xlabel(\"Head Index\")\nplt.xticks([0,1])\nplt.ylabel(\"Layer\")\nplt.yticks([0,1,2])\n\nplt.colorbar()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nComparison to “ground truth”\nAn in-depth investigation of the balanced bracket classifer reached the following circuit hypothesis:\n\nHead 0.0 allowed the first token to attend to all tokens in the sequence uniformly, and tally up (left - right) bracket counts.\nNeurons in MLP 0 activated when (left - right) &gt; 0 and neurons in MLP 1 activated when (left - right) &lt; 0, and write boolean information about left == right.\nHead 2.0 copied information from position 1 to position 0 (which is used for the final classification).\n\nObservations about neurons in the MLP layers which have a significant impact on the vector output in the unbalanced direction (class 0):\n\nSome neurons detect when the open-proportion is greater than 1/2, e.g. neurons 1.53, 1.39, 1.8 in layer 1. There are some in layer 0 as well, such as 0.33 or 0.43. Overall these seem more common in Layer 1.\nSome neurons detect when the open-proportion is less than 1/2. For instance, neurons 0.21, and 0.7. These are much more rare in layer 1, but you can see some such as 1.50 and 1.6.\nIn layer 1 that there are many neurons that output a composed property (activate when proportions are imbalanced in either direction). As a few examples, look at 1.10 and 1.3. It’s much harder for a single neuron in layer 0 to do this by themselves, given that ReLU is monotonic and it requires the output to be a non-monotonic function of the open-paren proportion. It is possible, however, to take advantage of the layernorm before mlp0 to approximate this – 0.19 and 0.34 are good examples of this. Note, there are some neurons which appear to work in the opposite direction (e.g. 0.0). It’s unclear exactly what the function of these neurons is.\n\nWe identify the “true” important neurons in the MLP layers using neuron contribution plots (see tutorial ). Neuron contribution is defined as the dot product (similarity) between the neuron’s output vector to the residual stream, and the unbalanced direction (residual stream vector which causes highest probability for unbalanced class). Note that the tutorial only investigates the MLP neurons in the first two layers.\nUsing the “true” important neurons, measure the accuracy, precision, F1 and Jaccard score of each attribution method.\n\n# Reference \"ground truth\" important neuron indices: (layer_id, neuron_idx)\nreference_opposite_mlp_indices = [(0,0), (0,2), (0,20), (0,23), (0,31), (0,37), (0,38), (0,48), (0,54), (1,36), (1,38)]\nreference_more_open_mlp_indices = [(0,33), (0,43), (1,8), (1,31), (1,39), (1,53)]\nreference_less_open_mlp_indices = [(0,7), (0,12), (0,21), (0,50), (1,50)]\nreference_composed_mlp_indices = [(0,10), (0,19), (0,34), (1,3), (1,10)]\n\nreference_all_mlp_indices = reference_opposite_mlp_indices + reference_more_open_mlp_indices + reference_less_open_mlp_indices + reference_composed_mlp_indices\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score, confusion_matrix, ConfusionMatrixDisplay\n\ntop_mlp_ig_binary_mat = indices_set_to_binary_matrix(top_mlp_ig_indices, mlp_ig_results.shape)\ntop_mlp_patch_binary_mat = indices_set_to_binary_matrix(top_mlp_patch_indices, mlp_patch_results.shape)\nreference_all_mlp_binary_mat = torch.zeros((2, top_mlp_ig_binary_mat.shape[1]), dtype=torch.long)\nfor i, j in reference_all_mlp_indices:\n    reference_all_mlp_binary_mat[i,j] = 1.0\n\n\nreference_all_mlp_binary_1d = reference_all_mlp_binary_mat.flatten()\ntop_mlp_ig_binary_1d = torch.cat((top_mlp_ig_binary_mat[0], top_mlp_ig_binary_mat[1])) # Only include first two layers to compare to ground truth\ntop_mlp_patch_binary_1d = torch.cat((top_mlp_patch_binary_mat[0], top_mlp_patch_binary_mat[1]))\n\n\nplt.figure(figsize=(10, 8))\nplt.imshow(reference_all_mlp_binary_mat, cmap=\"Greys\")\nplt.title(\"Top MLP neurons in first two layers (ground truth)\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1])\nplt.ylabel(\"Layer\")\nplt.show()\n\n\n\n\n\n\n\n\n\nig_accuracy = accuracy_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d)\nprint(f\"Integrated gradients accuracy: {ig_accuracy}\")\n\nig_precision_0, ig_precision_1 = precision_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, average=None)\nprint(f\"Integrated gradients precision: class 0 (low contribution) {ig_precision_0}, class 1 (high contribution) {ig_precision_1}\")\n\nig_recall_0, ig_recall_1 = recall_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, average=None)\nprint(f\"Integrated gradients recall: class 0 (low contribution) {ig_recall_0}, class 1 (high contribution) {ig_recall_1}\")\n\nig_jaccard_0, ig_jaccard_1 = jaccard_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, average=None)\nprint(f\"Integrated gradients Jaccard score: class 0 (low contribution) {ig_jaccard_0}, class 1 (high contribution) {ig_jaccard_1}\")\n\n\nig_confusion_matrix = confusion_matrix(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, normalize=\"true\")\nig_cm_display = ConfusionMatrixDisplay(ig_confusion_matrix, display_labels=[\"Low contribution\", \"High contribution\"])\nig_cm_display.plot(cmap=\"Blues\")\nplt.show()\n\nIntegrated gradients accuracy: 0.5178571428571429\nIntegrated gradients precision: class 0 (low contribution) 0.746031746031746, class 1 (high contribution) 0.22448979591836735\nIntegrated gradients recall: class 0 (low contribution) 0.5529411764705883, class 1 (high contribution) 0.4074074074074074\nIntegrated gradients Jaccard score: class 0 (low contribution) 0.46534653465346537, class 1 (high contribution) 0.16923076923076924\n\n\n\n\n\n\n\n\n\n\npatch_accuracy = accuracy_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d)\nprint(f\"Causal tracing accuracy: {patch_accuracy}\")\n\npatch_precision_0, patch_precision_1 = precision_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, average=None)\nprint(f\"Causal tracing precision: class 0 (low contribution) {patch_precision_0}, class 1 (high contribution) {patch_precision_1}\")\n\npatch_recall_0, patch_recall_1 = recall_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, average=None)\nprint(f\"Causal tracing recall: class 0 (low contribution) {patch_recall_0}, class 1 (high contribution) {patch_recall_1}\")\n\npatch_jaccard_0, patch_jaccard_1 = jaccard_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, average=None)\nprint(f\"Causal tracing Jaccard score: class 0 (low contribution) {patch_jaccard_0}, class 1 (high contribution) {patch_jaccard_1}\")\n\n\npatch_confusion_matrix = confusion_matrix(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, normalize=\"true\")\npatch_cm_display = ConfusionMatrixDisplay(patch_confusion_matrix, display_labels=[\"Low contribution\", \"High contribution\"])\npatch_cm_display.plot(cmap=\"Blues\")\nplt.show()\n\nCausal tracing accuracy: 0.6517857142857143\nCausal tracing precision: class 0 (low contribution) 0.73, class 1 (high contribution) 0.0\nCausal tracing recall: class 0 (low contribution) 0.8588235294117647, class 1 (high contribution) 0.0\nCausal tracing Jaccard score: class 0 (low contribution) 0.6517857142857143, class 1 (high contribution) 0.0\n\n\n\n\n\n\n\n\n\nIntegrated gradients and causal tracing seem to perform almost identically in terms of identifying known circuit neurons. Both methods identify true negatives and false negatives most of the time."
  },
  {
    "objectID": "Integrated Gradients vs Activation Patching.html#distribution-of-activations-and-baselines",
    "href": "Integrated Gradients vs Activation Patching.html#distribution-of-activations-and-baselines",
    "title": "Comparing Integrated Gradients and Activation Patching",
    "section": "Distribution of activations and baselines",
    "text": "Distribution of activations and baselines\nOne possible reason for the discrepancy between patching and IG is that the range of activations tested may be from different distributions.\n\nQualitatively, the corrupt activations are outside of the range of zero to clean activations in areas highlighted by IG exclusively.\nRunning integrated gradients with the baselines as the corrupt activations seems to bring results closer to causal tracing, although the attribution scores still have minor disagreements.\n\n\n# If the corrupt activations for tracing are outside of the bounds for gradient attribution, measure the distance\n\ndef measure_distance_from_bound(bounds, value):\n    lower_bound = min(bounds)\n    upper_bound = max(bounds)\n    if value &lt; lower_bound:\n        return value - lower_bound\n    if value &gt; upper_bound:\n        return value - upper_bound\n    return 0\n\nclean_input = tokenizer.tokenize(\"()()\")        # Balanced\ncorrupted_input = tokenizer.tokenize(\"(()(\")    # Unbalanced\n\nclean_logits, clean_cache = model.run_with_cache(clean_input)\ncorrupted_logits, corrupted_cache = model.run_with_cache(corrupted_input)\n\nmlp_distance = torch.zeros((model.cfg.n_layers, model.cfg.d_mlp))\n\n# Get distance for MLP neurons\nfor layer in range(model.cfg.n_layers):\n    hook_name = get_act_name(\"mlp_out\", layer)\n    layer_corrupt_acts = corrupted_cache[hook_name]\n    layer_clean_acts = clean_cache[hook_name]\n    for neuron_idx in range(model.cfg.d_mlp):\n        neuron_corrupt_acts = layer_corrupt_acts[0, :, neuron_idx]\n        neuron_clean_acts = layer_clean_acts[0, :, neuron_idx]\n        # Go over each token and take the maximum distance\n        max_distance = 0\n        for i in range(neuron_clean_acts.size(-1)):\n            # The clean activations are what is used for integrated gradients\n            distance = measure_distance_from_bound(bounds=(0, neuron_clean_acts[i]), value=neuron_corrupt_acts[i])\n            max_distance = max(distance, max_distance)\n        mlp_distance[layer, neuron_idx] = max_distance\n\n\nbound = max(torch.max(mlp_distance), abs(torch.min(mlp_distance)))\n\nplt.figure(figsize=(10, 5))\nplt.imshow(mlp_distance.detach(), cmap='RdBu', vmin=-bound, vmax=bound)\nplt.title(\"Max distance of corrupt activations outside IG gradient bounds\")\nplt.xticks(np.arange(0, 56, 2))\nplt.xlabel(\"Neuron Index\")\nplt.yticks([0,1,2])\nplt.ylabel(\"Layer\")\nplt.colorbar(orientation=\"horizontal\")\nplt.show()"
  }
]