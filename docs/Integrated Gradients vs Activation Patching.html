<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Comparing Integrated Gradients and Activation Patching</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-a37c72dd2dbac68997fcdc15a3622e78.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Comparing Integrated Gradients and Activation Patching</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="set-up" class="level2">
<h2 class="anchored" data-anchor-id="set-up">Set up</h2>
<p>We load a pre-trained toy transformer which performs balanced bracket classification.</p>
<p>Important note: the final classification for whether the sequence is balanced or not comes from position 0 of the output (all other outputs are discarded).</p>
<p><img src="reference/bracket-transformer-entire-model-short.png" alt="Transformer architecture" width="50%"></p>
<div id="cell-4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> importlib</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> captum.attr <span class="im">import</span> LayerIntegratedGradients</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.utils <span class="im">import</span> get_act_name</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> ActivationCache</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> toy_transformers.toy_bracket_transformer <span class="im">as</span> tt</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>importlib.<span class="bu">reload</span>(tt)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.toy_bracket_transformer <span class="im">import</span> load_toy_bracket_transformer, test_loaded_bracket_model</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>tokenizer, model <span class="op">=</span> load_toy_bracket_transformer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>HookedTransformer(
  (embed): Embed()
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (hook_tokens): HookPoint()
  (blocks): ModuleList(
    (0-2): 3 x TransformerBlock(
      (ln1): LayerNorm(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNorm(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNorm(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
)
Loaded model</code></pre>
</div>
</div>
</section>
<section id="integrated-gradients" class="level2">
<h2 class="anchored" data-anchor-id="integrated-gradients">Integrated gradients</h2>
<p>We need to choose an appropriate baseline to calculate Integrated Gradients from. An ideal baseline input will produce a final output which is close to zero, and meaningfully represents a lack of information (see Sundararajan et al.&nbsp;2017).</p>
<p>Here we test a series of inputs which are feasible baseline inputs, and check their final classification scores. The <code>[START, PAD, END]</code> token sequence with zero patching on target components seems to be the best baseline, because it produces a final output which is consistently closest to zero, and this sequence meaningfully represents an input with no information.</p>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The prediction at the baseline should be near zero (see Sundararajan et al. 2017)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we test a series of inputs which could be a baseline, and check their final classification scores</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_balanced(x):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> model(x)[:, <span class="dv">0</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.softmax(<span class="op">-</span><span class="dv">1</span>)[:, <span class="dv">1</span>]</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>balanced_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input:"</span>, balanced_input)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predict_balanced(balanced_input), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>unbalanced_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input:"</span>, unbalanced_input)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predict_balanced(unbalanced_input), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>all_padding <span class="op">=</span> torch.full_like(balanced_input, tokenizer.PAD_TOKEN)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input:"</span>, all_padding)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predict_balanced(all_padding), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(balanced_input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>start_pad_end <span class="op">=</span> balanced_input <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input:"</span>, start_pad_end)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predict_balanced(start_pad_end), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>all_zeroes <span class="op">=</span> torch.zeros_like(balanced_input)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input:"</span>, all_zeroes)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predict_balanced(all_zeroes), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_zero_hook(activations: torch.Tensor, hook: HookPoint):</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace activations with zeroes</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> torch.zeros_like(activations)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(<span class="st">"blocks.0.hook_mlp_out"</span>, patch_zero_hook)]):</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Patch zeroes in at target component"</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(predict_balanced(balanced_input))</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(predict_balanced(unbalanced_input))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(predict_balanced(all_padding))</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(predict_balanced(start_pad_end))</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(predict_balanced(all_zeroes))</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input: tensor([[0, 3, 4, 3, 4, 2]])
tensor([1.0000], grad_fn=&lt;SelectBackward0&gt;) 

Input: tensor([[0, 3, 3, 4, 3, 2]])
tensor([1.8627e-05], grad_fn=&lt;SelectBackward0&gt;) 

Input: tensor([[1, 1, 1, 1, 1, 1]])
tensor([0.0411], grad_fn=&lt;SelectBackward0&gt;) 

Input: tensor([[0, 1, 1, 1, 1, 2]])
tensor([3.6777e-06], grad_fn=&lt;SelectBackward0&gt;) 

Input: tensor([[0, 0, 0, 0, 0, 0]])
tensor([1.5444e-05], grad_fn=&lt;SelectBackward0&gt;) 

Patch zeroes in at target component
tensor([0.0074], grad_fn=&lt;SelectBackward0&gt;)
tensor([6.3960e-05], grad_fn=&lt;SelectBackward0&gt;)
tensor([0.0172], grad_fn=&lt;SelectBackward0&gt;)
tensor([1.0772e-05], grad_fn=&lt;SelectBackward0&gt;)
tensor([1.5005e-05], grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_zero_hook(activations: torch.Tensor, hook: HookPoint):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace activations with zeroes</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> torch.zeros_like(activations)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_zero_forward_fn(x, baseline, target_layer):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.equal(x, baseline):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch zeros into target layer when baseline measurement is given</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model.run_with_hooks(x, fwd_hooks<span class="op">=</span>[(target_layer.name, patch_zero_hook)])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(x)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> output[:, <span class="dv">0</span>]</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.softmax(<span class="op">-</span><span class="dv">1</span>)[:, <span class="dv">1</span>]</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    forward_fn <span class="op">=</span> <span class="kw">lambda</span> x: patch_zero_forward_fn(x, baseline, target_layer)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    ig_embed <span class="op">=</span> LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate layer integrated gradients for class 0 (unbalanced),</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    attributions, approximation_error <span class="op">=</span> ig_embed.attribute(inputs<span class="op">=</span><span class="bu">input</span>,</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>                                                    baselines<span class="op">=</span>baseline,</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>                                                    attribute_to_layer_input<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>                                                    return_convergence_delta<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Error (delta) for </span><span class="sc">{</span>target_layer<span class="sc">.</span>name<span class="sc">}</span><span class="ss"> attribution: </span><span class="sc">{</span>approximation_error<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attributions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-10" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>attn_ig_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(<span class="bu">input</span>, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline) <span class="co"># shape [1, seq_len, d_head, d_model]</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    attn_ig_results[layer] <span class="op">=</span> score</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.blocks[layer].hook_mlp_out</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline) <span class="co"># shape [1, seq_len, d_model]</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    mlp_ig_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Error (delta) for blocks.0.attn.hook_result attribution: -0.06375384330749512

Error (delta) for blocks.0.hook_mlp_out attribution: -0.0021299123764038086

Error (delta) for blocks.1.attn.hook_result attribution: -0.12871885299682617

Error (delta) for blocks.1.hook_mlp_out attribution: -0.019762754440307617

Error (delta) for blocks.2.attn.hook_result attribution: 0.0016323328018188477

Error (delta) for blocks.2.hook_mlp_out attribution: -0.9999805688858032</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_results)))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients)"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_results)))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients)"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="comparison-unbalanced-input-sequence" class="level3">
<h3 class="anchored" data-anchor-id="comparison-unbalanced-input-sequence">Comparison: unbalanced input sequence</h3>
<p>Repeat with unbalanced sequence for comparison.</p>
<div id="cell-14" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_unbalanced_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>attn_ig_unbalanced_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(<span class="bu">input</span>, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    attn_ig_unbalanced_results[layer] <span class="op">=</span> score</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.blocks[layer].hook_mlp_out</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    mlp_ig_unbalanced_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Error (delta) for blocks.0.attn.hook_result attribution: 2.431000211799983e-05

Error (delta) for blocks.0.hook_mlp_out attribution: -7.003468454058748e-06

Error (delta) for blocks.1.attn.hook_result attribution: 2.229996425739955e-06

Error (delta) for blocks.1.hook_mlp_out attribution: -1.0247405953123234e-05

Error (delta) for blocks.2.attn.hook_result attribution: 0.0020253800321370363

Error (delta) for blocks.2.hook_mlp_out attribution: -1.7768754332792014e-05</code></pre>
</div>
</div>
<p>Gradient attribution scores for MLP neurons do not depend on whether the sequence is balanced or unbalanced. This suggests that the MLP neurons may be involved in some universal processing of information, used downstream for final output classification.</p>
<p>Gradient attribution scores for attention heads change when the sequence is unbalanced. Notably, head 2.1 contributes significantly more highly to a positive “unbalanced” classification. This suggests that head 2.1 plays a strong role in the final output. The attribution scores for the other attention heads are less strong, suggesting a weaker role.</p>
<div id="cell-16" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_unbalanced_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_unbalanced_results)))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_unbalanced_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients) - Unbalanced"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_unbalanced_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_unbalanced_results)))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_unbalanced_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients) - Unbalanced"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comparison-empty-string-attributions" class="level3">
<h3 class="anchored" data-anchor-id="comparison-empty-string-attributions">Comparison: empty string attributions</h3>
<div id="cell-19" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_empty_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>attn_ig_empty_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> tokenizer.tokenize(<span class="st">""</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(<span class="bu">input</span>, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    attn_ig_empty_results[layer] <span class="op">=</span> score</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.blocks[layer].hook_mlp_out</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    mlp_ig_empty_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Error (delta) for blocks.0.attn.hook_result attribution: 0.0

Error (delta) for blocks.0.hook_mlp_out attribution: 0.0

Error (delta) for blocks.1.attn.hook_result attribution: 0.0

Error (delta) for blocks.1.hook_mlp_out attribution: 0.0

Error (delta) for blocks.2.attn.hook_result attribution: 0.0

Error (delta) for blocks.2.hook_mlp_out attribution: 0.0</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_empty_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_empty_results)))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_empty_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients) - Empty"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-21" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_empty_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_empty_results)))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_empty_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients) - Empty"</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="average-of-integrated-gradients-attributions" class="level3">
<h3 class="anchored" data-anchor-id="average-of-integrated-gradients-attributions">Average of integrated gradients attributions</h3>
<p>Take the average of the integrated gradients scores for balanced and unbalanced inputs.</p>
<div id="cell-23" class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>mlp_ig_sum_results <span class="op">=</span> (mlp_ig_results <span class="op">+</span> mlp_ig_unbalanced_results) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_sum_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_sum_results)))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_sum_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients) - Summed"</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="baseline-as-corrupted-activations" class="level3">
<h3 class="anchored" data-anchor-id="baseline-as-corrupted-activations">Baseline as corrupted activations</h3>
<p>Instead of using zero as the baseline activation, use the activation from the corrupt input in causal tracing.</p>
<div id="cell-25" class="cell" data-execution_count="222">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_from_layer_fn(x, original_input, prev_layer):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Instead of returning the current activations, return the given target layer outputs</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model.run_with_hooks(</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        original_input,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        fwd_hooks<span class="op">=</span>[(prev_layer.name, <span class="kw">lambda</span> act, hook: x)]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> output[:, <span class="dv">0</span>]</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.softmax(<span class="op">-</span><span class="dv">1</span>)[:, <span class="dv">1</span>]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer):</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take the model starting from the target layer</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    forward_fn <span class="op">=</span> <span class="kw">lambda</span> x: run_from_layer_fn(x, original_input, prev_layer)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    ig_embed <span class="op">=</span> LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    attributions, approximation_error <span class="op">=</span> ig_embed.attribute(inputs<span class="op">=</span>layer_input,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>                                                    baselines<span class="op">=</span>layer_baseline, </span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>                                                    attribute_to_layer_input<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>                                                    return_convergence_delta<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Error (delta) for </span><span class="sc">{</span>target_layer<span class="sc">.</span>name<span class="sc">}</span><span class="ss"> attribution: </span><span class="sc">{</span>approximation_error<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attributions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell" data-execution_count="229">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_corrupt_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>attn_ig_corrupt_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>_, clean_cache <span class="op">=</span> model.run_with_cache(clean_input)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># mask = np.isin(input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># corrupted_input = input * mask + tokenizer.PAD_TOKEN * (1 - mask)</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>_, corrupted_cache <span class="op">=</span> model.run_with_cache(corrupted_input)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"z"</span>, layer)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> clean_cache[prev_layer_hook]</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> corrupted_cache[prev_layer_hook]</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(run_from_layer_fn(layer_corrupt_input, clean_input, prev_layer))</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_head, d_model]</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    attn_ig_corrupt_baseline_results[layer] <span class="op">=</span> score</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"post"</span>, layer)</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> clean_cache[prev_layer_hook]</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> corrupted_cache[prev_layer_hook]</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_model]</span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>    mlp_ig_corrupt_baseline_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1.4471e-05], grad_fn=&lt;SelectBackward0&gt;)

Error (delta) for blocks.0.attn.hook_result attribution: -0.5329379439353943

Error (delta) for blocks.0.hook_mlp_out attribution: -0.010210990905761719
tensor([0.0496], grad_fn=&lt;SelectBackward0&gt;)

Error (delta) for blocks.1.attn.hook_result attribution: -0.0002592802047729492

Error (delta) for blocks.1.hook_mlp_out attribution: 2.187490463256836e-05
tensor([8.8178e-06], grad_fn=&lt;SelectBackward0&gt;)

Error (delta) for blocks.2.attn.hook_result attribution: -0.00015395879745483398

Error (delta) for blocks.2.hook_mlp_out attribution: 2.0354718799353577e-08</code></pre>
</div>
</div>
<div id="cell-27" class="cell" data-execution_count="230">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_corrupt_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_corrupt_baseline_results)))</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_corrupt_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients): Corrupt Baseline"</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_corrupt_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_corrupt_baseline_results)))</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_corrupt_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients): Corrupt Baseline"</span>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="causal-tracing" class="level2">
<h2 class="anchored" data-anchor-id="causal-tracing">Causal tracing</h2>
<p>Noising (a corrupt → clean patch) shows whether the patched activations were <em>necessary</em> to maintain the model behaviour. Therefore we patch corrupted activations into a clean run.</p>
<div id="cell-30" class="cell" data-execution_count="256">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)    <span class="co"># Unbalanced</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We run on the corrupted prompt with the cache so we store activations to patch in later.</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>corrupted_logits, corrupted_cache <span class="op">=</span> model.run_with_cache(corrupted_input)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>clean_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get probability of overall sequence being balanced (class 1) from position 0</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>corrupted_answer_logits <span class="op">=</span> corrupted_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Balanced input score: </span><span class="sc">{</span>clean_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbalanced input score: </span><span class="sc">{</span>corrupted_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>baseline_diff <span class="op">=</span> (corrupted_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline clean-corrupted logit difference: </span><span class="sc">{</span>baseline_diff<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced input score: 5.693811416625977
Unbalanced input score: -5.421151161193848
Baseline clean-corrupted logit difference: -11.11</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="257">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>mlp_patch_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>attn_patch_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: <span class="bu">int</span>):</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target neuron with activations from the cached run.</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    activations[:, :, neuron_idx] <span class="op">=</span> cached_activations[:, :, neuron_idx]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: <span class="bu">int</span>):</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target attention head with activations from the cached run.</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name]</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    activations[:, :, head_idx, :] <span class="op">=</span> cached_activations[:, :, head_idx, :]</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, corrupted_cache, head)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>        attn_patch_results[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, corrupted_cache, neuron)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a>        mlp_patch_results[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-32" class="cell" data-execution_count="163">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_results)))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Corrupt -&gt; Clean"</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-33" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_results)))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Corrupt -&gt; Clean"</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="patching-with-different-inputs" class="level3">
<h3 class="anchored" data-anchor-id="patching-with-different-inputs">Patching with different inputs</h3>
<div id="cell-35" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>clean_tokens_v2 <span class="op">=</span> <span class="st">"()()"</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>corrupted_tokens_v2 <span class="op">=</span> <span class="st">"))()"</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>clean_input_v2 <span class="op">=</span> tokenizer.tokenize(clean_tokens_v2)        <span class="co"># Balanced</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>corrupted_input_v2 <span class="op">=</span> tokenizer.tokenize(corrupted_tokens_v2)    <span class="co"># Unbalanced</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We run on the corrupted prompt with the cache so we store activations to patch in later.</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>corrupted_logits_v2, corrupted_cache_v2 <span class="op">=</span> model.run_with_cache(corrupted_input_v2)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>clean_logits_v2 <span class="op">=</span> model(clean_input_v2)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Get probability of overall sequence being balanced (class 1) from position 0</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>clean_answer_logits_v2 <span class="op">=</span> clean_logits_v2[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>corrupted_answer_logits_v2 <span class="op">=</span> corrupted_logits_v2[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Balanced input score: </span><span class="sc">{</span>clean_answer_logits_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbalanced input score: </span><span class="sc">{</span>corrupted_answer_logits_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>baseline_diff_v2 <span class="op">=</span> (corrupted_answer_logits_v2 <span class="op">-</span> clean_answer_logits_v2).item()</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline clean-corrupted logit difference: </span><span class="sc">{</span>baseline_diff_v2<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>mlp_patch_results_v2 <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>attn_patch_results_v2 <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, corrupted_cache_v2, head)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input_v2)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits_v2).item()</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>        attn_patch_results_v2[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff_v2</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, corrupted_cache_v2, neuron)</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input_v2)</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits_v2).item()</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>        mlp_patch_results_v2[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff_v2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced input score: 5.693811416625977
Unbalanced input score: -4.950194358825684
Baseline clean-corrupted logit difference: -10.64</code></pre>
</div>
</div>
<div id="cell-36" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_results_v2), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_results_v2)))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_results_v2, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"MLP Neuron Causal Tracing: Corrupt </span><span class="sc">{</span>corrupted_tokens_v2<span class="sc">}</span><span class="ss"> -&gt; Clean for </span><span class="sc">{</span>clean_tokens_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_results_v2), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_results_v2)))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_results_v2, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Attention Head Causal Tracing: Corrupt </span><span class="sc">{</span>corrupted_tokens_v2<span class="sc">}</span><span class="ss"> -&gt; Clean for </span><span class="sc">{</span>clean_tokens_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-37" class="cell" data-execution_count="140">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare difference with alternative inputs</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>mlp_patch_diff <span class="op">=</span> mlp_patch_results <span class="op">-</span> mlp_patch_results_v2</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_diff), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_diff)))</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_diff, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"MLP Neuron Causal Tracing: Difference from (()( and ))()"</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-38" class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine alternative inputs</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>mlp_patch_sum <span class="op">=</span> mlp_patch_results <span class="op">+</span> mlp_patch_results_v2</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_sum), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_sum)))</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_sum, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"MLP Neuron Causal Tracing: Sum of (()( and ))()"</span>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comparison-patching-in-the-opposite-direction-denoising" class="level3">
<h3 class="anchored" data-anchor-id="comparison-patching-in-the-opposite-direction-denoising">Comparison: patching in the opposite direction (denoising)</h3>
<div id="cell-40" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)    <span class="co"># Unbalanced</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We run on the clean prompt with the cache so we store activations to patch in later.</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>clean_logits, clean_cache <span class="op">=</span> model.run_with_cache(clean_input)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>corrupted_logits <span class="op">=</span> model(corrupted_input)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get probability of overall sequence being balanced (class 1) from position 0</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>corrupted_answer_logits <span class="op">=</span> corrupted_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Balanced input score: </span><span class="sc">{</span>clean_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbalanced input score: </span><span class="sc">{</span>corrupted_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>baseline_diff <span class="op">=</span> (corrupted_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline clean-corrupted logit difference: </span><span class="sc">{</span>baseline_diff<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced input score: 5.693811416625977
Unbalanced input score: -5.421151161193848
Baseline clean-corrupted logit difference: -11.11</code></pre>
</div>
</div>
<div id="cell-41" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>mlp_patch_denoising_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>attn_patch_denoising_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, clean_cache, head)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(corrupted_input)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> corrupted_answer_logits).item()</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>        attn_patch_denoising_results[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, clean_cache, neuron)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(corrupted_input)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> corrupted_answer_logits).item()</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>        mlp_patch_denoising_results[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-42" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_denoising_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_denoising_results)))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_denoising_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Clean -&gt; Corrupted"</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-43" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_denoising_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_denoising_results)))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_denoising_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Clean -&gt; Corrupted"</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comparison-causal-tracing-with-ig-zero-baseline" class="level3">
<h3 class="anchored" data-anchor-id="comparison-causal-tracing-with-ig-zero-baseline">Comparison: causal tracing with IG zero baseline</h3>
<p>Instead of patching in activation from unbalanced input, patch in zero ablation (same baseline as integrated gradients).</p>
<div id="cell-45" class="cell" data-execution_count="231">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>clean_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>zero_attn_cache <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>, model.cfg.n_heads, <span class="dv">1</span>))) <span class="co"># Return zero for any attention head</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>zero_mlp_cache <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>, model.cfg.d_mlp))) <span class="co"># Return zero for any MLP neuron</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-46" class="cell" data-execution_count="232">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>mlp_patch_zero_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>attn_patch_zero_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, zero_attn_cache, head)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        attn_patch_zero_baseline_results[layer, head] <span class="op">=</span> logit_diff</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, zero_mlp_cache, neuron)</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>        mlp_patch_zero_baseline_results[layer, neuron] <span class="op">=</span> logit_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-47" class="cell" data-execution_count="233">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_zero_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_zero_baseline_results)))</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_zero_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Zero -&gt; Clean"</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_zero_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_zero_baseline_results)))</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_zero_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Zero -&gt; Corrupted"</span>)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-33-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-33-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="patching-with-mean-value-of-corrupted-dataset" class="level3">
<h3 class="anchored" data-anchor-id="patching-with-mean-value-of-corrupted-dataset">Patching with mean value of corrupted dataset</h3>
<div id="cell-49" class="cell" data-execution_count="239">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.brackets_datasets <span class="im">import</span> BracketsDataset</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get corrupted datasets</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"toy_transformers/brackets_data.json"</span>) <span class="im">as</span> f:</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> json.load(f)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> data_tuples[:<span class="dv">5000</span>]</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> BracketsDataset(data_tuples)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>corrupted_dataset <span class="op">=</span> data.toks[<span class="op">~</span>data.isbal]</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>corrupted_dataset_answer_logits, corrupted_dataset_cache <span class="op">=</span> model.run_with_cache(corrupted_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-50" class="cell" data-execution_count="245">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_mean_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: <span class="bu">int</span>):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target neuron with activations from the cached run.</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    activations[:, :, neuron_idx] <span class="op">=</span> cached_activations[:, neuron_idx]</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_mean_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: <span class="bu">int</span>):</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target attention head with activations from the cached run.</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    activations[:, :, head_idx, :] <span class="op">=</span> cached_activations[:, head_idx, :]</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-51" class="cell" data-execution_count="254">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad token size to match corrupted_dataset size</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>padding <span class="op">=</span> torch.full((<span class="dv">1</span>, model.cfg.n_ctx <span class="op">-</span> clean_input.size(<span class="dv">1</span>)), tokenizer.PAD_TOKEN)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clean_input.shape, padding.shape)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> torch.cat([clean_input, padding], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clean_input.shape)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>clean_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>corrupted_dataset_answer_logits <span class="op">=</span> corrupted_dataset_answer_logits.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>baseline_mean_diff <span class="op">=</span> (corrupted_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>mlp_patch_mean_corrupt_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>attn_patch_mean_corrupt_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_mean_attn_hook(act, hook, corrupted_dataset_cache, head)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>        attn_patch_mean_corrupt_results[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_mean_diff</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_mean_neuron_hook(act, hook, corrupted_dataset_cache, neuron)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>        mlp_patch_mean_corrupt_results[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_mean_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 6]) torch.Size([1, 36])
torch.Size([1, 42])</code></pre>
</div>
</div>
<div id="cell-52" class="cell" data-execution_count="255">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_mean_corrupt_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_mean_corrupt_results)))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_mean_corrupt_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Mean Corrupted -&gt; Clean"</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_mean_corrupt_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_mean_corrupt_results)))</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_mean_corrupt_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Mean Corrupted -&gt; Clean"</span>)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-37-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-37-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="comparative-analysis" class="level2">
<h2 class="anchored" data-anchor-id="comparative-analysis">Comparative analysis</h2>
<div id="cell-54" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>mlp_ig_results <span class="op">=</span> mlp_ig_results.detach()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>attn_ig_results <span class="op">=</span> attn_ig_results.detach()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="correlation-between-attribution-scores" class="level3">
<h3 class="anchored" data-anchor-id="correlation-between-attribution-scores">Correlation between attribution scores</h3>
<ul>
<li>There is very little linear correlation between raw attribution scores for MLP neurons from Integrated Gradients and causal tracing.</li>
</ul>
<div id="cell-56" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the attribution scores against each other. Correlation: y = x.</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> mlp_ig_results.flatten().numpy()</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> mlp_patch_results.flatten().numpy()</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x, y<span class="op">=</span>y)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Integrated Gradients MLP Attribution Scores"</span>)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Causal Tracing MLP Attribution Scores"</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation coefficient between IG and causal tracing attributions for neurons: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-39-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation coefficient between IG and causal tracing attributions for neurons: 0.7712230344174876</code></pre>
</div>
</div>
<div id="cell-57" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> attn_ig_results.flatten().numpy()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> attn_patch_results.flatten().numpy()</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x, y<span class="op">=</span>y)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Integrated Gradients Attention Attribution Scores"</span>)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Causal Tracing Attention Attribution Scores"</span>)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation coefficient between IG and causal tracing attributions for attention: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-40-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation coefficient between IG and causal tracing attributions for attention: 0.9761361508593569</code></pre>
</div>
</div>
</section>
<section id="agreement-between-attribution-scores" class="level3">
<h3 class="anchored" data-anchor-id="agreement-between-attribution-scores">Agreement between attribution scores</h3>
<p>The Jaccard scores for MLP neuron attribution scores per transformer layer is low.</p>
<div id="cell-59" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_top_k_by_abs(data, k):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    _, indices <span class="op">=</span> torch.topk(data.<span class="bu">abs</span>(), k)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> indices, torch.gather(data, <span class="dv">1</span>, indices)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_attributions_above_threshold(data, percentile):</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    threshold <span class="op">=</span> torch.<span class="bu">min</span>(data) <span class="op">+</span> percentile <span class="op">*</span> (torch.<span class="bu">max</span>(data) <span class="op">-</span> torch.<span class="bu">min</span>(data))</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    masked_data <span class="op">=</span> torch.where(data <span class="op">&gt;</span> threshold, data, <span class="dv">0</span>)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    nonzero_indices <span class="op">=</span> torch.nonzero(masked_data)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nonzero_indices, masked_data</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_indices, top_mlp_ig_results <span class="op">=</span> get_attributions_above_threshold(mlp_ig_results, <span class="fl">0.3</span>)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_indices, top_mlp_patch_results <span class="op">=</span> get_attributions_above_threshold(mlp_patch_results, <span class="fl">0.3</span>)</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(top_mlp_ig_indices), <span class="bu">len</span>(top_mlp_patch_indices))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>49 12</code></pre>
</div>
</div>
<div id="cell-60" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># top_mlp_ig_sets = [set(row.tolist()) for row in top_mlp_ig_indices]</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co"># top_mlp_patch_sets = [set(row.tolist()) for row in top_mlp_patch_indices]</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_sets <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_ig_indices])</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_sets <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_patch_indices])</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>intersection <span class="op">=</span> top_mlp_ig_sets.intersection(top_mlp_patch_sets)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>union <span class="op">=</span> top_mlp_ig_sets.union(top_mlp_patch_sets)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>jaccard <span class="op">=</span> <span class="bu">len</span>(intersection) <span class="op">/</span> <span class="bu">len</span>(union)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Jaccard score for MLP neurons: </span><span class="sc">{</span>jaccard<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Jaccard score for MLP neurons: 0.22</code></pre>
</div>
</div>
<div id="cell-61" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> indices_set_to_binary_matrix(set_indices, shape):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    binary_mat <span class="op">=</span> torch.zeros(shape, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, j <span class="kw">in</span> set_indices:</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        binary_mat[i, j] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> binary_mat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-62" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>binary_mat_intersections <span class="op">=</span> indices_set_to_binary_matrix(intersection, mlp_ig_results.shape)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(binary_mat_intersections, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in both attribution methods"</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-44-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-63" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_exclusive <span class="op">=</span> top_mlp_ig_sets.difference(top_mlp_patch_sets)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>binary_mat_ig_exclusive <span class="op">=</span> indices_set_to_binary_matrix(top_mlp_ig_exclusive, mlp_ig_results.shape)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(binary_mat_ig_exclusive, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in only integrated gradients"</span>)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-45-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-64" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_exclusive <span class="op">=</span> top_mlp_patch_sets.difference(top_mlp_ig_sets)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>binary_mat_patch_exclusive <span class="op">=</span> indices_set_to_binary_matrix(top_mlp_patch_exclusive, mlp_patch_results.shape)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(binary_mat_patch_exclusive, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in only causal tracing"</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-46-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="measuring-agreement-tukey-mean-difference-plot" class="level3">
<h3 class="anchored" data-anchor-id="measuring-agreement-tukey-mean-difference-plot">Measuring agreement: Tukey mean-difference plot</h3>
<p>Assumptions: the two attribution methods have the same precision, the precision is constant and does not depend on the “true” attribution score, and the difference between the two methods is constant.</p>
<p>NOTE: since the scales of measurement may be different, this may not be applicable.</p>
<div id="cell-66" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MaxAbsScaler</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>mlp_ig_results_1d <span class="op">=</span> mlp_ig_results.flatten().numpy()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>mlp_patch_results_1d <span class="op">=</span> mlp_patch_results.flatten().numpy()</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean-difference plots</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean([mlp_ig_results_1d, mlp_patch_results_1d], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> mlp_patch_results_1d <span class="op">-</span> mlp_ig_results_1d</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>md <span class="op">=</span> np.mean(diff) <span class="co"># Mean of the difference</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> np.std(diff, axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># Standard deviation of the difference</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>mean, y<span class="op">=</span>diff, fit_reg<span class="op">=</span><span class="va">True</span>, scatter<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>plt.axhline(md, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Mean difference"</span>)</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'pink'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"1.96 SD of difference"</span>)</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">-</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'lightblue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"-1.96 SD of difference"</span>)</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Mean of attribution scores per neuron"</span>)</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Difference (activation patching - integrated gradients) per neuron"</span>)</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mean-difference plot of attribution scores from integrated gradients and activation patching"</span>)</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-47-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-67" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean difference plot with scaled data</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>scaled_mlp_ig_results_1d <span class="op">=</span> MaxAbsScaler().fit_transform(mlp_ig_results_1d.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>scaled_mlp_patch_results_1d <span class="op">=</span> MaxAbsScaler().fit_transform(mlp_patch_results_1d.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean([scaled_mlp_ig_results_1d, scaled_mlp_patch_results_1d], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> scaled_mlp_patch_results_1d <span class="op">-</span> scaled_mlp_ig_results_1d</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>md <span class="op">=</span> np.mean(diff) <span class="co"># Mean of the difference</span></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> np.std(diff, axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># Standard deviation of the difference</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>mean, y<span class="op">=</span>diff, fit_reg<span class="op">=</span><span class="va">True</span>, scatter<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>plt.axhline(md, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Mean difference"</span>)</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'pink'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"1.96 SD of difference"</span>)</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">-</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'lightblue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"-1.96 SD of difference"</span>)</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Mean of attribution scores per neuron"</span>)</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Difference (activation patching - integrated gradients) per neuron"</span>)</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mean-difference plot of scaled attribution scores from integrated gradients and activation patching"</span>)</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>Mean difference is close to zero, indicating a lack of fixed bias: methods tend to agree.</li>
<li>Deviation from the mean difference increases as the average attribution score increases, indicating proportional bias. Methods tend to agree on which neurons contribute less to the output, but disagree more on neurons which are identified as important by one/both of the methods. Specifically, for larger attribution scores, integrated gradients assigns greater attribution scores to neurons than causal tracing.</li>
<li>The scaled limits of agreement (95% of the differences between attribution scores) lie within approximately -0.25 to 0.2. This is a fairly small but still noticeable range of error, given that the scaled attribution scores lie between -1 and 1.</li>
</ul>
</section>
<section id="difference-in-scores-for-attention-heads" class="level3">
<h3 class="anchored" data-anchor-id="difference-in-scores-for-attention-heads">Difference in scores for attention heads</h3>
<div id="cell-70" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MaxAbsScaler</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>scaled_attn_ig_results <span class="op">=</span> MaxAbsScaler().fit_transform(attn_ig_results)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>scaled_attn_patch_results <span class="op">=</span> MaxAbsScaler().fit_transform(attn_patch_results)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>diff_attn_results <span class="op">=</span> scaled_attn_ig_results <span class="op">-</span> scaled_attn_patch_results</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>diff_attn_results_abs <span class="op">=</span> np.<span class="bu">abs</span>(scaled_attn_ig_results) <span class="op">-</span> np.<span class="bu">abs</span>(scaled_attn_patch_results)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(diff_attn_results, cmap<span class="op">=</span><span class="st">"RdBu"</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Difference in attributions for attention heads"</span>)</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(diff_attn_results_abs, cmap<span class="op">=</span><span class="st">"RdBu"</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Difference in (absolute) attributions for attention heads"</span>)</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-49-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comparison-to-ground-truth" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-ground-truth">Comparison to “ground truth”</h3>
<p>An in-depth investigation of the balanced bracket classifer reached the following circuit hypothesis:</p>
<ul>
<li>Head 0.0 allowed the first token to attend to all tokens in the sequence uniformly, and tally up <code>(left - right)</code> bracket counts.</li>
<li>Neurons in MLP 0 activated when <code>(left - right) &gt; 0</code> and neurons in MLP 1 activated when <code>(left - right) &lt; 0</code>, and write boolean information about <code>left == right</code>.</li>
<li>Head 2.0 copied information from position 1 to position 0 (which is used for the final classification).</li>
</ul>
<p>Observations about neurons in the MLP layers which have a significant impact on the vector output in the unbalanced direction (class 0):</p>
<ul>
<li>Some neurons detect when the open-proportion is greater than 1/2, e.g.&nbsp;neurons 1.53, 1.39, 1.8 in layer 1. There are some in layer 0 as well, such as 0.33 or 0.43. Overall these seem more common in Layer 1.</li>
<li>Some neurons detect when the open-proportion is less than 1/2. For instance, neurons 0.21, and 0.7. These are much more rare in layer 1, but you can see some such as 1.50 and 1.6.</li>
<li>In layer 1 that there are many neurons that output a composed property (activate when proportions are imbalanced in either direction). As a few examples, look at 1.10 and 1.3. It’s much harder for a single neuron in layer 0 to do this by themselves, given that ReLU is monotonic and it requires the output to be a non-monotonic function of the open-paren proportion. It is possible, however, to take advantage of the layernorm before mlp0 to approximate this – 0.19 and 0.34 are good examples of this. Note, there are some neurons which appear to work in the opposite direction (e.g.&nbsp;0.0). It’s unclear exactly what the function of these neurons is.</li>
</ul>
<p>We identify the “true” important neurons in the MLP layers using neuron contribution plots (see tutorial <img src="reference/[1_5_1]_Balanced_Bracket_Classifier_(solutions).ipynb" class="img-fluid" alt="Balanced Bracket Classifier section 3.1">). Neuron contribution is defined as the dot product (similarity) between the neuron’s output vector to the residual stream, and the unbalanced direction (residual stream vector which causes highest probability for unbalanced class). Note that the tutorial only investigates the MLP neurons in the first two layers.</p>
<p>Using the “true” important neurons, measure the accuracy, precision, F1 and Jaccard score of each attribution method.</p>
<div id="cell-74" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reference "ground truth" important neuron indices: (layer_id, neuron_idx)</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>reference_opposite_mlp_indices <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">0</span>,<span class="dv">2</span>), (<span class="dv">0</span>,<span class="dv">20</span>), (<span class="dv">0</span>,<span class="dv">23</span>), (<span class="dv">0</span>,<span class="dv">31</span>), (<span class="dv">0</span>,<span class="dv">37</span>), (<span class="dv">0</span>,<span class="dv">38</span>), (<span class="dv">0</span>,<span class="dv">48</span>), (<span class="dv">0</span>,<span class="dv">54</span>), (<span class="dv">1</span>,<span class="dv">36</span>), (<span class="dv">1</span>,<span class="dv">38</span>)]</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>reference_more_open_mlp_indices <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">33</span>), (<span class="dv">0</span>,<span class="dv">43</span>), (<span class="dv">1</span>,<span class="dv">8</span>), (<span class="dv">1</span>,<span class="dv">31</span>), (<span class="dv">1</span>,<span class="dv">39</span>), (<span class="dv">1</span>,<span class="dv">53</span>)]</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>reference_less_open_mlp_indices <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">7</span>), (<span class="dv">0</span>,<span class="dv">12</span>), (<span class="dv">0</span>,<span class="dv">21</span>), (<span class="dv">0</span>,<span class="dv">50</span>), (<span class="dv">1</span>,<span class="dv">50</span>)]</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>reference_composed_mlp_indices <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">10</span>), (<span class="dv">0</span>,<span class="dv">19</span>), (<span class="dv">0</span>,<span class="dv">34</span>), (<span class="dv">1</span>,<span class="dv">3</span>), (<span class="dv">1</span>,<span class="dv">10</span>)]</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>reference_all_mlp_indices <span class="op">=</span> reference_opposite_mlp_indices <span class="op">+</span> reference_more_open_mlp_indices <span class="op">+</span> reference_less_open_mlp_indices <span class="op">+</span> reference_composed_mlp_indices</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-75" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, jaccard_score, confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_binary_mat <span class="op">=</span> indices_set_to_binary_matrix(top_mlp_ig_indices, mlp_ig_results.shape)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_binary_mat <span class="op">=</span> indices_set_to_binary_matrix(top_mlp_patch_indices, mlp_patch_results.shape)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>reference_all_mlp_binary_mat <span class="op">=</span> torch.zeros((<span class="dv">2</span>, top_mlp_ig_binary_mat.shape[<span class="dv">1</span>]), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, j <span class="kw">in</span> reference_all_mlp_indices:</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    reference_all_mlp_binary_mat[i,j] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>reference_all_mlp_binary_1d <span class="op">=</span> reference_all_mlp_binary_mat.flatten()</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_binary_1d <span class="op">=</span> torch.cat((top_mlp_ig_binary_mat[<span class="dv">0</span>], top_mlp_ig_binary_mat[<span class="dv">1</span>])) <span class="co"># Only include first two layers to compare to ground truth</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_binary_1d <span class="op">=</span> torch.cat((top_mlp_patch_binary_mat[<span class="dv">0</span>], top_mlp_patch_binary_mat[<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-76" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(reference_all_mlp_binary_mat, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in first two layers (ground truth)"</span>)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-77" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>ig_accuracy <span class="op">=</span> accuracy_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Integrated gradients accuracy: </span><span class="sc">{</span>ig_accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>ig_precision_0, ig_precision_1 <span class="op">=</span> precision_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Integrated gradients precision: class 0 (low contribution) </span><span class="sc">{</span>ig_precision_0<span class="sc">}</span><span class="ss">, class 1 (high contribution) </span><span class="sc">{</span>ig_precision_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>ig_recall_0, ig_recall_1 <span class="op">=</span> recall_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Integrated gradients recall: class 0 (low contribution) </span><span class="sc">{</span>ig_recall_0<span class="sc">}</span><span class="ss">, class 1 (high contribution) </span><span class="sc">{</span>ig_recall_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>ig_jaccard_0, ig_jaccard_1 <span class="op">=</span> jaccard_score(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Integrated gradients Jaccard score: class 0 (low contribution) </span><span class="sc">{</span>ig_jaccard_0<span class="sc">}</span><span class="ss">, class 1 (high contribution) </span><span class="sc">{</span>ig_jaccard_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>ig_confusion_matrix <span class="op">=</span> confusion_matrix(reference_all_mlp_binary_1d, top_mlp_ig_binary_1d, normalize<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>ig_cm_display <span class="op">=</span> ConfusionMatrixDisplay(ig_confusion_matrix, display_labels<span class="op">=</span>[<span class="st">"Low contribution"</span>, <span class="st">"High contribution"</span>])</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>ig_cm_display.plot(cmap<span class="op">=</span><span class="st">"Blues"</span>)</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Integrated gradients accuracy: 0.5178571428571429
Integrated gradients precision: class 0 (low contribution) 0.746031746031746, class 1 (high contribution) 0.22448979591836735
Integrated gradients recall: class 0 (low contribution) 0.5529411764705883, class 1 (high contribution) 0.4074074074074074
Integrated gradients Jaccard score: class 0 (low contribution) 0.46534653465346537, class 1 (high contribution) 0.16923076923076924</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-53-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-78" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>patch_accuracy <span class="op">=</span> accuracy_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Causal tracing accuracy: </span><span class="sc">{</span>patch_accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>patch_precision_0, patch_precision_1 <span class="op">=</span> precision_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Causal tracing precision: class 0 (low contribution) </span><span class="sc">{</span>patch_precision_0<span class="sc">}</span><span class="ss">, class 1 (high contribution) </span><span class="sc">{</span>patch_precision_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>patch_recall_0, patch_recall_1 <span class="op">=</span> recall_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Causal tracing recall: class 0 (low contribution) </span><span class="sc">{</span>patch_recall_0<span class="sc">}</span><span class="ss">, class 1 (high contribution) </span><span class="sc">{</span>patch_recall_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>patch_jaccard_0, patch_jaccard_1 <span class="op">=</span> jaccard_score(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Causal tracing Jaccard score: class 0 (low contribution) </span><span class="sc">{</span>patch_jaccard_0<span class="sc">}</span><span class="ss">, class 1 (high contribution) </span><span class="sc">{</span>patch_jaccard_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>patch_confusion_matrix <span class="op">=</span> confusion_matrix(reference_all_mlp_binary_1d, top_mlp_patch_binary_1d, normalize<span class="op">=</span><span class="st">"true"</span>)</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>patch_cm_display <span class="op">=</span> ConfusionMatrixDisplay(patch_confusion_matrix, display_labels<span class="op">=</span>[<span class="st">"Low contribution"</span>, <span class="st">"High contribution"</span>])</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>patch_cm_display.plot(cmap<span class="op">=</span><span class="st">"Blues"</span>)</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Causal tracing accuracy: 0.6517857142857143
Causal tracing precision: class 0 (low contribution) 0.73, class 1 (high contribution) 0.0
Causal tracing recall: class 0 (low contribution) 0.8588235294117647, class 1 (high contribution) 0.0
Causal tracing Jaccard score: class 0 (low contribution) 0.6517857142857143, class 1 (high contribution) 0.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-54-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Integrated gradients and causal tracing seem to perform almost identically in terms of identifying known circuit neurons. Both methods identify true negatives and false negatives most of the time.</p>
</section>
</section>
<section id="distribution-of-activations-and-baselines" class="level2">
<h2 class="anchored" data-anchor-id="distribution-of-activations-and-baselines">Distribution of activations and baselines</h2>
<p>One possible reason for the discrepancy between patching and IG is that the range of activations tested may be from different distributions.</p>
<ul>
<li>Qualitatively, the corrupt activations are outside of the range of zero to clean activations in areas highlighted by IG exclusively.</li>
<li>Running integrated gradients with the baselines as the corrupt activations seems to bring results closer to causal tracing, although the attribution scores still have minor disagreements.</li>
</ul>
<div id="cell-81" class="cell" data-execution_count="159">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># If the corrupt activations for tracing are outside of the bounds for gradient attribution, measure the distance</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> measure_distance_from_bound(bounds, value):</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    lower_bound <span class="op">=</span> <span class="bu">min</span>(bounds)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>    upper_bound <span class="op">=</span> <span class="bu">max</span>(bounds)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> value <span class="op">&lt;</span> lower_bound:</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> value <span class="op">-</span> lower_bound</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> value <span class="op">&gt;</span> upper_bound:</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> value <span class="op">-</span> upper_bound</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)    <span class="co"># Unbalanced</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>clean_logits, clean_cache <span class="op">=</span> model.run_with_cache(clean_input)</span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>corrupted_logits, corrupted_cache <span class="op">=</span> model.run_with_cache(corrupted_input)</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a>mlp_distance <span class="op">=</span> torch.zeros((model.cfg.n_layers, model.cfg.d_mlp))</span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Get distance for MLP neurons</span></span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_acts <span class="op">=</span> corrupted_cache[hook_name]</span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a>    layer_clean_acts <span class="op">=</span> clean_cache[hook_name]</span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron_idx <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a>        neuron_corrupt_acts <span class="op">=</span> layer_corrupt_acts[<span class="dv">0</span>, :, neuron_idx]</span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a>        neuron_clean_acts <span class="op">=</span> layer_clean_acts[<span class="dv">0</span>, :, neuron_idx]</span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Go over each token and take the maximum distance</span></span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a>        max_distance <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(neuron_clean_acts.size(<span class="op">-</span><span class="dv">1</span>)):</span>
<span id="cb70-31"><a href="#cb70-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The clean activations are what is used for integrated gradients</span></span>
<span id="cb70-32"><a href="#cb70-32" aria-hidden="true" tabindex="-1"></a>            distance <span class="op">=</span> measure_distance_from_bound(bounds<span class="op">=</span>(<span class="dv">0</span>, neuron_clean_acts[i]), value<span class="op">=</span>neuron_corrupt_acts[i])</span>
<span id="cb70-33"><a href="#cb70-33" aria-hidden="true" tabindex="-1"></a>            max_distance <span class="op">=</span> <span class="bu">max</span>(distance, max_distance)</span>
<span id="cb70-34"><a href="#cb70-34" aria-hidden="true" tabindex="-1"></a>        mlp_distance[layer, neuron_idx] <span class="op">=</span> max_distance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-82" class="cell" data-execution_count="160">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_distance), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_distance)))</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_distance.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Max distance of corrupt activations outside IG gradient bounds"</span>)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="discrepancy-analysis" class="level1">
<h1>Discrepancy analysis</h1>
<p>We evaluate the discrepancy between the two methods, to investigate possible causes of the different attributions.</p>
<div id="cell-84" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.brackets_datasets <span class="im">import</span> BracketsDataset</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get clean and corrupted datasets</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"toy_transformers/brackets_data.json"</span>) <span class="im">as</span> f:</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> json.load(f)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> data_tuples[:<span class="dv">6000</span>]</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> BracketsDataset(data_tuples)</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>clean_dataset <span class="op">=</span> data.toks[data.isbal]</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>clean_is_balanced <span class="op">=</span> torch.ones(clean_dataset.size(<span class="dv">0</span>))</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>corrupted_dataset <span class="op">=</span> data.toks[<span class="op">~</span>data.isbal]</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>corrupted_is_balanced <span class="op">=</span> torch.zeros(corrupted_dataset.size(<span class="dv">0</span>))</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clean_dataset.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2732, 42])</code></pre>
</div>
</div>
<div id="cell-85" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.toy_bracket_transformer <span class="im">import</span> test_loaded_bracket_model_on_dataset</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate baseline performance on sample dataset</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>baseline_logits, baseline_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(baseline_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 2732 out of 2732 training examples correct!
tensor(1.)</code></pre>
</div>
</div>
<div id="cell-86" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>_, corrupted_dataset_cache <span class="op">=</span> model.run_with_cache(corrupted_dataset)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>_, baseline_corrupted_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, corrupted_dataset, corrupted_is_balanced)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(baseline_corrupted_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 3268 out of 3268 training examples correct!
tensor(1.)</code></pre>
</div>
</div>
<div id="cell-87" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ablate_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, set_neurons_to_keep: <span class="bu">set</span>):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># Replace with mean value of activation samples</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    n_neurons <span class="op">=</span> activations.shape[<span class="dv">2</span>]</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> hook.layer()</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(n_neurons):</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (layer, idx) <span class="kw">not</span> <span class="kw">in</span> set_neurons_to_keep:</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>            activations[:, :, idx] <span class="op">=</span> cached_activations[:, idx]</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ablate_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, set_heads_to_keep: <span class="bu">set</span>):</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    n_heads <span class="op">=</span> activations.shape[<span class="dv">2</span>]</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> hook.layer()</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head_idx <span class="kw">in</span> <span class="bu">range</span>(n_heads):</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (layer, head_idx) <span class="kw">not</span> <span class="kw">in</span> set_heads_to_keep:</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>            activations[:, :, head_idx, :] <span class="op">=</span> cached_activations[:, head_idx, :]</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-88" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline comparison: ablating all MLP neurons and attention heads</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>model_ablate_all_hooks <span class="op">=</span> []</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    model_ablate_all_hooks.append((mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    model_ablate_all_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_ablate_all_hooks)</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>model_ablate_all_hooks):</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>    _, model_ablate_all_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(model_ablate_all_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[('blocks.0.hook_mlp_out', &lt;function &lt;lambda&gt; at 0x18a139c60&gt;), ('blocks.0.attn.hook_result', &lt;function &lt;lambda&gt; at 0x18a1384a0&gt;), ('blocks.1.hook_mlp_out', &lt;function &lt;lambda&gt; at 0x18a138900&gt;), ('blocks.1.attn.hook_result', &lt;function &lt;lambda&gt; at 0x18a138a40&gt;), ('blocks.2.hook_mlp_out', &lt;function &lt;lambda&gt; at 0x18a1387c0&gt;), ('blocks.2.attn.hook_result', &lt;function &lt;lambda&gt; at 0x18a1389a0&gt;)]

Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
<div id="cell-89" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline comparison: ablating all MLP neurons</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>mlp_ablate_all_hooks <span class="op">=</span> []</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>    hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    mlp_ablate_all_hooks.append((hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>mlp_ablate_all_hooks):</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>    _, mlp_ablate_all_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(mlp_ablate_all_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
<div id="cell-90" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline comparison: ablating all attention heads</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>attn_ablate_all_hooks <span class="op">=</span> []</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    attn_ablate_all_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>attn_ablate_all_hooks):</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    _, attn_ablate_all_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(attn_ablate_all_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
<section id="ablation-studies-for-integrated-gradients" class="level3">
<h3 class="anchored" data-anchor-id="ablation-studies-for-integrated-gradients">Ablation studies for integrated gradients</h3>
<div id="cell-92" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate faithfulness: when "unimportant" neurons and attention heads are ablated, performance should not be affected</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>ig_isolation_hooks <span class="op">=</span> []</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>set_top_mlp_ig_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_ig_indices])</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_top_mlp_ig_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>set_ig_heads <span class="op">=</span> <span class="bu">set</span>([(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">1</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">1</span>)])</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    ig_isolation_hooks.append((mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_top_mlp_ig_indices)))</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>    ig_isolation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_ig_heads)))</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>ig_isolation_hooks):</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>    ig_isolated_logits, ig_isolated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ig_isolated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 49 out of 168

Model got 2528 out of 2732 training examples correct!
tensor(0.9253)</code></pre>
</div>
</div>
<div id="cell-93" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get neurons to keep: complementary to neurons to keep</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_complementary_neuron_indices(neuron_indices: torch.Tensor, mlp_shape: <span class="bu">tuple</span>):</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.ones(mlp_shape)</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> neuron_indices:</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>        layer_idx, neuron_idx <span class="op">=</span> <span class="bu">tuple</span>(idx)</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layer_idx <span class="op">&lt;</span> mlp_shape[<span class="dv">0</span>] <span class="kw">and</span> neuron_idx <span class="op">&lt;</span> mlp_shape[<span class="dv">1</span>]:</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>            mask[layer_idx, neuron_idx] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>    complementary_indices <span class="op">=</span> torch.nonzero(mask)</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> complementary_indices</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate completeness: when "important" neurons are ablated, performance should be affected</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>mlp_ig_excluded_indices <span class="op">=</span> get_complementary_neuron_indices(top_mlp_ig_indices, (model.cfg.n_layers, model.cfg.d_mlp))</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>set_mlp_ig_excluded_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> mlp_ig_excluded_indices])</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_mlp_ig_excluded_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>set_ig_excluded_heads <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">1</span>), (<span class="dv">1</span>,<span class="dv">1</span>)]</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>ig_ablation_hooks <span class="op">=</span> []</span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>    hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>    ig_ablation_hooks.append((hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_neurons_to_keep<span class="op">=</span>set_mlp_ig_excluded_indices)))</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>    ig_ablation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_ig_excluded_heads)))</span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-28"><a href="#cb87-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-29"><a href="#cb87-29" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>ig_ablation_hooks):</span>
<span id="cb87-30"><a href="#cb87-30" aria-hidden="true" tabindex="-1"></a>    _, ig_ablated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb87-31"><a href="#cb87-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ig_ablated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 119 out of 168

Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
</section>
<section id="ablation-studies-on-causal-tracing" class="level3">
<h3 class="anchored" data-anchor-id="ablation-studies-on-causal-tracing">Ablation studies on causal tracing</h3>
<div id="cell-95" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate faithfulness: when "unimportant" neurons and attention heads are ablated, performance should not be affected</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>patch_isolation_hooks <span class="op">=</span> []</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>set_top_mlp_patch_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_patch_indices])</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_top_mlp_patch_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>set_patch_heads <span class="op">=</span> <span class="bu">set</span>([(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">1</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">1</span>)])</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    patch_isolation_hooks.append((mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_top_mlp_patch_indices)))</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>    patch_isolation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_patch_heads)))</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>patch_isolation_hooks):</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>    patch_isolated_logits, patch_isolated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(patch_isolated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 12 out of 168

Model got 1150 out of 2732 training examples correct!
tensor(0.4209)</code></pre>
</div>
</div>
<div id="cell-96" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate completeness: when "important" neurons are ablated, performance should be affected</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>patch_ig_excluded_indices <span class="op">=</span> get_complementary_neuron_indices(top_mlp_patch_indices, (model.cfg.n_layers, model.cfg.d_mlp))</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>set_mlp_patch_excluded_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> patch_ig_excluded_indices])</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_mlp_patch_excluded_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>set_patch_excluded_heads <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">1</span>), (<span class="dv">1</span>,<span class="dv">1</span>)]</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>patch_ablation_hooks <span class="op">=</span> []</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>    hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>    patch_ablation_hooks.append((hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_neurons_to_keep<span class="op">=</span>set_mlp_patch_excluded_indices)))</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    patch_ablation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_patch_excluded_heads)))</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>patch_ablation_hooks):</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>    _, patch_ablated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(patch_ablated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 156 out of 168

Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
</section>
<section id="analysis-of-ablation-studies" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-ablation-studies">Analysis of ablation studies</h3>
<div id="cell-98" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model performance under corruption of different components"</span>)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Corrupted model components"</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model performance on bracket classification"</span>)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>plt.bar(</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"None"</span>, <span class="st">"All neurons"</span>, <span class="st">"All attention heads"</span>, <span class="st">"All heads and neurons"</span>],</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    [baseline_performance, mlp_ablate_all_performance, attn_ablate_all_performance, model_ablate_all_performance]</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=-</span><span class="dv">15</span>)</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-68-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-99" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model performance under corruption of components with high attribution scores"</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Attribution method used to identify components for corruption"</span>)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model performance on bracket classification"</span>)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>plt.bar(</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"All heads + neurons"</span>, <span class="st">"Integrated Gradients"</span>, <span class="st">"Activation patching"</span>],</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    [model_ablate_all_performance, ig_ablated_performance, patch_ablated_performance],</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>[<span class="st">'grey'</span>, <span class="st">'tab:blue'</span>, <span class="st">'tab:blue'</span>]</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-69-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-100" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model performance under corruption of components with low attribution scores"</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Attribution method used to identify components to preserve"</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model performance on bracket classification"</span>)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>plt.bar(</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Original model"</span>, <span class="st">"Integrated Gradients"</span>, <span class="st">"Activation patching"</span>],</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>    [baseline_performance, ig_isolated_performance, patch_isolated_performance],</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>[<span class="st">'grey'</span>, <span class="st">'tab:blue'</span>, <span class="st">'tab:blue'</span>]</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-70-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradual-ablation" class="level3">
<h3 class="anchored" data-anchor-id="gradual-ablation">Gradual ablation</h3>
<p>Instead of taking the top 20% components and corrupting them all, we gradually corrupt components from the lowest attribution scores first to the highest attribution scores, and measure the drop in performance.</p>
<div id="cell-102" class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Order MLP neurons and attention heads by ABSOLUTE attribution scores</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>combined_ig_results <span class="op">=</span> torch.concat([mlp_ig_results.flatten(), attn_ig_results.flatten()])</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>sorted_indices_1d <span class="op">=</span> torch.argsort(combined_ig_results.<span class="bu">abs</span>())</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>NEURON_LABEL, ATTN_LABEL <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> convert_1d_indices_to_2d(indices_1d):</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>    indices_2d <span class="op">=</span> []</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>    max_mlp_index <span class="op">=</span> model.cfg.d_mlp <span class="op">*</span> model.cfg.n_layers</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> indices_1d:</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">&lt;</span> max_mlp_index:</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># MLP neuron</span></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> idx <span class="op">//</span> model.cfg.d_mlp</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>            neuron_pos <span class="op">=</span> idx <span class="op">%</span> model.cfg.d_mlp</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>            indices_2d.append([NEURON_LABEL, layer.item(), neuron_pos.item()])</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Attention</span></span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>            attn_idx <span class="op">=</span> idx <span class="op">-</span> (model.cfg.d_mlp <span class="op">*</span> model.cfg.n_layers)</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> attn_idx <span class="op">//</span> model.cfg.n_heads</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>            attn_head_pos <span class="op">=</span> attn_idx <span class="op">%</span> model.cfg.n_heads</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>            indices_2d.append([ATTN_LABEL, layer.item(), attn_head_pos.item()])</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> indices_2d</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>sorted_indices_2d <span class="op">=</span> convert_1d_indices_to_2d(sorted_indices_1d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-103" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradually corrupt components with lowest attribution scores</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>gradual_ig_corruption_performance <span class="op">=</span> []</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Preserve all neurons at first, then delete</span></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>set_gradual_neurons_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp)])</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>set_gradual_attn_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads)])</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>model.clear_contexts()</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sorted_indices_2d)):</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>    keep_type, keep_layer, keep_idx <span class="op">=</span> sorted_indices_2d[idx]</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keep_type <span class="op">==</span> NEURON_LABEL:</span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>        set_gradual_neurons_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>        mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, keep_layer)</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a>        model.add_hook(mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_gradual_neurons_to_keep.copy()))</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>        set_gradual_attn_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a>        attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, keep_layer)</span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>        model.add_hook(attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_gradual_attn_to_keep.copy()))</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a>    _, performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset[:<span class="dv">500</span>], clean_is_balanced[:<span class="dv">500</span>])</span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(performance)</span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>    gradual_ig_corruption_performance.append(performance)</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 500 out of 500 training examples correct!
tensor(1.)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 488 out of 500 training examples correct!
tensor(0.9760)

Model got 399 out of 500 training examples correct!
tensor(0.7980)

Model got 399 out of 500 training examples correct!
tensor(0.7980)

Model got 399 out of 500 training examples correct!
tensor(0.7980)

Model got 398 out of 500 training examples correct!
tensor(0.7960)

Model got 396 out of 500 training examples correct!
tensor(0.7920)

Model got 396 out of 500 training examples correct!
tensor(0.7920)

Model got 392 out of 500 training examples correct!
tensor(0.7840)

Model got 389 out of 500 training examples correct!
tensor(0.7780)

Model got 389 out of 500 training examples correct!
tensor(0.7780)

Model got 422 out of 500 training examples correct!
tensor(0.8440)

Model got 422 out of 500 training examples correct!
tensor(0.8440)

Model got 419 out of 500 training examples correct!
tensor(0.8380)

Model got 420 out of 500 training examples correct!
tensor(0.8400)

Model got 420 out of 500 training examples correct!
tensor(0.8400)

Model got 418 out of 500 training examples correct!
tensor(0.8360)

Model got 409 out of 500 training examples correct!
tensor(0.8180)

Model got 405 out of 500 training examples correct!
tensor(0.8100)

Model got 402 out of 500 training examples correct!
tensor(0.8040)

Model got 401 out of 500 training examples correct!
tensor(0.8020)

Model got 401 out of 500 training examples correct!
tensor(0.8020)

Model got 400 out of 500 training examples correct!
tensor(0.8000)

Model got 400 out of 500 training examples correct!
tensor(0.8000)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)

Model got 0 out of 500 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
<div id="cell-104" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>plt.plot(gradual_ig_corruption_performance)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Gradual corruption starting with lowest IG attribution score"</span>)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of corrupted components"</span>)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Performance on balanced bracket classification"</span>)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-73-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-105" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"gradual_ig_corruption_performance_v2"</span>, gradual_ig_corruption_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-106" class="cell">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat with activation patching results</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>combined_patch_results <span class="op">=</span> torch.concat([mlp_patch_results.flatten(), attn_patch_results.flatten()])</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>sorted_patch_indices_1d <span class="op">=</span> torch.argsort(combined_patch_results.<span class="bu">abs</span>())</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>sorted_patch_indices_2d <span class="op">=</span> convert_1d_indices_to_2d(sorted_patch_indices_1d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-107" class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradually corrupt components with lowest attribution scores from activation patching</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>gradual_patch_corruption_performance <span class="op">=</span> []</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>set_gradual_patch_neurons_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp)])</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>set_gradual_patch_attn_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads)])</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>model.clear_contexts()</span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sorted_patch_indices_2d)):</span>
<span id="cb102-12"><a href="#cb102-12" aria-hidden="true" tabindex="-1"></a>    keep_type, keep_layer, keep_idx <span class="op">=</span> sorted_patch_indices_2d[idx]</span>
<span id="cb102-13"><a href="#cb102-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keep_type <span class="op">==</span> NEURON_LABEL:</span>
<span id="cb102-14"><a href="#cb102-14" aria-hidden="true" tabindex="-1"></a>        set_gradual_patch_neurons_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb102-15"><a href="#cb102-15" aria-hidden="true" tabindex="-1"></a>        mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, keep_layer)</span>
<span id="cb102-16"><a href="#cb102-16" aria-hidden="true" tabindex="-1"></a>        model.add_hook(mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_gradual_patch_neurons_to_keep.copy()))</span>
<span id="cb102-17"><a href="#cb102-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb102-18"><a href="#cb102-18" aria-hidden="true" tabindex="-1"></a>        set_gradual_patch_attn_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb102-19"><a href="#cb102-19" aria-hidden="true" tabindex="-1"></a>        attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, keep_layer)</span>
<span id="cb102-20"><a href="#cb102-20" aria-hidden="true" tabindex="-1"></a>        model.add_hook(attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_gradual_patch_attn_to_keep.copy()))</span>
<span id="cb102-21"><a href="#cb102-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-22"><a href="#cb102-22" aria-hidden="true" tabindex="-1"></a>    _, performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset[:<span class="dv">500</span>], clean_is_balanced[:<span class="dv">500</span>])</span>
<span id="cb102-23"><a href="#cb102-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(performance)</span>
<span id="cb102-24"><a href="#cb102-24" aria-hidden="true" tabindex="-1"></a>    gradual_patch_corruption_performance.append(performance)</span>
<span id="cb102-25"><a href="#cb102-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-26"><a href="#cb102-26" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-108" class="cell">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>plt.plot(gradual_patch_corruption_performance)</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Gradual corruption starting with lowest activation patching attribution score"</span>)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of corrupted components"</span>)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Performance on balanced bracket classification"</span>)</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="knowledge-localisation" class="level1">
<h1>Knowledge localisation</h1>
<p>We identify which components localise information for specific tokens, and evaluate the effectiveness of steering for each method.</p>
<div id="cell-110" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> integrated_gradient_attribution(input_string: <span class="bu">str</span>):</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>    mlp_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>    attn_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> tokenizer.tokenize(input_string)</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> np.isin(<span class="bu">input</span>, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb104-9"><a href="#cb104-9" aria-hidden="true" tabindex="-1"></a>    baseline <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb104-10"><a href="#cb104-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-11"><a href="#cb104-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb104-12"><a href="#cb104-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient attribution on heads</span></span>
<span id="cb104-13"><a href="#cb104-13" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb104-14"><a href="#cb104-14" aria-hidden="true" tabindex="-1"></a>        target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb104-15"><a href="#cb104-15" aria-hidden="true" tabindex="-1"></a>        attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline) <span class="co"># shape [1, seq_len, d_head, d_model]</span></span>
<span id="cb104-16"><a href="#cb104-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb104-17"><a href="#cb104-17" aria-hidden="true" tabindex="-1"></a>        per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb104-18"><a href="#cb104-18" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb104-19"><a href="#cb104-19" aria-hidden="true" tabindex="-1"></a>        attn_results[layer] <span class="op">=</span> score</span>
<span id="cb104-20"><a href="#cb104-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-21"><a href="#cb104-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb104-22"><a href="#cb104-22" aria-hidden="true" tabindex="-1"></a>        target_layer <span class="op">=</span> model.blocks[layer].hook_mlp_out</span>
<span id="cb104-23"><a href="#cb104-23" aria-hidden="true" tabindex="-1"></a>        attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, target_layer, baseline) <span class="co"># shape [1, seq_len, d_model]</span></span>
<span id="cb104-24"><a href="#cb104-24" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb104-25"><a href="#cb104-25" aria-hidden="true" tabindex="-1"></a>        mlp_results[layer] <span class="op">=</span> score</span>
<span id="cb104-26"><a href="#cb104-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-27"><a href="#cb104-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mlp_results, attn_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-111" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>mlp_open_results, attn_open_results <span class="op">=</span> integrated_gradient_attribution(<span class="st">"(("</span>)</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>mlp_closed_results, attn_closed_results <span class="op">=</span> integrated_gradient_attribution(<span class="st">"))"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Error (delta) for blocks.0.attn.hook_result attribution: 0.0013695928500965238

Error (delta) for blocks.0.hook_mlp_out attribution: -0.0001100528534152545

Error (delta) for blocks.1.attn.hook_result attribution: 0.0001736992271617055

Error (delta) for blocks.1.hook_mlp_out attribution: -0.0010857651941478252

Error (delta) for blocks.2.attn.hook_result attribution: -0.013093475252389908

Error (delta) for blocks.2.hook_mlp_out attribution: -0.00012520691961981356

Error (delta) for blocks.0.attn.hook_result attribution: 0.0013547124108299613

Error (delta) for blocks.0.hook_mlp_out attribution: -0.000110413740912918

Error (delta) for blocks.1.attn.hook_result attribution: -0.0004306236805859953

Error (delta) for blocks.1.hook_mlp_out attribution: -0.00031769595807418227

Error (delta) for blocks.2.attn.hook_result attribution: 0.001997644081711769

Error (delta) for blocks.2.hook_mlp_out attribution: -8.628690557088703e-05</code></pre>
</div>
</div>
<div id="cell-112" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_open_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_open_results)))</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_open_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients): '(('"</span>)</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_closed_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_closed_results)))</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_closed_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients): '))'"</span>)</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_open_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_open_results)))</span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_open_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb107-30"><a href="#cb107-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients): '('"</span>)</span>
<span id="cb107-31"><a href="#cb107-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-32"><a href="#cb107-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb107-33"><a href="#cb107-33" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb107-34"><a href="#cb107-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-35"><a href="#cb107-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb107-36"><a href="#cb107-36" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb107-37"><a href="#cb107-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-38"><a href="#cb107-38" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb107-39"><a href="#cb107-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb107-40"><a href="#cb107-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-41"><a href="#cb107-41" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_closed_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_closed_results)))</span>
<span id="cb107-42"><a href="#cb107-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-43"><a href="#cb107-43" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb107-44"><a href="#cb107-44" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_closed_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb107-45"><a href="#cb107-45" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients): ')'"</span>)</span>
<span id="cb107-46"><a href="#cb107-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-47"><a href="#cb107-47" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb107-48"><a href="#cb107-48" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb107-49"><a href="#cb107-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-50"><a href="#cb107-50" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb107-51"><a href="#cb107-51" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb107-52"><a href="#cb107-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-53"><a href="#cb107-53" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb107-54"><a href="#cb107-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-80-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-80-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-80-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Integrated Gradients vs Activation Patching_files/figure-html/cell-80-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="circuit-construction" class="level1">
<h1>Circuit construction</h1>
<p>We construct a circuit for each attribution method, by removing all unimportant components and their connections.</p>
<p>We investigate the completeness, faithfulness and minimality of the constructed circuits.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>