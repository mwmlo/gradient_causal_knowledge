<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Integrated Gradients vs Activation Patching</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-a37c72dd2dbac68997fcdc15a3622e78.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="floating quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background-motivation-and-set-up" id="toc-background-motivation-and-set-up" class="nav-link active" data-scroll-target="#background-motivation-and-set-up">Background, motivation and set up</a>
  <ul class="collapse">
  <li><a href="#set-up" id="toc-set-up" class="nav-link" data-scroll-target="#set-up">Set-up</a></li>
  </ul></li>
  <li><a href="#initial-results" id="toc-initial-results" class="nav-link" data-scroll-target="#initial-results">Initial results</a>
  <ul class="collapse">
  <li><a href="#evaluating-baselines-for-integrated-gradients" id="toc-evaluating-baselines-for-integrated-gradients" class="nav-link" data-scroll-target="#evaluating-baselines-for-integrated-gradients">Evaluating baselines for integrated gradients</a></li>
  <li><a href="#basic-integrated-gradients" id="toc-basic-integrated-gradients" class="nav-link" data-scroll-target="#basic-integrated-gradients">Basic Integrated Gradients</a></li>
  <li><a href="#basic-activation-patching-causal-tracing" id="toc-basic-activation-patching-causal-tracing" class="nav-link" data-scroll-target="#basic-activation-patching-causal-tracing">Basic Activation Patching (Causal Tracing)</a></li>
  </ul></li>
  <li><a href="#comparison-metrics" id="toc-comparison-metrics" class="nav-link" data-scroll-target="#comparison-metrics">Comparison metrics</a>
  <ul class="collapse">
  <li><a href="#correlation-between-attribution-scores" id="toc-correlation-between-attribution-scores" class="nav-link" data-scroll-target="#correlation-between-attribution-scores">Correlation between attribution scores</a></li>
  <li><a href="#agreement-between-attribution-scores" id="toc-agreement-between-attribution-scores" class="nav-link" data-scroll-target="#agreement-between-attribution-scores">Agreement between attribution scores</a></li>
  <li><a href="#measuring-agreement-tukey-mean-difference-plot" id="toc-measuring-agreement-tukey-mean-difference-plot" class="nav-link" data-scroll-target="#measuring-agreement-tukey-mean-difference-plot">Measuring agreement: Tukey mean-difference plot</a></li>
  <li><a href="#difference-in-scores-for-attention-heads" id="toc-difference-in-scores-for-attention-heads" class="nav-link" data-scroll-target="#difference-in-scores-for-attention-heads">Difference in scores for attention heads</a></li>
  </ul></li>
  <li><a href="#factors-which-impact-integrated-gradients" id="toc-factors-which-impact-integrated-gradients" class="nav-link" data-scroll-target="#factors-which-impact-integrated-gradients">Factors which impact Integrated Gradients</a>
  <ul class="collapse">
  <li><a href="#sensitivity-to-input-opposite-classification" id="toc-sensitivity-to-input-opposite-classification" class="nav-link" data-scroll-target="#sensitivity-to-input-opposite-classification">Sensitivity to input: opposite classification</a></li>
  <li><a href="#average-of-integrated-gradients-attributions" id="toc-average-of-integrated-gradients-attributions" class="nav-link" data-scroll-target="#average-of-integrated-gradients-attributions">Average of Integrated Gradients attributions</a></li>
  </ul></li>
  <li><a href="#factors-which-impact-activation-patching" id="toc-factors-which-impact-activation-patching" class="nav-link" data-scroll-target="#factors-which-impact-activation-patching">Factors which impact Activation Patching</a>
  <ul class="collapse">
  <li><a href="#patching-with-different-inputs" id="toc-patching-with-different-inputs" class="nav-link" data-scroll-target="#patching-with-different-inputs">Patching with different inputs</a></li>
  <li><a href="#patching-in-the-opposite-direction-denoising" id="toc-patching-in-the-opposite-direction-denoising" class="nav-link" data-scroll-target="#patching-in-the-opposite-direction-denoising">Patching in the opposite direction (denoising)</a></li>
  </ul></li>
  <li><a href="#distribution-of-activations-and-baselines" id="toc-distribution-of-activations-and-baselines" class="nav-link" data-scroll-target="#distribution-of-activations-and-baselines">Distribution of activations and baselines</a>
  <ul class="collapse">
  <li><a href="#baseline-as-corrupted-activations" id="toc-baseline-as-corrupted-activations" class="nav-link" data-scroll-target="#baseline-as-corrupted-activations">Baseline as corrupted activations</a></li>
  <li><a href="#activation-patching-with-ig-zero-baseline" id="toc-activation-patching-with-ig-zero-baseline" class="nav-link" data-scroll-target="#activation-patching-with-ig-zero-baseline">Activation patching with IG zero baseline</a></li>
  <li><a href="#patching-with-mean-value-of-corrupted-dataset" id="toc-patching-with-mean-value-of-corrupted-dataset" class="nav-link" data-scroll-target="#patching-with-mean-value-of-corrupted-dataset">Patching with mean value of corrupted dataset</a></li>
  </ul></li>
  <li><a href="#ablation-studies" id="toc-ablation-studies" class="nav-link" data-scroll-target="#ablation-studies">Ablation studies</a>
  <ul class="collapse">
  <li><a href="#ablation-studies-for-integrated-gradients" id="toc-ablation-studies-for-integrated-gradients" class="nav-link" data-scroll-target="#ablation-studies-for-integrated-gradients">Ablation studies for integrated gradients</a></li>
  <li><a href="#ablation-studies-on-causal-tracing" id="toc-ablation-studies-on-causal-tracing" class="nav-link" data-scroll-target="#ablation-studies-on-causal-tracing">Ablation studies on causal tracing</a></li>
  <li><a href="#analysis-of-ablation-studies" id="toc-analysis-of-ablation-studies" class="nav-link" data-scroll-target="#analysis-of-ablation-studies">Analysis of ablation studies</a></li>
  </ul></li>
  <li><a href="#gradual-ablation" id="toc-gradual-ablation" class="nav-link" data-scroll-target="#gradual-ablation">Gradual ablation</a>
  <ul class="collapse">
  <li><a href="#gradual-ablation-for-integrated-gradients" id="toc-gradual-ablation-for-integrated-gradients" class="nav-link" data-scroll-target="#gradual-ablation-for-integrated-gradients">Gradual ablation for Integrated Gradients</a></li>
  <li><a href="#gradual-ablation-with-activation-patching" id="toc-gradual-ablation-with-activation-patching" class="nav-link" data-scroll-target="#gradual-ablation-with-activation-patching">Gradual ablation with Activation Patching</a></li>
  </ul></li>
  <li><a href="#next-steps-and-further-ideas" id="toc-next-steps-and-further-ideas" class="nav-link" data-scroll-target="#next-steps-and-further-ideas">Next steps and further ideas</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Integrated Gradients vs Activation Patching</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="background-motivation-and-set-up" class="level2">
<h2 class="anchored" data-anchor-id="background-motivation-and-set-up">Background, motivation and set up</h2>
<p><strong>Objective</strong>: Compare attributions using integrated gradients and activation patching, and investigate the discrepancies between the two methods.</p>
<p><strong>Background</strong>:</p>
<ul>
<li>Both integrated gradients (IG) and activation patching (AP) are used extensively in explainability and interpretability.
<ul>
<li>IG is path-based: integrates gradients along a path from a baseline to the input.</li>
<li>AP is perturbation-based: directly measures causal effect of replacing activations.</li>
</ul></li>
<li>Activation patching can be approximated cheaply using methods inspired by integrated gradients; see <a href="https://arxiv.org/abs/2403.17806">Hanna et al (2024)</a></li>
<li>Gradient-based attribution methods can be used to identify important model components as an alternative to activation patching; see <a href="https://aclanthology.org/2024.emnlp-main.965/">Ferrando and Voita (2024)</a></li>
<li>Activation patching is often used as the “gold standard” to evaluate attribution methods</li>
</ul>
<p><strong>Motivation</strong>:</p>
<ul>
<li>Understand when and why do IG and AP disagree: e.g.&nbsp;methodological limitations, or suitability to model tasks, etc.</li>
<li>Investigate if discrepancies help uncover different hidden model behaviours</li>
<li>Understand when and why linear approximations to activation patching fail</li>
<li>Investigate limitations of using activation patching for evaluations: if results are different because of other unknown factors (not just because the method evaluated is “incorrect”)</li>
</ul>
<p><em>Some ideas</em>:</p>
<ul>
<li>Components which might not be identified by activation patching: generic components which are used for both clean and corrupted examples in activation patching</li>
<li>Components which might not be identified by integrated gradients: backup attention heads, components which only influence the output when interacting with other components, i.e.&nbsp;OR circuits</li>
</ul>
<section id="set-up" class="level3">
<h3 class="anchored" data-anchor-id="set-up">Set-up</h3>
<p>We load a pre-trained toy transformer which performs balanced bracket classification. The model has three layers, each with two attention heads and one MLP layer of 56 neurons.</p>
<p><img src="reference/bracket-transformer-entire-model-short.png" alt="Transformer architecture" width="100%"></p>
<div id="cell-4" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> importlib</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> captum.attr <span class="im">import</span> LayerIntegratedGradients</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.utils <span class="im">import</span> get_act_name</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> ActivationCache</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> toy_transformers.toy_bracket_transformer <span class="im">as</span> tt</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>importlib.<span class="bu">reload</span>(tt)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.toy_bracket_transformer <span class="im">import</span> load_toy_bracket_transformer, test_loaded_bracket_model</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-5" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>tokenizer, model <span class="op">=</span> load_toy_bracket_transformer()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="initial-results" class="level1">
<h1>Initial results</h1>
<section id="evaluating-baselines-for-integrated-gradients" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-baselines-for-integrated-gradients">Evaluating baselines for integrated gradients</h3>
<p>We need to choose an appropriate baseline to calculate Integrated Gradients from. An ideal baseline input will produce a final output which is close to zero, and meaningfully represents a lack of information (see Sundararajan et al.&nbsp;2017).</p>
<p>Here we test a series of inputs which are feasible baseline inputs, and check their final classification scores.</p>
<div id="cell-8" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_from_layer_fn(x, original_input, prev_layer):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Force the layer before the target layer to output the given values, i.e. pass the given input into the target layer</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># original_input value does not matter; useful to keep shapes nice, but its activations will be overwritten</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model.run_with_hooks(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        original_input,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        fwd_hooks<span class="op">=</span>[(prev_layer.name, <span class="kw">lambda</span> act, hook: x)]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> output[:, <span class="dv">0</span>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.softmax(<span class="op">-</span><span class="dv">1</span>)[:, <span class="dv">1</span>]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_layer_to_output_attributions(original_input, layer_input, layer_baseline, target_layer, prev_layer):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take the model starting from the target layer</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    forward_fn <span class="op">=</span> <span class="kw">lambda</span> x: run_from_layer_fn(x, original_input, prev_layer)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attribute to the target_layer's output</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    ig_embed <span class="op">=</span> LayerIntegratedGradients(forward_fn, target_layer, multiply_by_inputs<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    attributions, approximation_error <span class="op">=</span> ig_embed.attribute(inputs<span class="op">=</span>layer_input,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                                                    baselines<span class="op">=</span>layer_baseline, </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>                                                    attribute_to_layer_input<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>                                                    return_convergence_delta<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Error (delta) for </span><span class="sc">{</span>target_layer<span class="sc">.</span>name<span class="sc">}</span><span class="ss"> attribution: </span><span class="sc">{</span>approximation_error<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attributions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-9" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The prediction at the baseline should be near zero (see Sundararajan et al. 2017)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we test a series of inputs which could be a baseline, and check their final classification scores</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_balanced(x):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> model(x)[:, <span class="dv">0</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits.softmax(<span class="op">-</span><span class="dv">1</span>)[:, <span class="dv">1</span>].item()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>balanced_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input: ()()"</span>, balanced_input)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output:"</span>, predict_balanced(balanced_input), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>unbalanced_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input: (()("</span>, unbalanced_input)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output:"</span>, predict_balanced(unbalanced_input), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>all_padding <span class="op">=</span> torch.full_like(balanced_input, tokenizer.PAD_TOKEN)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input: PAD PAD PAD PAD PAD PAD"</span>, all_padding)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output:"</span>, predict_balanced(all_padding), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(balanced_input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>start_pad_end <span class="op">=</span> balanced_input <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input: START PAD PAD PAD PAD END"</span>, start_pad_end)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output:"</span>, predict_balanced(start_pad_end), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>all_zeroes <span class="op">=</span> torch.zeros_like(balanced_input)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input: (all zeroes)"</span>, all_zeroes)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output:"</span>, predict_balanced(all_zeroes), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Patch zeroes in at target component, e.g. MLP neurons in layer 1</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"post"</span>, <span class="dv">1</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">input</span> <span class="kw">in</span> [balanced_input, unbalanced_input, all_padding, start_pad_end, all_zeroes]:</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    _, test_cache <span class="op">=</span> model.run_with_cache(<span class="bu">input</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    prev_layer_acts <span class="op">=</span> test_cache[prev_layer_hook]</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    prev_layer_acts[:, :] <span class="op">=</span> torch.zeros(prev_layer_acts.size(<span class="dv">2</span>))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Input with patching:"</span>, <span class="bu">input</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Output:"</span>, run_from_layer_fn(prev_layer_acts, <span class="bu">input</span>, prev_layer).item(), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input: ()() tensor([[0, 3, 4, 3, 4, 2]])
Output: 0.9999862909317017 

Input: (()( tensor([[0, 3, 3, 4, 3, 2]])
Output: 1.8626720702741295e-05 

Input: PAD PAD PAD PAD PAD PAD tensor([[1, 1, 1, 1, 1, 1]])
Output: 0.04108979180455208 

Input: START PAD PAD PAD PAD END tensor([[0, 1, 1, 1, 1, 2]])
Output: 3.6776664273929782e-06 

Input: (all zeroes) tensor([[0, 0, 0, 0, 0, 0]])
Output: 1.5444304153788835e-05 

Patch zeroes in at target component, e.g. MLP neurons in layer 1

Input with patching: tensor([[0, 3, 4, 3, 4, 2]])
Output: 0.9983036518096924 

Input with patching: tensor([[0, 3, 3, 4, 3, 2]])
Output: 0.00015179907495621592 

Input with patching: tensor([[1, 1, 1, 1, 1, 1]])
Output: 0.0017063161358237267 

Input with patching: tensor([[0, 1, 1, 1, 1, 2]])
Output: 2.0060035240021534e-05 

Input with patching: tensor([[0, 0, 0, 0, 0, 0]])
Output: 6.123560524429195e-06 
</code></pre>
</div>
</div>
<p>The <code>[START, PAD, END]</code> token sequence seems to be the best baseline, because it produces a final output which is consistently closest to zero, and this sequence meaningfully represents an input with no information.</p>
<p>Forcing zero activations to be fed into the target component (i.e.&nbsp;patching zeroes in) does not seem to have much of an effect on the final output. This is probably because, even though we overwrite the activations fed into the target component with zeros, the original input continues to be processed due to the residual stream in the transformer model.</p>
<p>Therefore, at the present, it doesn’t seem to really matter whether or not we patch in zero activations before the target component, as long as we use input <code>[START, PAD, END]</code>. Intuitively, it makes sense to patch in zero activations as the baseline for IG, as this is the default baseline.</p>
</section>
<section id="basic-integrated-gradients" class="level3">
<h3 class="anchored" data-anchor-id="basic-integrated-gradients">Basic Integrated Gradients</h3>
<p>We perform integrated gradients for a balanced input sequence <code>()()</code>. The baseline used is zero activations fed into the target components, and the original input as <code>[START, PAD, END]</code>.</p>
<div id="cell-12" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>attn_ig_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(<span class="bu">input</span>, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>_, input_cache <span class="op">=</span> model.run_with_cache(<span class="bu">input</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>_, baseline_cache <span class="op">=</span> model.run_with_cache(baseline)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"z"</span>, layer)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> input_cache[prev_layer_hook]</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> baseline_cache[prev_layer_hook]</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_head, d_model]</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    attn_ig_results[layer] <span class="op">=</span> score</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"post"</span>, layer)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> input_cache[prev_layer_hook]</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> baseline_cache[prev_layer_hook]</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_model]</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    mlp_ig_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Error (delta) for blocks.0.attn.hook_result attribution: -0.063778817653656

Error (delta) for blocks.0.hook_mlp_out attribution: -0.002097785472869873

Error (delta) for blocks.1.attn.hook_result attribution: 0.00013774633407592773

Error (delta) for blocks.1.hook_mlp_out attribution: -5.960464477539062e-07

Error (delta) for blocks.2.attn.hook_result attribution: -0.0004189014434814453

Error (delta) for blocks.2.hook_mlp_out attribution: 2.5789859137148596e-09</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_results)))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients)"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-14" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_results)))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients)"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="basic-activation-patching-causal-tracing" class="level3">
<h3 class="anchored" data-anchor-id="basic-activation-patching-causal-tracing">Basic Activation Patching (Causal Tracing)</h3>
<p>Noising (a corrupt → clean patch) shows whether the patched activations were <em>necessary</em> to maintain the model behaviour. Therefore we patch corrupted activations into a clean run.</p>
<div id="cell-17" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)    <span class="co"># Unbalanced</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We run on the corrupted prompt with the cache so we store activations to patch in later.</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>corrupted_logits, corrupted_cache <span class="op">=</span> model.run_with_cache(corrupted_input)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>clean_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get probability of overall sequence being balanced (class 1) from position 0</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>corrupted_answer_logits <span class="op">=</span> corrupted_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Balanced input score: </span><span class="sc">{</span>clean_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbalanced input score: </span><span class="sc">{</span>corrupted_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>baseline_diff <span class="op">=</span> (corrupted_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline clean-corrupted logit difference: </span><span class="sc">{</span>baseline_diff<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-18" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>mlp_patch_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>attn_patch_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: <span class="bu">int</span>):</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target neuron with activations from the cached run.</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    activations[:, :, neuron_idx] <span class="op">=</span> cached_activations[:, :, neuron_idx]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: <span class="bu">int</span>):</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target attention head with activations from the cached run.</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    activations[:, :, head_idx, :] <span class="op">=</span> cached_activations[:, :, head_idx, :]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, corrupted_cache, head)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        attn_patch_results[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, corrupted_cache, neuron)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        mlp_patch_results[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-19" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_results)))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Corrupt -&gt; Clean"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_results)))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Corrupt -&gt; Clean"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="comparison-metrics" class="level1">
<h1>Comparison metrics</h1>
<div id="cell-22" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>mlp_ig_results <span class="op">=</span> mlp_ig_results.detach()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>attn_ig_results <span class="op">=</span> attn_ig_results.detach()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="correlation-between-attribution-scores" class="level3">
<h3 class="anchored" data-anchor-id="correlation-between-attribution-scores">Correlation between attribution scores</h3>
<div id="cell-24" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the attribution scores against each other. Correlation: y = x.</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> mlp_ig_results.flatten().numpy()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> mlp_patch_results.flatten().numpy()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x, y<span class="op">=</span>y)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Integrated Gradients MLP Attribution Scores"</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Causal Tracing MLP Attribution Scores"</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation coefficient between IG and causal tracing attributions for neurons: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation coefficient between IG and causal tracing attributions for neurons: 0.7712230344174876</code></pre>
</div>
</div>
<div id="cell-25" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> attn_ig_results.flatten().numpy()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> attn_patch_results.flatten().numpy()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x, y<span class="op">=</span>y)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Integrated Gradients Attention Attribution Scores"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Causal Tracing Attention Attribution Scores"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation coefficient between IG and causal tracing attributions for attention: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation coefficient between IG and causal tracing attributions for attention: 0.9761361508593569</code></pre>
</div>
</div>
</section>
<section id="agreement-between-attribution-scores" class="level3">
<h3 class="anchored" data-anchor-id="agreement-between-attribution-scores">Agreement between attribution scores</h3>
<p>The Jaccard scores for MLP neuron attribution scores per transformer layer is low.</p>
<div id="cell-27" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_top_k_by_abs(data, k):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    _, indices <span class="op">=</span> torch.topk(data.flatten().<span class="bu">abs</span>(), k)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    top_k_values <span class="op">=</span> torch.gather(data.flatten(), <span class="dv">0</span>, indices)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    formatted_indices <span class="op">=</span> []</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        layer <span class="op">=</span> idx <span class="op">//</span> model.cfg.d_mlp</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        neuron_pos <span class="op">=</span> idx <span class="op">%</span> model.cfg.d_mlp</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        formatted_indices.append([layer, neuron_pos])</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(formatted_indices), top_k_values</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_attributions_above_threshold(data, percentile):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    threshold <span class="op">=</span> torch.<span class="bu">min</span>(data) <span class="op">+</span> percentile <span class="op">*</span> (torch.<span class="bu">max</span>(data) <span class="op">-</span> torch.<span class="bu">min</span>(data))</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    masked_data <span class="op">=</span> torch.where(data <span class="op">&gt;</span> threshold, data, <span class="dv">0</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    nonzero_indices <span class="op">=</span> torch.nonzero(masked_data)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nonzero_indices, masked_data</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co"># top_mlp_ig_indices, top_mlp_ig_results = get_attributions_above_threshold(mlp_ig_results, 0.5)</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># top_mlp_patch_indices, top_mlp_patch_results = get_attributions_above_threshold(mlp_patch_results, 0.5)</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_indices, top_mlp_ig_results <span class="op">=</span> get_top_k_by_abs(mlp_ig_results, <span class="dv">50</span>)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_indices, top_mlp_patch_results <span class="op">=</span> get_top_k_by_abs(mlp_patch_results, <span class="dv">50</span>)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(top_mlp_ig_indices.shape, top_mlp_patch_indices.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([50, 2]) torch.Size([50, 2])</code></pre>
</div>
</div>
<div id="cell-28" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># top_mlp_ig_sets = [set(row.tolist()) for row in top_mlp_ig_indices]</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># top_mlp_patch_sets = [set(row.tolist()) for row in top_mlp_patch_indices]</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_sets <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_ig_indices])</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_sets <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_patch_indices])</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>intersection <span class="op">=</span> top_mlp_ig_sets.intersection(top_mlp_patch_sets)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>union <span class="op">=</span> top_mlp_ig_sets.union(top_mlp_patch_sets)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>jaccard <span class="op">=</span> <span class="bu">len</span>(intersection) <span class="op">/</span> <span class="bu">len</span>(union)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Jaccard score for MLP neurons: </span><span class="sc">{</span>jaccard<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Jaccard score for MLP neurons: 0.5625</code></pre>
</div>
</div>
<div id="cell-29" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> indices_set_to_binary_matrix(set_indices, shape):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    binary_mat <span class="op">=</span> torch.zeros(shape, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, j <span class="kw">in</span> set_indices:</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        binary_mat[i, j] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> binary_mat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-30" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>binary_mat_intersections <span class="op">=</span> indices_set_to_binary_matrix(intersection, mlp_ig_results.shape)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(binary_mat_intersections, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in both attribution methods"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>top_mlp_ig_exclusive <span class="op">=</span> top_mlp_ig_sets.difference(top_mlp_patch_sets)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>binary_mat_ig_exclusive <span class="op">=</span> indices_set_to_binary_matrix(top_mlp_ig_exclusive, mlp_ig_results.shape)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(binary_mat_ig_exclusive, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in only integrated gradients"</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="34">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_exclusive <span class="op">=</span> top_mlp_patch_sets.difference(top_mlp_ig_sets)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>binary_mat_patch_exclusive <span class="op">=</span> indices_set_to_binary_matrix(top_mlp_patch_exclusive, mlp_patch_results.shape)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(binary_mat_patch_exclusive, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Top MLP neurons in only causal tracing"</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="measuring-agreement-tukey-mean-difference-plot" class="level3">
<h3 class="anchored" data-anchor-id="measuring-agreement-tukey-mean-difference-plot">Measuring agreement: Tukey mean-difference plot</h3>
<p>Assumptions: the two attribution methods have the same precision, the precision is constant and does not depend on the “true” attribution score, and the difference between the two methods is constant.</p>
<p>NOTE: since the scales of measurement may be different, this may not be applicable.</p>
<div id="cell-34" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MaxAbsScaler</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>mlp_ig_results_1d <span class="op">=</span> mlp_ig_results.flatten().numpy()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>mlp_patch_results_1d <span class="op">=</span> mlp_patch_results.flatten().numpy()</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean-difference plots</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean([mlp_ig_results_1d, mlp_patch_results_1d], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> mlp_patch_results_1d <span class="op">-</span> mlp_ig_results_1d</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>md <span class="op">=</span> np.mean(diff) <span class="co"># Mean of the difference</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> np.std(diff, axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># Standard deviation of the difference</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>mean, y<span class="op">=</span>diff, fit_reg<span class="op">=</span><span class="va">True</span>, scatter<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>plt.axhline(md, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Mean difference"</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'pink'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"1.96 SD of difference"</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">-</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'lightblue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"-1.96 SD of difference"</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Mean of attribution scores per neuron"</span>)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Difference (activation patching - integrated gradients) per neuron"</span>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mean-difference plot of attribution scores from integrated gradients and activation patching"</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-35" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean difference plot with scaled data</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>scaled_mlp_ig_results_1d <span class="op">=</span> MaxAbsScaler().fit_transform(mlp_ig_results_1d.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>scaled_mlp_patch_results_1d <span class="op">=</span> MaxAbsScaler().fit_transform(mlp_patch_results_1d.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean([scaled_mlp_ig_results_1d, scaled_mlp_patch_results_1d], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> scaled_mlp_patch_results_1d <span class="op">-</span> scaled_mlp_ig_results_1d</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>md <span class="op">=</span> np.mean(diff) <span class="co"># Mean of the difference</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>sd <span class="op">=</span> np.std(diff, axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># Standard deviation of the difference</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>mean, y<span class="op">=</span>diff, fit_reg<span class="op">=</span><span class="va">True</span>, scatter<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>plt.axhline(md, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Mean difference"</span>)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">+</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'pink'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"1.96 SD of difference"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>plt.axhline(md <span class="op">-</span> <span class="fl">1.96</span><span class="op">*</span>sd, color<span class="op">=</span><span class="st">'lightblue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"-1.96 SD of difference"</span>)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Mean of attribution scores per neuron"</span>)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Difference (activation patching - integrated gradients) per neuron"</span>)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mean-difference plot of scaled attribution scores from integrated gradients and activation patching"</span>)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>Mean difference is close to zero, indicating a lack of fixed bias: methods tend to agree.</li>
<li>Deviation from the mean difference increases as the average attribution score increases, indicating proportional bias. Methods tend to agree on which neurons contribute less to the output, but disagree more on neurons which are identified as important by one/both of the methods. Specifically, for larger attribution scores, integrated gradients assigns greater attribution scores to neurons than causal tracing.</li>
<li>The limits of agreement (95% of the differences between attribution scores) lie within approximately -0.007 to 0.010. This is a fairly small but still noticeable range of error, given that the difference in attribution scores lies between -0.009 and 0.030.</li>
</ul>
</section>
<section id="difference-in-scores-for-attention-heads" class="level3">
<h3 class="anchored" data-anchor-id="difference-in-scores-for-attention-heads">Difference in scores for attention heads</h3>
<div id="cell-38" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MaxAbsScaler</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>scaled_attn_ig_results <span class="op">=</span> MaxAbsScaler().fit_transform(attn_ig_results)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>scaled_attn_patch_results <span class="op">=</span> MaxAbsScaler().fit_transform(attn_patch_results)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>diff_attn_results <span class="op">=</span> scaled_attn_ig_results <span class="op">-</span> scaled_attn_patch_results</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>diff_attn_results_abs <span class="op">=</span> np.<span class="bu">abs</span>(scaled_attn_ig_results) <span class="op">-</span> np.<span class="bu">abs</span>(scaled_attn_patch_results)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(diff_attn_results, cmap<span class="op">=</span><span class="st">"RdBu"</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Difference in attributions for attention heads"</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(diff_attn_results_abs, cmap<span class="op">=</span><span class="st">"RdBu"</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Difference in (absolute) attributions for attention heads"</span>)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="factors-which-impact-integrated-gradients" class="level1">
<h1>Factors which impact Integrated Gradients</h1>
<section id="sensitivity-to-input-opposite-classification" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity-to-input-opposite-classification">Sensitivity to input: opposite classification</h3>
<p><em>Aim</em>: Verify that results (and implementation) are reasonable.</p>
<p>Repeat IG attribution with unbalanced sequence for comparison.</p>
<div id="cell-52" class="cell" data-execution_count="35">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_unbalanced_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>attn_ig_unbalanced_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.isin(<span class="bu">input</span>, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> <span class="bu">input</span> <span class="op">*</span> mask <span class="op">+</span> tokenizer.PAD_TOKEN <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> mask)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>_, input_cache <span class="op">=</span> model.run_with_cache(<span class="bu">input</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>_, baseline_cache <span class="op">=</span> model.run_with_cache(baseline)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"z"</span>, layer)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> input_cache[prev_layer_hook]</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> baseline_cache[prev_layer_hook]</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_head, d_model]</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    attn_ig_unbalanced_results[layer] <span class="op">=</span> score</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"post"</span>, layer)</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> input_cache[prev_layer_hook]</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> baseline_cache[prev_layer_hook]</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(<span class="bu">input</span>, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_model]</span></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>    mlp_ig_unbalanced_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Error (delta) for blocks.0.attn.hook_result attribution: 3.848981577903032e-09

Error (delta) for blocks.0.hook_mlp_out attribution: -9.032419256982394e-11

Error (delta) for blocks.1.attn.hook_result attribution: 3.583409124985337e-10

Error (delta) for blocks.1.hook_mlp_out attribution: -1.2732925824820995e-11

Error (delta) for blocks.2.attn.hook_result attribution: 1.4551915228366852e-11

Error (delta) for blocks.2.hook_mlp_out attribution: -2.319211489520967e-11</code></pre>
</div>
</div>
<div id="cell-53" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_unbalanced_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_unbalanced_results)))</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_unbalanced_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients) - Unbalanced"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-31-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-54" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_unbalanced_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_unbalanced_results)))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_unbalanced_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients) - Unbalanced"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-32-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Gradient attribution scores for neurons and attention heads change when the sequence is unbalanced. Notably, head 2.1 contributes significantly more highly to a positive “unbalanced” classification. This suggests that head 2.1 plays a strong role in the final output. The attribution scores for the other attention heads are less strong, suggesting a weaker role. Overall, this seems to confirm that IG is sensitive to the input as expected.</p>
</section>
<section id="average-of-integrated-gradients-attributions" class="level3">
<h3 class="anchored" data-anchor-id="average-of-integrated-gradients-attributions">Average of Integrated Gradients attributions</h3>
<p><em>Observation</em>: The activation patching attribution scores look like they could be the average of the integrated gradients attribution scores for the clean and corrupted inputs.</p>
<p><em>Aim</em>: Take the average of the integrated gradients scores for balanced and unbalanced inputs.</p>
<div id="cell-63" class="cell" data-execution_count="36">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>mlp_ig_sum_results <span class="op">=</span> (mlp_ig_results <span class="op">+</span> mlp_ig_unbalanced_results) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_sum_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_sum_results)))</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_sum_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients) - Average"</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-36-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The average of IG attribution scores is somewhat noisier.</p>
</section>
</section>
<section id="factors-which-impact-activation-patching" class="level1">
<h1>Factors which impact Activation Patching</h1>
<section id="patching-with-different-inputs" class="level3">
<h3 class="anchored" data-anchor-id="patching-with-different-inputs">Patching with different inputs</h3>
<p><em>Hypothesis</em>: Activation patching could be sensitive to the pair of inputs used for corruption.</p>
<div id="cell-67" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>clean_tokens_v2 <span class="op">=</span> <span class="st">"()()"</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>corrupted_tokens_v2 <span class="op">=</span> <span class="st">"))()"</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>clean_input_v2 <span class="op">=</span> tokenizer.tokenize(clean_tokens_v2)        <span class="co"># Balanced</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>corrupted_input_v2 <span class="op">=</span> tokenizer.tokenize(corrupted_tokens_v2)    <span class="co"># Unbalanced</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We run on the corrupted prompt with the cache so we store activations to patch in later.</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>corrupted_logits_v2, corrupted_cache_v2 <span class="op">=</span> model.run_with_cache(corrupted_input_v2)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>clean_logits_v2 <span class="op">=</span> model(clean_input_v2)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Get probability of overall sequence being balanced (class 1) from position 0</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>clean_answer_logits_v2 <span class="op">=</span> clean_logits_v2[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>corrupted_answer_logits_v2 <span class="op">=</span> corrupted_logits_v2[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Balanced input score: </span><span class="sc">{</span>clean_answer_logits_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbalanced input score: </span><span class="sc">{</span>corrupted_answer_logits_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>baseline_diff_v2 <span class="op">=</span> (corrupted_answer_logits_v2 <span class="op">-</span> clean_answer_logits_v2).item()</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline clean-corrupted logit difference: </span><span class="sc">{</span>baseline_diff_v2<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>mlp_patch_results_v2 <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>attn_patch_results_v2 <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, corrupted_cache_v2, head)</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input_v2)</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits_v2).item()</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>        attn_patch_results_v2[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff_v2</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, corrupted_cache_v2, neuron)</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input_v2)</span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits_v2).item()</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>        mlp_patch_results_v2[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff_v2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced input score: 5.693811416625977
Unbalanced input score: -4.950194358825684
Baseline clean-corrupted logit difference: -10.64</code></pre>
</div>
</div>
<div id="cell-68" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_results_v2), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_results_v2)))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_results_v2, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"MLP Neuron Causal Tracing: Corrupt </span><span class="sc">{</span>corrupted_tokens_v2<span class="sc">}</span><span class="ss"> -&gt; Clean for </span><span class="sc">{</span>clean_tokens_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_results_v2), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_results_v2)))</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_results_v2, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Attention Head Causal Tracing: Corrupt </span><span class="sc">{</span>corrupted_tokens_v2<span class="sc">}</span><span class="ss"> -&gt; Clean for </span><span class="sc">{</span>clean_tokens_v2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-38-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-38-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-69" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare difference with alternative inputs</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>mlp_patch_diff <span class="op">=</span> mlp_patch_results <span class="op">-</span> mlp_patch_results_v2</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_diff), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_diff)))</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_diff, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"MLP Neuron Causal Tracing: Difference from (()( and ))()"</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-39-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-70" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine alternative inputs</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>mlp_patch_sum <span class="op">=</span> mlp_patch_results <span class="op">+</span> mlp_patch_results_v2</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_sum), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_sum)))</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_sum, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"MLP Neuron Causal Tracing: Sum of (()( and ))()"</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-40-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="patching-in-the-opposite-direction-denoising" class="level3">
<h3 class="anchored" data-anchor-id="patching-in-the-opposite-direction-denoising">Patching in the opposite direction (denoising)</h3>
<p><em>Hypothesis</em>: activation patching is sensitive to the direction of patching. Patching from clean to corrupted activations should reveal sufficient components to restore model behaviour.</p>
<div id="cell-72" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)    <span class="co"># Unbalanced</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We run on the clean prompt with the cache so we store activations to patch in later.</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>clean_logits, clean_cache <span class="op">=</span> model.run_with_cache(clean_input)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>corrupted_logits <span class="op">=</span> model(corrupted_input)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get probability of overall sequence being balanced (class 1) from position 0</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>corrupted_answer_logits <span class="op">=</span> corrupted_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Balanced input score: </span><span class="sc">{</span>clean_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unbalanced input score: </span><span class="sc">{</span>corrupted_answer_logits<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>baseline_diff <span class="op">=</span> (corrupted_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Baseline clean-corrupted logit difference: </span><span class="sc">{</span>baseline_diff<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced input score: 5.693811416625977
Unbalanced input score: -5.421151161193848
Baseline clean-corrupted logit difference: -11.11</code></pre>
</div>
</div>
<div id="cell-73" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>mlp_patch_denoising_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>attn_patch_denoising_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, clean_cache, head)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(corrupted_input)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> corrupted_answer_logits).item()</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>        attn_patch_denoising_results[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, clean_cache, neuron)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(corrupted_input)</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> corrupted_answer_logits).item()</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>        mlp_patch_denoising_results[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-74" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_denoising_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_denoising_results)))</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_denoising_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Clean -&gt; Corrupted"</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-43-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-75" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_denoising_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_denoising_results)))</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_denoising_results, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Clean -&gt; Corrupted"</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-44-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Patching in the opposite direction highlights similar components to original activation patching. May be possible to get both necessary and sufficient components by patching in both directions, although this is expensive.</p>
</section>
</section>
<section id="distribution-of-activations-and-baselines" class="level1">
<h1>Distribution of activations and baselines</h1>
<p><em>Hypothesis</em>: One possible reason for the discrepancy between patching and IG is that the range of activations tested may be from different distributions.</p>
<p>Both gradient methods rely on counterfactual reasoning. IG computes the integral between some baseline (which produces zero output) and given input, whereas causal tracing computes the logit difference between two counterfactual inputs. If the counterfactuals used are different, then this could cause a discrepancy.</p>
<p>I plotted the maximum difference between the corrupted activations and the range of zero to clean activations. The plot indicates that the corrupt activations for many MLP neurons lie outside of the range, suggesting that there could be out-of-distribution issues going on.</p>
<div id="cell-79" class="cell" data-execution_count="93">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># If the corrupt activations for tracing are outside of the bounds for gradient attribution, measure the distance</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> measure_distance_from_bound(bounds, value):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    lower_bound <span class="op">=</span> <span class="bu">min</span>(bounds)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    upper_bound <span class="op">=</span> <span class="bu">max</span>(bounds)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> value <span class="op">&lt;</span> lower_bound:</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> value <span class="op">-</span> lower_bound</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> value <span class="op">&gt;</span> upper_bound:</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> value <span class="op">-</span> upper_bound</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)    <span class="co"># Unbalanced</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>clean_logits, clean_cache <span class="op">=</span> model.run_with_cache(clean_input)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>corrupted_logits, corrupted_cache <span class="op">=</span> model.run_with_cache(corrupted_input)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>mlp_distance <span class="op">=</span> torch.zeros((model.cfg.n_layers, model.cfg.d_mlp))</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>attn_distance <span class="op">=</span> torch.zeros((model.cfg.n_layers, model.cfg.n_heads))</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get distance for attention heads</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_acts <span class="op">=</span> corrupted_cache[hook_name]</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    layer_clean_acts <span class="op">=</span> clean_cache[hook_name]</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Go over each attention head and take the maximum distance</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>        head_corrupt_acts <span class="op">=</span> layer_corrupt_acts.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">3</span>)[<span class="dv">0</span>].<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>, head]</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>        head_clean_acts <span class="op">=</span> layer_clean_acts.mean(dim<span class="op">=</span><span class="dv">3</span>).mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>, head]</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The clean activations are what is used for integrated gradients</span></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>        distance <span class="op">=</span> measure_distance_from_bound(bounds<span class="op">=</span>(<span class="dv">0</span>, head_clean_acts), value<span class="op">=</span>head_corrupt_acts)</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>        max_distance <span class="op">=</span> <span class="bu">max</span>(distance, max_distance)</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>        attn_distance[layer, head] <span class="op">=</span> max_distance</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get distance for MLP neurons</span></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_acts <span class="op">=</span> corrupted_cache[hook_name]</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>    layer_clean_acts <span class="op">=</span> clean_cache[hook_name]</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron_idx <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>        neuron_corrupt_acts <span class="op">=</span> layer_corrupt_acts[<span class="dv">0</span>, :, neuron_idx]</span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>        neuron_clean_acts <span class="op">=</span> layer_clean_acts[<span class="dv">0</span>, :, neuron_idx]</span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Go over each token and take the maximum distance</span></span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a>        max_distance <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(neuron_clean_acts.size(<span class="op">-</span><span class="dv">1</span>)):</span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The clean activations are what is used for integrated gradients</span></span>
<span id="cb45-47"><a href="#cb45-47" aria-hidden="true" tabindex="-1"></a>            distance <span class="op">=</span> measure_distance_from_bound(bounds<span class="op">=</span>(<span class="dv">0</span>, neuron_clean_acts[i]), value<span class="op">=</span>neuron_corrupt_acts[i])</span>
<span id="cb45-48"><a href="#cb45-48" aria-hidden="true" tabindex="-1"></a>            max_distance <span class="op">=</span> <span class="bu">max</span>(distance, max_distance)</span>
<span id="cb45-49"><a href="#cb45-49" aria-hidden="true" tabindex="-1"></a>        mlp_distance[layer, neuron_idx] <span class="op">=</span> max_distance</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-80" class="cell" data-execution_count="94">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_distance), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_distance)))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_distance.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Max distance of corrupt neuron activations outside IG gradient bounds"</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_distance), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_distance)))</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_distance, cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Max distance of corrupt attention head activations outside IG gradient bounds"</span>)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-46-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-46-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Qualitatively, for MLP neurons, the corrupt activations are outside of the range of zero to clean activations in areas highlighted by both methods. For attention heads, most of the corrupt attention head activations seem to be outside of IG bounds.</p>
<section id="baseline-as-corrupted-activations" class="level3">
<h3 class="anchored" data-anchor-id="baseline-as-corrupted-activations">Baseline as corrupted activations</h3>
<p>Instead of using zero as the baseline activation, use the activation from the corrupt input in causal tracing.</p>
<div id="cell-83" class="cell" data-execution_count="44">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for neurons in MLP layers</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>mlp_ig_corrupt_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient attribution for attention heads</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>attn_ig_corrupt_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate integrated gradients for each layer</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>_, clean_cache <span class="op">=</span> model.run_with_cache(clean_input)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>corrupted_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"(()("</span>)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co"># mask = np.isin(input, [tokenizer.START_TOKEN, tokenizer.END_TOKEN])</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co"># corrupted_input = input * mask + tokenizer.PAD_TOKEN * (1 - mask)</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>_, corrupted_cache <span class="op">=</span> model.run_with_cache(corrupted_input)</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on heads</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"z"</span>, layer)</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> clean_cache[prev_layer_hook]</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> corrupted_cache[prev_layer_hook]</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(run_from_layer_fn(layer_corrupt_input, clean_input, prev_layer))</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_head, d_model]</span></span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate attribution score based on mean over each embedding, for each token</span></span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>    per_token_score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> per_token_score.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>    attn_ig_corrupt_baseline_results[layer] <span class="op">=</span> score</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient attribution on MLP neurons</span></span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>    hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a>    target_layer <span class="op">=</span> model.hook_dict[hook_name]</span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a>    prev_layer_hook <span class="op">=</span> get_act_name(<span class="st">"post"</span>, layer)</span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>    prev_layer <span class="op">=</span> model.hook_dict[prev_layer_hook]</span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a>    layer_clean_input <span class="op">=</span> clean_cache[prev_layer_hook]</span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>    layer_corrupt_input <span class="op">=</span> corrupted_cache[prev_layer_hook]</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a>    attributions <span class="op">=</span> compute_layer_to_output_attributions(clean_input, layer_clean_input, layer_corrupt_input, target_layer, prev_layer) <span class="co"># shape [1, seq_len, d_model]</span></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> attributions.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a>    mlp_ig_corrupt_baseline_results[layer] <span class="op">=</span> score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1.4471e-05], grad_fn=&lt;SelectBackward0&gt;)

Error (delta) for blocks.0.attn.hook_result attribution: -0.5329379439353943

Error (delta) for blocks.0.hook_mlp_out attribution: -0.010210990905761719
tensor([0.0496], grad_fn=&lt;SelectBackward0&gt;)

Error (delta) for blocks.1.attn.hook_result attribution: -0.0002592802047729492

Error (delta) for blocks.1.hook_mlp_out attribution: 2.187490463256836e-05
tensor([8.8178e-06], grad_fn=&lt;SelectBackward0&gt;)

Error (delta) for blocks.2.attn.hook_result attribution: -0.00015395879745483398

Error (delta) for blocks.2.hook_mlp_out attribution: 2.0354718799353577e-08</code></pre>
</div>
</div>
<div id="cell-84" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_ig_corrupt_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_ig_corrupt_baseline_results)))</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_ig_corrupt_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Gradient Attribution (Integrated Gradients): Corrupt Baseline"</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_ig_corrupt_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_ig_corrupt_baseline_results)))</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_ig_corrupt_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Gradient Attribution (Integrated Gradients): Corrupt Baseline"</span>)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-48-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-48-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Running integrated gradients with the baselines as the corrupt activations seems to bring results closer to causal tracing for MLP neurons, but further away for attention heads. The MLP neuron attribution scores still have minor disagreements.</p>
<div id="cell-86" class="cell" data-execution_count="61">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the attribution scores against each other. Correlation: y = x.</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> mlp_ig_corrupt_baseline_results.flatten().numpy()</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> mlp_patch_results.flatten().numpy()</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x, y<span class="op">=</span>y)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Integrated Gradients with Corrupted Baseline MLP Attribution Scores"</span>)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Activation Patching MLP Attribution Scores"</span>)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation coefficient between IG with corrupted baseline and AP attributions for neurons: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> attn_ig_corrupt_baseline_results.flatten().numpy()</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> attn_patch_results.flatten().numpy()</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x, y<span class="op">=</span>y)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Integrated Gradients with Corrupted Baseline Attention Attribution Scores"</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Causal Tracing Attention Attribution Scores"</span>)</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Correlation coefficient between IG with corrupted baseline and AP attributions for attention: </span><span class="sc">{</span>np<span class="sc">.</span>corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-49-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation coefficient between IG with corrupted baseline and AP attributions for neurons: 0.823746490811176</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-49-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation coefficient between IG with corrupted baseline and AP attributions for attention: 0.7527592281308121</code></pre>
</div>
</div>
<div id="cell-87" class="cell" data-execution_count="74">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>top_mlp_corrupt_baseline_ig_indices, _ <span class="op">=</span> get_attributions_above_threshold(mlp_ig_corrupt_baseline_results, <span class="fl">0.3</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>top_mlp_corrupt_baseline_ig_sets <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_corrupt_baseline_ig_indices])</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>top_mlp_patch_sets <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_patch_indices])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>intersection <span class="op">=</span> top_mlp_corrupt_baseline_ig_sets.intersection(top_mlp_patch_sets)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>union <span class="op">=</span> top_mlp_corrupt_baseline_ig_sets.union(top_mlp_patch_sets)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>jaccard <span class="op">=</span> <span class="bu">len</span>(intersection) <span class="op">/</span> <span class="bu">len</span>(union)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Jaccard score for MLP neurons using IG with corrupted baseline: </span><span class="sc">{</span>jaccard<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Jaccard score for MLP neurons using IG with corrupted baseline: 0.5757575757575758</code></pre>
</div>
</div>
</section>
<section id="activation-patching-with-ig-zero-baseline" class="level3">
<h3 class="anchored" data-anchor-id="activation-patching-with-ig-zero-baseline">Activation patching with IG zero baseline</h3>
<p>Instead of patching in activation from unbalanced input, patch in zero ablation (same baseline as integrated gradients).</p>
<div id="cell-89" class="cell" data-execution_count="231">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens <span class="im">import</span> HookedTransformer, ActivationCache</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformer_lens.hook_points <span class="im">import</span> HookPoint</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)        <span class="co"># Balanced</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>clean_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>zero_attn_cache <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>, model.cfg.n_heads, <span class="dv">1</span>))) <span class="co"># Return zero for any attention head</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>zero_mlp_cache <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>, model.cfg.d_mlp))) <span class="co"># Return zero for any MLP neuron</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-90" class="cell" data-execution_count="232">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>mlp_patch_zero_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>attn_patch_zero_baseline_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_attn_hook(act, hook, zero_attn_cache, head)</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>        attn_patch_zero_baseline_results[layer, head] <span class="op">=</span> logit_diff</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_neuron_hook(act, hook, zero_mlp_cache, neuron)</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>        mlp_patch_zero_baseline_results[layer, neuron] <span class="op">=</span> logit_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-91" class="cell" data-execution_count="233">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_zero_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_zero_baseline_results)))</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_zero_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Zero -&gt; Clean"</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_zero_baseline_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_zero_baseline_results)))</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_zero_baseline_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Zero -&gt; Corrupted"</span>)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-53-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-53-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Performing causal tracing by ablating the components with zero activations produces a very different attribution score distribution. This might not be meaningful since zero ablations are most likely out-of-distribution wrt the model’s typical activation patterns.</p>
</section>
<section id="patching-with-mean-value-of-corrupted-dataset" class="level3">
<h3 class="anchored" data-anchor-id="patching-with-mean-value-of-corrupted-dataset">Patching with mean value of corrupted dataset</h3>
<div id="cell-94" class="cell" data-execution_count="239">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.brackets_datasets <span class="im">import</span> BracketsDataset</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get corrupted datasets</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"toy_transformers/brackets_data.json"</span>) <span class="im">as</span> f:</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> json.load(f)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> data_tuples[:<span class="dv">5000</span>]</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> BracketsDataset(data_tuples)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>corrupted_dataset <span class="op">=</span> data.toks[<span class="op">~</span>data.isbal]</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>corrupted_dataset_answer_logits, corrupted_dataset_cache <span class="op">=</span> model.run_with_cache(corrupted_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-95" class="cell" data-execution_count="245">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_mean_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, neuron_idx: <span class="bu">int</span>):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target neuron with activations from the cached run.</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    activations[:, :, neuron_idx] <span class="op">=</span> cached_activations[:, neuron_idx]</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> patch_mean_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, head_idx: <span class="bu">int</span>):</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Replace the activations for the target attention head with activations from the cached run.</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>    activations[:, :, head_idx, :] <span class="op">=</span> cached_activations[:, head_idx, :]</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-96" class="cell" data-execution_count="254">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> tokenizer.tokenize(<span class="st">"()()"</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad token size to match corrupted_dataset size</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>padding <span class="op">=</span> torch.full((<span class="dv">1</span>, model.cfg.n_ctx <span class="op">-</span> clean_input.size(<span class="dv">1</span>)), tokenizer.PAD_TOKEN)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clean_input.shape, padding.shape)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>clean_input <span class="op">=</span> torch.cat([clean_input, padding], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clean_input.shape)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>clean_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>clean_answer_logits <span class="op">=</span> clean_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>corrupted_dataset_answer_logits <span class="op">=</span> corrupted_dataset_answer_logits.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>baseline_mean_diff <span class="op">=</span> (corrupted_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch neurons in MLP layers</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>mlp_patch_mean_corrupt_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.d_mlp)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch attention heads</span></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>attn_patch_mean_corrupt_results <span class="op">=</span> torch.zeros(model.cfg.n_layers, model.cfg.n_heads)</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on heads</span></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads):</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_mean_attn_hook(act, hook, corrupted_dataset_cache, head)</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>        attn_patch_mean_corrupt_results[layer, head] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_mean_diff</span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Activation patching on MLP neurons</span></span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> neuron <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp):</span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>        hook_name <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a>        temp_hook <span class="op">=</span> <span class="kw">lambda</span> act, hook: patch_mean_neuron_hook(act, hook, corrupted_dataset_cache, neuron)</span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>[(hook_name, temp_hook)]):</span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>            patched_logits <span class="op">=</span> model(clean_input)</span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>        patched_answer_logits <span class="op">=</span> patched_logits[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>        logit_diff <span class="op">=</span> (patched_answer_logits <span class="op">-</span> clean_answer_logits).item()</span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalise result by clean and corrupted logit difference</span></span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a>        mlp_patch_mean_corrupt_results[layer, neuron] <span class="op">=</span> logit_diff <span class="op">/</span> baseline_mean_diff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 6]) torch.Size([1, 36])
torch.Size([1, 42])</code></pre>
</div>
</div>
<div id="cell-97" class="cell" data-execution_count="255">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(mlp_patch_mean_corrupt_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(mlp_patch_mean_corrupt_results)))</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(mlp_patch_mean_corrupt_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MLP Neuron Causal Tracing: Mean Corrupted -&gt; Clean"</span>)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(np.arange(<span class="dv">0</span>, <span class="dv">56</span>, <span class="dv">2</span>))</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Neuron Index"</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(orientation<span class="op">=</span><span class="st">"horizontal"</span>)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>bound <span class="op">=</span> <span class="bu">max</span>(torch.<span class="bu">max</span>(attn_patch_mean_corrupt_results), <span class="bu">abs</span>(torch.<span class="bu">min</span>(attn_patch_mean_corrupt_results)))</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(attn_patch_mean_corrupt_results.detach(), cmap<span class="op">=</span><span class="st">'RdBu'</span>, vmin<span class="op">=-</span>bound, vmax<span class="op">=</span>bound)</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Attention Head Causal Tracing: Mean Corrupted -&gt; Clean"</span>)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Head Index"</span>)</span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>plt.xticks([<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Layer"</span>)</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>plt.yticks([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-57-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-57-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I performed casual tracing by ablating components with the mean activation for a dataset of corrupted outputs. The attribution results look different from existing IG and causal tracing results with the corrupt baseline <code>(()(</code>. This is the sharpest attribution result so far - could potentially be the most generalisable?</p>
</section>
</section>
<section id="ablation-studies" class="level1">
<h1>Ablation studies</h1>
<p>We evaluate the discrepancy between the two methods using ablation studies, to investigate possible causes of the different attributions.</p>
<div id="cell-100" class="cell" data-execution_count="44">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.brackets_datasets <span class="im">import</span> BracketsDataset</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toy_transformers.toy_bracket_transformer <span class="im">import</span> test_loaded_bracket_model_on_dataset</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Get clean and corrupted datasets</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"toy_transformers/brackets_data.json"</span>) <span class="im">as</span> f:</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> json.load(f)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    data_tuples <span class="op">=</span> data_tuples[:<span class="dv">6000</span>]</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> BracketsDataset(data_tuples)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>clean_dataset <span class="op">=</span> data.toks[data.isbal]</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>clean_is_balanced <span class="op">=</span> torch.ones(clean_dataset.size(<span class="dv">0</span>))</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>corrupted_dataset <span class="op">=</span> data.toks[<span class="op">~</span>data.isbal]</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>corrupted_is_balanced <span class="op">=</span> torch.zeros(corrupted_dataset.size(<span class="dv">0</span>))</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>_, corrupted_dataset_cache <span class="op">=</span> model.run_with_cache(corrupted_dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-101" class="cell" data-execution_count="54">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate baseline performance on sample dataset</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>baseline_logits, baseline_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original model performance on positive samples:"</span>, baseline_performance.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 2732 out of 2732 training examples correct!
Original model performance on positive samples: 1.0</code></pre>
</div>
</div>
<div id="cell-102" class="cell" data-execution_count="53">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>_, baseline_corrupted_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, corrupted_dataset, corrupted_is_balanced)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original model performance on negative samples:"</span>, baseline_corrupted_performance.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 3268 out of 3268 training examples correct!
Original model performance on negative samples: 1.0</code></pre>
</div>
</div>
<div id="cell-103" class="cell" data-execution_count="42">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ablate_neuron_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, set_neurons_to_keep: <span class="bu">set</span>):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># Replace with mean value of activation samples</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    n_neurons <span class="op">=</span> activations.shape[<span class="dv">2</span>]</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> hook.layer()</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(n_neurons):</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (layer, idx) <span class="kw">not</span> <span class="kw">in</span> set_neurons_to_keep:</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>            activations[:, :, idx] <span class="op">=</span> cached_activations[:, idx]</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ablate_attn_hook(activations: torch.Tensor, hook: HookPoint, cache: ActivationCache, set_heads_to_keep: <span class="bu">set</span>):</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    cached_activations <span class="op">=</span> cache[hook.name].mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    n_heads <span class="op">=</span> activations.shape[<span class="dv">2</span>]</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> hook.layer()</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> head_idx <span class="kw">in</span> <span class="bu">range</span>(n_heads):</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (layer, head_idx) <span class="kw">not</span> <span class="kw">in</span> set_heads_to_keep:</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>            activations[:, :, head_idx, :] <span class="op">=</span> cached_activations[:, head_idx, :]</span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-104" class="cell" data-execution_count="52">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline comparison: ablating all MLP neurons and attention heads</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>model_ablate_all_hooks <span class="op">=</span> []</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    model_ablate_all_hooks.append((mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    model_ablate_all_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>model_ablate_all_hooks):</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>    _, model_ablate_all_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Performance when ablating all MLP neurons and attention heads:"</span>, model_ablate_all_performance.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 0 out of 2732 training examples correct!
Performance when ablating all MLP neurons and attention heads: 0.0</code></pre>
</div>
</div>
<div id="cell-105" class="cell" data-execution_count="51">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline comparison: ablating all MLP neurons</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>mlp_ablate_all_hooks <span class="op">=</span> []</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    mlp_ablate_all_hooks.append((hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>mlp_ablate_all_hooks):</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    _, mlp_ablate_all_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Performance when ablating all MLP neurons:"</span>, mlp_ablate_all_performance.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 0 out of 2732 training examples correct!
Performance when ablating all MLP neurons: 0.0</code></pre>
</div>
</div>
<div id="cell-106" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline comparison: ablating all attention heads</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>attn_ablate_all_hooks <span class="op">=</span> []</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    attn_ablate_all_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, <span class="bu">set</span>())))</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>attn_ablate_all_hooks):</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    _, attn_ablate_all_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Performance when ablating all attention heads:"</span>, attn_ablate_all_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Model got 0 out of 2732 training examples correct!
Performance when ablating all attention heads: tensor(0.)</code></pre>
</div>
</div>
<section id="ablation-studies-for-integrated-gradients" class="level3">
<h3 class="anchored" data-anchor-id="ablation-studies-for-integrated-gradients">Ablation studies for integrated gradients</h3>
<p>Evaluate faithfulness: when “unimportant” neurons and attention heads are ablated, performance should not be affected.</p>
<div id="cell-109" class="cell" data-execution_count="46">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate faithfulness: when "unimportant" neurons and attention heads are ablated, performance should not be affected</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>ig_isolation_hooks <span class="op">=</span> []</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>set_top_mlp_ig_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_ig_indices])</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_top_mlp_ig_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>set_ig_heads <span class="op">=</span> <span class="bu">set</span>([(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">1</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">1</span>)])</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>    ig_isolation_hooks.append((mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_top_mlp_ig_indices)))</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>    ig_isolation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_ig_heads)))</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>ig_isolation_hooks):</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    ig_isolated_logits, ig_isolated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ig_isolated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 50 out of 168

Model got 2614 out of 2732 training examples correct!
tensor(0.9568)</code></pre>
</div>
</div>
<p>Evaluate completeness: when “important” neurons are ablated, performance should be affected.</p>
<div id="cell-111" class="cell" data-execution_count="47">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get neurons to keep: complementary to neurons to keep</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_complementary_neuron_indices(neuron_indices: torch.Tensor, mlp_shape: <span class="bu">tuple</span>):</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.ones(mlp_shape)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> neuron_indices:</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>        layer_idx, neuron_idx <span class="op">=</span> <span class="bu">tuple</span>(idx)</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layer_idx <span class="op">&lt;</span> mlp_shape[<span class="dv">0</span>] <span class="kw">and</span> neuron_idx <span class="op">&lt;</span> mlp_shape[<span class="dv">1</span>]:</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>            mask[layer_idx, neuron_idx] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    complementary_indices <span class="op">=</span> torch.nonzero(mask)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> complementary_indices</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate completeness: when "important" neurons are ablated, performance should be affected</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>mlp_ig_excluded_indices <span class="op">=</span> get_complementary_neuron_indices(top_mlp_ig_indices, (model.cfg.n_layers, model.cfg.d_mlp))</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>set_mlp_ig_excluded_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> mlp_ig_excluded_indices])</span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_mlp_ig_excluded_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>set_ig_excluded_heads <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">1</span>), (<span class="dv">1</span>,<span class="dv">1</span>)]</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>ig_ablation_hooks <span class="op">=</span> []</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>    hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>    ig_ablation_hooks.append((hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_neurons_to_keep<span class="op">=</span>set_mlp_ig_excluded_indices)))</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>    ig_ablation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_ig_excluded_heads)))</span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-29"><a href="#cb77-29" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>ig_ablation_hooks):</span>
<span id="cb77-30"><a href="#cb77-30" aria-hidden="true" tabindex="-1"></a>    _, ig_ablated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb77-31"><a href="#cb77-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ig_ablated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 118 out of 168

Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
</section>
<section id="ablation-studies-on-causal-tracing" class="level3">
<h3 class="anchored" data-anchor-id="ablation-studies-on-causal-tracing">Ablation studies on causal tracing</h3>
<p>Evaluate faithfulness: when “unimportant” neurons and attention heads are ablated, performance should not be affected</p>
<div id="cell-114" class="cell" data-execution_count="48">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate faithfulness: when "unimportant" neurons and attention heads are ablated, performance should not be affected</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>patch_isolation_hooks <span class="op">=</span> []</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>set_top_mlp_patch_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> top_mlp_patch_indices])</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_top_mlp_patch_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>set_patch_heads <span class="op">=</span> <span class="bu">set</span>([(<span class="dv">0</span>,<span class="dv">0</span>), (<span class="dv">1</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">0</span>), (<span class="dv">2</span>,<span class="dv">1</span>)])</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>    mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>    patch_isolation_hooks.append((mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_top_mlp_patch_indices)))</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>    patch_isolation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_patch_heads)))</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>patch_isolation_hooks):</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>    patch_isolated_logits, patch_isolated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(patch_isolated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 50 out of 168

Model got 2519 out of 2732 training examples correct!
tensor(0.9220)</code></pre>
</div>
</div>
<p>Evaluate completeness: when “important” neurons are ablated, performance should be affected</p>
<div id="cell-116" class="cell" data-execution_count="49">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate completeness: when "important" neurons are ablated, performance should be affected</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>patch_ig_excluded_indices <span class="op">=</span> get_complementary_neuron_indices(top_mlp_patch_indices, (model.cfg.n_layers, model.cfg.d_mlp))</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>set_mlp_patch_excluded_indices <span class="op">=</span> <span class="bu">set</span>([<span class="bu">tuple</span>(t.tolist()) <span class="cf">for</span> t <span class="kw">in</span> patch_ig_excluded_indices])</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Neurons to keep: </span><span class="sc">{</span><span class="bu">len</span>(set_mlp_patch_excluded_indices)<span class="sc">}</span><span class="ss"> out of </span><span class="sc">{</span>model<span class="sc">.</span>cfg<span class="sc">.</span>n_layers <span class="op">*</span> model<span class="sc">.</span>cfg<span class="sc">.</span>d_mlp<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>set_patch_excluded_heads <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">1</span>), (<span class="dv">1</span>,<span class="dv">1</span>)]</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>patch_ablation_hooks <span class="op">=</span> []</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers):</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>    hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, layer)</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    patch_ablation_hooks.append((hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_neurons_to_keep<span class="op">=</span>set_mlp_patch_excluded_indices)))</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>    attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, layer)</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>    patch_ablation_hooks.append((attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_patch_excluded_heads)))</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model.hooks(fwd_hooks<span class="op">=</span>patch_ablation_hooks):</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>    _, patch_ablated_performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset, clean_is_balanced)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(patch_ablated_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Neurons to keep: 118 out of 168

Model got 0 out of 2732 training examples correct!
tensor(0.)</code></pre>
</div>
</div>
</section>
<section id="analysis-of-ablation-studies" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-ablation-studies">Analysis of ablation studies</h3>
<div id="cell-118" class="cell" data-execution_count="55">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model performance under corruption of different components"</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Corrupted model components"</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model performance on bracket classification"</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>plt.bar(</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"None"</span>, <span class="st">"All neurons"</span>, <span class="st">"All attention heads"</span>, <span class="st">"All heads and neurons"</span>],</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    [baseline_performance, mlp_ablate_all_performance, attn_ablate_all_performance, model_ablate_all_performance]</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=-</span><span class="dv">15</span>)</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-69-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-119" class="cell" data-execution_count="56">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model performance under corruption of components with high attribution scores"</span>)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Attribution method used to identify components for corruption"</span>)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model performance on bracket classification"</span>)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>plt.bar(</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"All heads + neurons"</span>, <span class="st">"Integrated Gradients"</span>, <span class="st">"Activation patching"</span>],</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    [model_ablate_all_performance, ig_ablated_performance, patch_ablated_performance],</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>[<span class="st">'grey'</span>, <span class="st">'tab:blue'</span>, <span class="st">'tab:blue'</span>]</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-70-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-120" class="cell" data-execution_count="57">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model performance under corruption of components with low attribution scores"</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Attribution method used to identify components to preserve"</span>)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Model performance on bracket classification"</span>)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>plt.bar(</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"Original model"</span>, <span class="st">"Integrated Gradients"</span>, <span class="st">"Activation patching"</span>],</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    [baseline_performance, ig_isolated_performance, patch_isolated_performance],</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span>[<span class="st">'grey'</span>, <span class="st">'tab:blue'</span>, <span class="st">'tab:blue'</span>]</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-71-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Preliminary ablation studies show that integrated gradients and activation patching both find important circuit components, as corrupting identified components reduces performance to 0.</p>
<p>However, activation patching may identify an over-complete circuit. For activation patching, corrupting all components <em>outside</em> of the identified neurons and attention heads reduces performance more than integrated gradients.</p>
<ul>
<li>One explanation is that activation patching only picks up components which are important to the specific circuit under examination, whereas integrated gradients identifies components which are overall important to the model.</li>
<li>Another explanation (more likely) is that the dataset used for evaluations is small, and minor differences in performance are not significant.</li>
</ul>
</section>
</section>
<section id="gradual-ablation" class="level1">
<h1>Gradual ablation</h1>
<p>Instead of taking the top 20% components and corrupting them all, we gradually corrupt components from the lowest attribution scores first to the highest attribution scores, and measure the drop in performance.</p>
<section id="gradual-ablation-for-integrated-gradients" class="level3">
<h3 class="anchored" data-anchor-id="gradual-ablation-for-integrated-gradients">Gradual ablation for Integrated Gradients</h3>
<div id="cell-124" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Order MLP neurons and attention heads by ABSOLUTE attribution scores</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>combined_ig_results <span class="op">=</span> torch.concat([mlp_ig_results.flatten(), attn_ig_results.flatten()])</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>sorted_indices_1d <span class="op">=</span> torch.argsort(combined_ig_results.<span class="bu">abs</span>())</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>NEURON_LABEL, ATTN_LABEL <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> convert_1d_indices_to_2d(indices_1d):</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>    indices_2d <span class="op">=</span> []</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a>    max_mlp_index <span class="op">=</span> model.cfg.d_mlp <span class="op">*</span> model.cfg.n_layers</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> indices_1d:</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">&lt;</span> max_mlp_index:</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># MLP neuron</span></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> idx <span class="op">//</span> model.cfg.d_mlp</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>            neuron_pos <span class="op">=</span> idx <span class="op">%</span> model.cfg.d_mlp</span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>            indices_2d.append([NEURON_LABEL, layer.item(), neuron_pos.item()])</span>
<span id="cb86-17"><a href="#cb86-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb86-18"><a href="#cb86-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Attention</span></span>
<span id="cb86-19"><a href="#cb86-19" aria-hidden="true" tabindex="-1"></a>            attn_idx <span class="op">=</span> idx <span class="op">-</span> (model.cfg.d_mlp <span class="op">*</span> model.cfg.n_layers)</span>
<span id="cb86-20"><a href="#cb86-20" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> attn_idx <span class="op">//</span> model.cfg.n_heads</span>
<span id="cb86-21"><a href="#cb86-21" aria-hidden="true" tabindex="-1"></a>            attn_head_pos <span class="op">=</span> attn_idx <span class="op">%</span> model.cfg.n_heads</span>
<span id="cb86-22"><a href="#cb86-22" aria-hidden="true" tabindex="-1"></a>            indices_2d.append([ATTN_LABEL, layer.item(), attn_head_pos.item()])</span>
<span id="cb86-23"><a href="#cb86-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> indices_2d</span>
<span id="cb86-24"><a href="#cb86-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-25"><a href="#cb86-25" aria-hidden="true" tabindex="-1"></a>sorted_indices_2d <span class="op">=</span> convert_1d_indices_to_2d(sorted_indices_1d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-125" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradually corrupt components with lowest attribution scores</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>gradual_ig_corruption_performance <span class="op">=</span> []</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Preserve all neurons at first, then delete</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>set_gradual_neurons_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp)])</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>set_gradual_attn_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads)])</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>model.clear_contexts()</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sorted_indices_2d)):</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>    keep_type, keep_layer, keep_idx <span class="op">=</span> sorted_indices_2d[idx]</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keep_type <span class="op">==</span> NEURON_LABEL:</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>        set_gradual_neurons_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>        mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, keep_layer)</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>        model.add_hook(mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_gradual_neurons_to_keep.copy()))</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>        set_gradual_attn_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>        attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, keep_layer)</span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>        model.add_hook(attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_gradual_attn_to_keep.copy()))</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>    _, performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset[:<span class="dv">500</span>], clean_is_balanced[:<span class="dv">500</span>])</span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(performance)</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>    gradual_ig_corruption_performance.append(performance)</span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-27"><a href="#cb87-27" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-126" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>plt.plot(gradual_ig_corruption_performance)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Gradual corruption starting with lowest IG attribution score"</span>)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of corrupted components"</span>)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Performance on balanced bracket classification"</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-74-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-127" class="cell" data-execution_count="98">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"gradual_ig_corruption_performance_v2"</span>, gradual_ig_corruption_performance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="gradual-ablation-with-activation-patching" class="level3">
<h3 class="anchored" data-anchor-id="gradual-ablation-with-activation-patching">Gradual ablation with Activation Patching</h3>
<div id="cell-129" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat with activation patching results</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>combined_patch_results <span class="op">=</span> torch.concat([mlp_patch_results.flatten(), attn_patch_results.flatten()])</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>sorted_patch_indices_1d <span class="op">=</span> torch.argsort(combined_patch_results.<span class="bu">abs</span>())</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>sorted_patch_indices_2d <span class="op">=</span> convert_1d_indices_to_2d(sorted_patch_indices_1d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-130" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradually corrupt components with lowest attribution scores from activation patching</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>gradual_patch_corruption_performance <span class="op">=</span> []</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>set_gradual_patch_neurons_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.d_mlp)])</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>set_gradual_patch_attn_to_keep <span class="op">=</span> <span class="bu">set</span>([(layer, pos) <span class="cf">for</span> layer <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_layers) <span class="cf">for</span> pos <span class="kw">in</span> <span class="bu">range</span>(model.cfg.n_heads)])</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>model.clear_contexts()</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sorted_patch_indices_2d)):</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>    keep_type, keep_layer, keep_idx <span class="op">=</span> sorted_patch_indices_2d[idx]</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> keep_type <span class="op">==</span> NEURON_LABEL:</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>        set_gradual_patch_neurons_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>        mlp_hook_point <span class="op">=</span> get_act_name(<span class="st">"mlp_out"</span>, keep_layer)</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>        model.add_hook(mlp_hook_point, <span class="kw">lambda</span> act, hook: ablate_neuron_hook(act, hook, corrupted_dataset_cache, set_gradual_patch_neurons_to_keep.copy()))</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>        set_gradual_patch_attn_to_keep.remove((keep_layer, keep_idx))</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>        attn_hook_point <span class="op">=</span> get_act_name(<span class="st">"result"</span>, keep_layer)</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a>        model.add_hook(attn_hook_point, <span class="kw">lambda</span> act, hook: ablate_attn_hook(act, hook, corrupted_dataset_cache, set_gradual_patch_attn_to_keep.copy()))</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>    _, performance <span class="op">=</span> test_loaded_bracket_model_on_dataset(model, clean_dataset[:<span class="dv">500</span>], clean_is_balanced[:<span class="dv">500</span>])</span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(performance)</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>    gradual_patch_corruption_performance.append(performance)</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>model.remove_all_hook_fns()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-131" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>plt.plot(gradual_patch_corruption_performance)</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Gradual corruption starting with lowest activation patching attribution score"</span>)</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Number of corrupted components"</span>)</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Performance on balanced bracket classification"</span>)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="next-steps-and-further-ideas" class="level1">
<h1>Next steps and further ideas</h1>
<ul>
<li>Investigate out-of-distribution error idea further
<ul>
<li>Why do attention head scores change so dramatically?</li>
<li>TBF approximations to activation patching use IG with clean/corrupt baselines</li>
</ul></li>
<li>Investigate alternative hypotheses for discrepancies in results
<ul>
<li>Identify components which are always highlighted as important by IG</li>
<li>Generic components: results of IG attribution might match the cumulative scores from causal tracing with a range of different counterfactual activations.</li>
</ul></li>
<li>Complete gradual ablation for causal tracing, and investigate performance results</li>
</ul>
<p>Since both methods rely on counterfactual reasoning, I wonder if there is some way to generate counterfactual inputs for a target component, such that causal tracing can identify that target component. - Connect counterfactuals and activation patching templates - It would be difficult to extract the original counterfactual input, because the space of activation possibilities at a target layer is infinite.</p>
<p>Future steps: - Extrapolate to full-sized transformer model - Test alternative attribution methods</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>